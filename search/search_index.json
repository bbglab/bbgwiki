{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the BBG-Wiki! \u00b6 This website is meant to include information of all the tools and data used by the bbglab team, so that it serves both as a guide to understand them and as a place where to find information about everything.","title":"Home"},{"location":"#welcome-to-the-bbg-wiki","text":"This website is meant to include information of all the tools and data used by the bbglab team, so that it serves both as a guide to understand them and as a place where to find information about everything.","title":"Welcome to the BBG-Wiki!"},{"location":"Edit%20BBG-Wiki/","text":"Edit BBG-Wiki \u00b6 The main language of the wiki documentation is Markdown . There are several online editors which can help writting Markdown text and automatically visualize what is being written. StackEdit Editor.md This wiki is stored in a GitHub repository , where each section of the wiki corresponds to a single Markdown file ( *.md ). By editing these files either online or locally, the wiki can be updated by everyone. Markdown cheatsheet Online \u00b6 Go to the bbg-wiki repository and edit any file inside the docs/ folder, which contains all the files of the documentation. Local \u00b6 Installation \u00b6 git clone git@github.com:bbglab/bbgwiki.git pip install -r bbgwiki/requirements.txt Requirements \u00b6 mkdocs >= 1.2.2 mkdocs-material >= 7.1.11 mkdocs-static-i18n >= 0.18 Commands \u00b6 Running at localhost (to try stuff before updating the main web) mkdocs serve Update web: git add <edited file or directory> git commit -m \"Message\" git push References \u00b6 Carlos L\u00f3pez Elorduy","title":"Edit BBG-Wiki"},{"location":"Edit%20BBG-Wiki/#edit-bbg-wiki","text":"The main language of the wiki documentation is Markdown . There are several online editors which can help writting Markdown text and automatically visualize what is being written. StackEdit Editor.md This wiki is stored in a GitHub repository , where each section of the wiki corresponds to a single Markdown file ( *.md ). By editing these files either online or locally, the wiki can be updated by everyone. Markdown cheatsheet","title":"Edit BBG-Wiki"},{"location":"Edit%20BBG-Wiki/#online","text":"Go to the bbg-wiki repository and edit any file inside the docs/ folder, which contains all the files of the documentation.","title":"Online"},{"location":"Edit%20BBG-Wiki/#local","text":"","title":"Local"},{"location":"Edit%20BBG-Wiki/#installation","text":"git clone git@github.com:bbglab/bbgwiki.git pip install -r bbgwiki/requirements.txt","title":"Installation"},{"location":"Edit%20BBG-Wiki/#requirements","text":"mkdocs >= 1.2.2 mkdocs-material >= 7.1.11 mkdocs-static-i18n >= 0.18","title":"Requirements"},{"location":"Edit%20BBG-Wiki/#commands","text":"Running at localhost (to try stuff before updating the main web) mkdocs serve Update web: git add <edited file or directory> git commit -m \"Message\" git push","title":"Commands"},{"location":"Edit%20BBG-Wiki/#references","text":"Carlos L\u00f3pez Elorduy","title":"References"},{"location":"Cluster%20basics/Backups/","text":"Backups \u00b6 Description \u00b6 Reference \u00b6","title":"Backups"},{"location":"Cluster%20basics/Backups/#backups","text":"","title":"Backups"},{"location":"Cluster%20basics/Backups/#description","text":"","title":"Description"},{"location":"Cluster%20basics/Backups/#reference","text":"","title":"Reference"},{"location":"Cluster%20basics/Headers/","text":"Headers \u00b6 Description \u00b6 Reference \u00b6","title":"Headers"},{"location":"Cluster%20basics/Headers/#headers","text":"","title":"Headers"},{"location":"Cluster%20basics/Headers/#description","text":"","title":"Description"},{"location":"Cluster%20basics/Headers/#reference","text":"","title":"Reference"},{"location":"Cluster%20basics/Interactive/","text":"Interactive \u00b6 The interactive command gives to the user an interactive shell in the cluster with slurm allocation. In other words, it allocates the user to a specific node of the cluster so that the jobs can be executed there without disturbing the rest of the users. Usage \u00b6 Once you enter the bbgcluster, you will see in the terminal <username>@login01 . It is here where, if you want to be allocated to your own node, you can just run the command: $ interactive salloc: Granted job allocation ******* If the login01 has changed to bbgn### where ### is the number identifying the current node. Apart from the basic use, there are optional arguments/flags for extra features: interactive [ -c ] [ -m ] [ -w ] [ -J ] [ -x ] -c : Number of CPU cores (default: 1) -m : Total amount of memory (GB) (default: 8 [GB]) -w : Target node -J : Job name -x : Binary that you want to run interactively Reference \u00b6 Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Interactive"},{"location":"Cluster%20basics/Interactive/#interactive","text":"The interactive command gives to the user an interactive shell in the cluster with slurm allocation. In other words, it allocates the user to a specific node of the cluster so that the jobs can be executed there without disturbing the rest of the users.","title":"Interactive"},{"location":"Cluster%20basics/Interactive/#usage","text":"Once you enter the bbgcluster, you will see in the terminal <username>@login01 . It is here where, if you want to be allocated to your own node, you can just run the command: $ interactive salloc: Granted job allocation ******* If the login01 has changed to bbgn### where ### is the number identifying the current node. Apart from the basic use, there are optional arguments/flags for extra features: interactive [ -c ] [ -m ] [ -w ] [ -J ] [ -x ] -c : Number of CPU cores (default: 1) -m : Total amount of memory (GB) (default: 8 [GB]) -w : Target node -J : Job name -x : Binary that you want to run interactively","title":"Usage"},{"location":"Cluster%20basics/Interactive/#reference","text":"Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Reference"},{"location":"Cluster%20basics/Notebooks_in_cluster/","text":"Running notebooks in the cluster \u00b6 Description \u00b6 Running a jupyter notebook in the cluster allows you to work with a notebook which will be running even if you disconnect from the cluster . This is especially useful for time-consuming/memory-consuming processes or notebooks with a high number of variables/packages needed, so that you have more computational power than your local computer, you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress and you can come back to a notebook without the need of loading all the variables/packages again. To run a notebook in the cluster, a screen and an interactive will be used. Usage \u00b6 You will need to follow the next steps: Connect to the cluster: ssh -p 22022 <username>@bbgcluster Open a screen: <username>@login01:~$ screen -S <screen_name> Run an interactive job: [ screen_name ] <username>@login01:~$ interactive Note If your notebook needs more than 8G and 2 cores, you can specify it here -- see interactive section. Activate conda base or the conda environment that you need in your notebook: [ screen_name ] <username>@bbgn005:~$ conda activate Go to the folder that you wish to run the notebook: ( base )[ screen_name ] <username>@bbgn005:~$ cd /workspace/folder Run the jupyter notebook: ( base )[ screen_name ] <username>@bbgn005:~/workspace/folder$ unset XDG_RUNTIME_DIR && jupyter notebook --ip = 0 .0.0.0 Keep the URL with the token and the node in which the interactive is running: [ I 10 :37:20.371 NotebookApp ] The Jupyter Notebook is running at: http://127.0.0.1:8888/?token = 730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c Now, your notebook is running and you can dettach from the screen: Ctrl + A -> D Open a new terminal and create an ssh tunnel to be able to access the port 8888 in the node -- following the example, this is node bbgn005 : ssh -L 8888 :bbgn005:8888 -p 22022 <username>@bbgcluster Now use the URL, replacing 127.0.0.1 with localhost and open the URL in your browser: http://localhost:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c Using the tunnel (ssh -L terminal) and the URL in the browser, you can enter the notebook as many times as you need. When you don't need the notebook to continue running in the cluster, reconnect to the screen: screen -r <screen_name> And kill jupyter and exit the interactive session. Reference \u00b6 Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Ferran Mui\u00f1os Paula Gomis","title":"Running notebooks in the cluster"},{"location":"Cluster%20basics/Notebooks_in_cluster/#running-notebooks-in-the-cluster","text":"","title":"Running notebooks in the cluster"},{"location":"Cluster%20basics/Notebooks_in_cluster/#description","text":"Running a jupyter notebook in the cluster allows you to work with a notebook which will be running even if you disconnect from the cluster . This is especially useful for time-consuming/memory-consuming processes or notebooks with a high number of variables/packages needed, so that you have more computational power than your local computer, you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress and you can come back to a notebook without the need of loading all the variables/packages again. To run a notebook in the cluster, a screen and an interactive will be used.","title":"Description"},{"location":"Cluster%20basics/Notebooks_in_cluster/#usage","text":"You will need to follow the next steps: Connect to the cluster: ssh -p 22022 <username>@bbgcluster Open a screen: <username>@login01:~$ screen -S <screen_name> Run an interactive job: [ screen_name ] <username>@login01:~$ interactive Note If your notebook needs more than 8G and 2 cores, you can specify it here -- see interactive section. Activate conda base or the conda environment that you need in your notebook: [ screen_name ] <username>@bbgn005:~$ conda activate Go to the folder that you wish to run the notebook: ( base )[ screen_name ] <username>@bbgn005:~$ cd /workspace/folder Run the jupyter notebook: ( base )[ screen_name ] <username>@bbgn005:~/workspace/folder$ unset XDG_RUNTIME_DIR && jupyter notebook --ip = 0 .0.0.0 Keep the URL with the token and the node in which the interactive is running: [ I 10 :37:20.371 NotebookApp ] The Jupyter Notebook is running at: http://127.0.0.1:8888/?token = 730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c Now, your notebook is running and you can dettach from the screen: Ctrl + A -> D Open a new terminal and create an ssh tunnel to be able to access the port 8888 in the node -- following the example, this is node bbgn005 : ssh -L 8888 :bbgn005:8888 -p 22022 <username>@bbgcluster Now use the URL, replacing 127.0.0.1 with localhost and open the URL in your browser: http://localhost:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c Using the tunnel (ssh -L terminal) and the URL in the browser, you can enter the notebook as many times as you need. When you don't need the notebook to continue running in the cluster, reconnect to the screen: screen -r <screen_name> And kill jupyter and exit the interactive session.","title":"Usage"},{"location":"Cluster%20basics/Notebooks_in_cluster/#reference","text":"Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Ferran Mui\u00f1os Paula Gomis","title":"Reference"},{"location":"Cluster%20basics/Screen/","text":"Screen \u00b6 Description \u00b6 The screen command opens a session which will be running even if you disconnect from the cluster . This is especially useful for time-consuming processes, so that you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress. You can also open several screens for different processes, which you can detach and attach to them as you like. Basic commands \u00b6 New screen \u00b6 Creates a new screen with name \" custom_name \". screen -S <custom_name> Warning When opening a new screen, this should be done from the login01 node, since this guarantees that the screen will be constantly running and not shut down (which could happen if the screen is opened in one of the other nodes). List screens \u00b6 List all the created screens. screen -ls Detach \u00b6 Detaches from a screen Ctrl + A -> D Re-attach \u00b6 Re-attaches to a detached screen. screen -r [ #] Note If there are multiple screens available, include the number of the screen id (or name ) to identify which screen to re-attach. Exit and kill screen \u00b6 exit Documentation \u00b6 For a more extensive list of commands, check the screen cheatsheet . You can also check the full documentation . Reference \u00b6 Carlos L\u00f3pez Elorduy Jordi Deu-Pons Miguel Grau","title":"Screen"},{"location":"Cluster%20basics/Screen/#screen","text":"","title":"Screen"},{"location":"Cluster%20basics/Screen/#description","text":"The screen command opens a session which will be running even if you disconnect from the cluster . This is especially useful for time-consuming processes, so that you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress. You can also open several screens for different processes, which you can detach and attach to them as you like.","title":"Description"},{"location":"Cluster%20basics/Screen/#basic-commands","text":"","title":"Basic commands"},{"location":"Cluster%20basics/Screen/#new-screen","text":"Creates a new screen with name \" custom_name \". screen -S <custom_name> Warning When opening a new screen, this should be done from the login01 node, since this guarantees that the screen will be constantly running and not shut down (which could happen if the screen is opened in one of the other nodes).","title":"New screen"},{"location":"Cluster%20basics/Screen/#list-screens","text":"List all the created screens. screen -ls","title":"List screens"},{"location":"Cluster%20basics/Screen/#detach","text":"Detaches from a screen Ctrl + A -> D","title":"Detach"},{"location":"Cluster%20basics/Screen/#re-attach","text":"Re-attaches to a detached screen. screen -r [ #] Note If there are multiple screens available, include the number of the screen id (or name ) to identify which screen to re-attach.","title":"Re-attach"},{"location":"Cluster%20basics/Screen/#exit-and-kill-screen","text":"exit","title":"Exit and kill screen"},{"location":"Cluster%20basics/Screen/#documentation","text":"For a more extensive list of commands, check the screen cheatsheet . You can also check the full documentation .","title":"Documentation"},{"location":"Cluster%20basics/Screen/#reference","text":"Carlos L\u00f3pez Elorduy Jordi Deu-Pons Miguel Grau","title":"Reference"},{"location":"Cluster%20basics/Structure/","text":"Structure \u00b6 Description \u00b6 Reference \u00b6","title":"Structure"},{"location":"Cluster%20basics/Structure/#structure","text":"","title":"Structure"},{"location":"Cluster%20basics/Structure/#description","text":"","title":"Description"},{"location":"Cluster%20basics/Structure/#reference","text":"","title":"Reference"},{"location":"Cluster%20basics/Submitting%20jobs/Qmap/","text":"Qmap \u00b6 Description \u00b6 Reference \u00b6","title":"Qmap"},{"location":"Cluster%20basics/Submitting%20jobs/Qmap/#qmap","text":"","title":"Qmap"},{"location":"Cluster%20basics/Submitting%20jobs/Qmap/#description","text":"","title":"Description"},{"location":"Cluster%20basics/Submitting%20jobs/Qmap/#reference","text":"","title":"Reference"},{"location":"Cluster%20basics/Submitting%20jobs/SLURM/","text":"SLURM \u00b6 Description \u00b6 Reference \u00b6","title":"SLURM"},{"location":"Cluster%20basics/Submitting%20jobs/SLURM/#slurm","text":"","title":"SLURM"},{"location":"Cluster%20basics/Submitting%20jobs/SLURM/#description","text":"","title":"Description"},{"location":"Cluster%20basics/Submitting%20jobs/SLURM/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/BeastAML/","text":"BeastAML \u00b6 Description \u00b6 Reference \u00b6","title":"BeastAML"},{"location":"Datasets/General%20datasets/BeastAML/#beastaml","text":"","title":"BeastAML"},{"location":"Datasets/General%20datasets/BeastAML/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/BeastAML/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/CGCI/","text":"CGCI \u00b6 Description \u00b6 Reference \u00b6","title":"CGCI"},{"location":"Datasets/General%20datasets/CGCI/#cgci","text":"","title":"CGCI"},{"location":"Datasets/General%20datasets/CGCI/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/CGCI/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/CPTAC/","text":"CPTAC \u00b6 Description \u00b6 Reference \u00b6","title":"CPTAC"},{"location":"Datasets/General%20datasets/CPTAC/#cptac","text":"","title":"CPTAC"},{"location":"Datasets/General%20datasets/CPTAC/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/CPTAC/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/GENIE/","text":"GENIE \u00b6 Description \u00b6 Reference \u00b6","title":"GENIE"},{"location":"Datasets/General%20datasets/GENIE/#genie","text":"","title":"GENIE"},{"location":"Datasets/General%20datasets/GENIE/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/GENIE/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/Hartwig/","text":"Hartwig \u00b6 Description \u00b6 Reference \u00b6","title":"Hartwig"},{"location":"Datasets/General%20datasets/Hartwig/#hartwig","text":"","title":"Hartwig"},{"location":"Datasets/General%20datasets/Hartwig/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/Hartwig/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/ICGC/","text":"ICGC \u00b6 Description \u00b6 Reference \u00b6","title":"ICGC"},{"location":"Datasets/General%20datasets/ICGC/#icgc","text":"","title":"ICGC"},{"location":"Datasets/General%20datasets/ICGC/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/ICGC/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/PCAWG/","text":"PCAWG \u00b6 Description \u00b6 Reference \u00b6","title":"PCAWG"},{"location":"Datasets/General%20datasets/PCAWG/#pcawg","text":"","title":"PCAWG"},{"location":"Datasets/General%20datasets/PCAWG/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/PCAWG/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/PedcBioPortal/","text":"PedcBioPortal \u00b6 Description \u00b6 Reference \u00b6","title":"PedcBioPortal"},{"location":"Datasets/General%20datasets/PedcBioPortal/#pedcbioportal","text":"","title":"PedcBioPortal"},{"location":"Datasets/General%20datasets/PedcBioPortal/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/PedcBioPortal/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/StJude/","text":"StJude \u00b6 Description \u00b6 Reference \u00b6","title":"StJude"},{"location":"Datasets/General%20datasets/StJude/#stjude","text":"","title":"StJude"},{"location":"Datasets/General%20datasets/StJude/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/StJude/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/StJudeLife/","text":"StJudeLife \u00b6 Description \u00b6 Reference \u00b6","title":"StJudeLife"},{"location":"Datasets/General%20datasets/StJudeLife/#stjudelife","text":"","title":"StJudeLife"},{"location":"Datasets/General%20datasets/StJudeLife/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/StJudeLife/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/TARGET/","text":"TARGET \u00b6 Description \u00b6 Reference \u00b6","title":"TARGET"},{"location":"Datasets/General%20datasets/TARGET/#target","text":"","title":"TARGET"},{"location":"Datasets/General%20datasets/TARGET/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/TARGET/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/TCGA/","text":"TCGA \u00b6 Description \u00b6 Reference \u00b6","title":"TCGA"},{"location":"Datasets/General%20datasets/TCGA/#tcga","text":"","title":"TCGA"},{"location":"Datasets/General%20datasets/TCGA/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/TCGA/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/UK%20Biobank/","text":"UK Biobank \u00b6 Description \u00b6 Reference \u00b6","title":"UK Biobank"},{"location":"Datasets/General%20datasets/UK%20Biobank/#uk-biobank","text":"","title":"UK Biobank"},{"location":"Datasets/General%20datasets/UK%20Biobank/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/UK%20Biobank/#reference","text":"","title":"Reference"},{"location":"Datasets/General%20datasets/cBioPortal/","text":"cBioPortal \u00b6 Description \u00b6 Reference \u00b6","title":"cBioPortal"},{"location":"Datasets/General%20datasets/cBioPortal/#cbioportal","text":"","title":"cBioPortal"},{"location":"Datasets/General%20datasets/cBioPortal/#description","text":"","title":"Description"},{"location":"Datasets/General%20datasets/cBioPortal/#reference","text":"","title":"Reference"},{"location":"Datasets/Inhouse%20datasets/ALL%20cohort/","text":"ALL cohort \u00b6 Description \u00b6 Reference \u00b6","title":"ALL cohort"},{"location":"Datasets/Inhouse%20datasets/ALL%20cohort/#all-cohort","text":"","title":"ALL cohort"},{"location":"Datasets/Inhouse%20datasets/ALL%20cohort/#description","text":"","title":"Description"},{"location":"Datasets/Inhouse%20datasets/ALL%20cohort/#reference","text":"","title":"Reference"},{"location":"Datasets/Inhouse%20datasets/Damage%20maps/","text":"Damage maps \u00b6 Description \u00b6 Reference \u00b6","title":"Damage maps"},{"location":"Datasets/Inhouse%20datasets/Damage%20maps/#damage-maps","text":"","title":"Damage maps"},{"location":"Datasets/Inhouse%20datasets/Damage%20maps/#description","text":"","title":"Description"},{"location":"Datasets/Inhouse%20datasets/Damage%20maps/#reference","text":"","title":"Reference"},{"location":"Datasets/Inhouse%20datasets/Nanopore%20data/","text":"Nanopore data \u00b6 Description \u00b6 Reference \u00b6","title":"Nanopore data"},{"location":"Datasets/Inhouse%20datasets/Nanopore%20data/#nanopore-data","text":"","title":"Nanopore data"},{"location":"Datasets/Inhouse%20datasets/Nanopore%20data/#description","text":"","title":"Description"},{"location":"Datasets/Inhouse%20datasets/Nanopore%20data/#reference","text":"","title":"Reference"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Rhabdoid%20cohort/","text":"Pediatric Rhabdoid cohort \u00b6 Description \u00b6 Reference \u00b6","title":"Pediatric Rhabdoid cohort"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Rhabdoid%20cohort/#pediatric-rhabdoid-cohort","text":"","title":"Pediatric Rhabdoid cohort"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Rhabdoid%20cohort/#description","text":"","title":"Description"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Rhabdoid%20cohort/#reference","text":"","title":"Reference"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Secondary%20neoplasms/","text":"Pediatric Secondary neoplasms \u00b6 Description \u00b6 Reference \u00b6","title":"Pediatric Secondary neoplasms"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Secondary%20neoplasms/#pediatric-secondary-neoplasms","text":"","title":"Pediatric Secondary neoplasms"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Secondary%20neoplasms/#description","text":"","title":"Description"},{"location":"Datasets/Inhouse%20datasets/Pediatric%20Secondary%20neoplasms/#reference","text":"","title":"Reference"},{"location":"Datasets/Other%20data/Canonical%20transcripts/","text":"Canonical transcripts \u00b6 Description \u00b6 Reference \u00b6","title":"Canonical transcripts"},{"location":"Datasets/Other%20data/Canonical%20transcripts/#canonical-transcripts","text":"","title":"Canonical transcripts"},{"location":"Datasets/Other%20data/Canonical%20transcripts/#description","text":"","title":"Description"},{"location":"Datasets/Other%20data/Canonical%20transcripts/#reference","text":"","title":"Reference"},{"location":"Datasets/Other%20data/Genomic%20regions/","text":"Genomic regions \u00b6 Description \u00b6 Reference \u00b6","title":"Genomic regions"},{"location":"Datasets/Other%20data/Genomic%20regions/#genomic-regions","text":"","title":"Genomic regions"},{"location":"Datasets/Other%20data/Genomic%20regions/#description","text":"","title":"Description"},{"location":"Datasets/Other%20data/Genomic%20regions/#reference","text":"","title":"Reference"},{"location":"Datasets/Other%20data/Nmdetective/","text":"Nmdetective \u00b6 Description \u00b6 Reference \u00b6","title":"Nmdetective"},{"location":"Datasets/Other%20data/Nmdetective/#nmdetective","text":"","title":"Nmdetective"},{"location":"Datasets/Other%20data/Nmdetective/#description","text":"","title":"Description"},{"location":"Datasets/Other%20data/Nmdetective/#reference","text":"","title":"Reference"},{"location":"Datasets/Other%20data/Reference%20genomes/","text":"Reference genomes \u00b6 Description \u00b6 Reference \u00b6","title":"Reference genomes"},{"location":"Datasets/Other%20data/Reference%20genomes/#reference-genomes","text":"","title":"Reference genomes"},{"location":"Datasets/Other%20data/Reference%20genomes/#description","text":"","title":"Description"},{"location":"Datasets/Other%20data/Reference%20genomes/#reference","text":"","title":"Reference"},{"location":"Plots%20and%20scripts/Intogen-BoostDM%20plots/","text":"Intogen-BoostDM plots \u00b6 Description \u00b6 Reference \u00b6","title":"Intogen-BoostDM plots"},{"location":"Plots%20and%20scripts/Intogen-BoostDM%20plots/#intogen-boostdm-plots","text":"","title":"Intogen-BoostDM plots"},{"location":"Plots%20and%20scripts/Intogen-BoostDM%20plots/#description","text":"","title":"Description"},{"location":"Plots%20and%20scripts/Intogen-BoostDM%20plots/#reference","text":"","title":"Reference"},{"location":"Plots%20and%20scripts/Mutational%20profile/","text":"Mutational profile \u00b6 The function plot_signature will allow you to plot the mutational profile of a sample given the vector of 96 channels (see y axis in the example figure) with the frequencies of each nucleotide change. It takes as input the the vector with the mutations frequency ( profile ) and the title of the plot ( title ). The function minor_tick_labels is needed to generate the labels of the plot. Example \u00b6 Function \u00b6 import seaborn as sns import numpy as np def minor_tick_labels (): major_labels = [ 'C>A' , 'C>G' , 'C>T' , 'T>A' , 'T>C' , 'T>G' ] flanks = [ 'AA' , 'AC' , 'AG' , 'AT' , 'CA' , 'CC' , 'CG' , 'CT' , 'GA' , 'GC' , 'GG' , 'GT' , 'TA' , 'TC' , 'TG' , 'TT' ] minor_labels = [] for subs in major_labels : for flank in flanks : minor_labels . append ( flank [ 0 ] + subs [ 0 ] + flank [ 1 ]) return minor_labels def plot_signature ( profile , title = None ): \"\"\" Args: profile: 96-array in lexicographic order title: string Returns: produces the signature bar plot \"\"\" fig , ax = plt . subplots ( figsize = ( 15 , 2 )) total = np . sum ( profile ) if abs ( total - 1 ) > 0.01 : profile = profile / total sns . set ( font_scale = 1.5 ) sns . set_style ( 'white' ) # bar plot barlist = ax . bar ( range ( 96 ), profile ) color_list = [ '#72bcd4' , 'k' , 'r' , '#7e7e7e' , 'g' , '#e6add8' ] for category in range ( 6 ): for i in range ( 16 ): barlist [ category * 16 + i ] . set_color ( color_list [ category ]) ax . set_xlim ([ - 0.5 , 96 ]) ymax = np . max ( profile ) * 1.2 ax . set_ylim ( 0 , ymax ) # ax.set_ylabel('subs rel freq') labels = [ 'C>A' , 'C>G' , 'C>T' , 'T>A' , 'T>C' , 'T>G' ] major_ticks = np . arange ( 8 , 8 + 16 * 5 + 1 , 16 ) minor_ticks = np . arange ( 0.2 , 96.2 , 1 ) ax . tick_params ( length = 0 , which = 'major' , pad = 30 , labelsize = 12 ) ax . tick_params ( length = 0 , which = 'minor' , pad = 5 , labelsize = 10 ) ax . set_xticks ( major_ticks , minor = False ) ax . set_xticklabels ( labels , minor = False ) ax . set_xticks ( minor_ticks , minor = True ) ax . set_xticklabels ( minor_tick_labels (), minor = True , rotation = 90 ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'bottom' ] . set_visible ( False ) ax . spines [ 'left' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . set_title ( title , fontsize = 24 ) plt . show () Note The function normalizes the vector so that the sum of all the frequencies is equal to 1. If you want to normalize the frequencies so that the trinucleotide composition of the genomic regions from which the mutations have been obtained, you need to normalize the vector taking into account the trinucleotide composition before using the function plot_signature. Normalization of the vector \u00b6 In order to normalize the vector you will need to import from bgreference the reference genome in which the data has been sequenced. You will also need the vector with the mutations frequency ( profile ) and the directory of a file with the genomic regions from which the mutations have been obtained ( regions_file_dir ), with at least the columns: CHROMOSOME , START , END . Needed functions \u00b6 from itertools import product import pandas as pd import numpy as np cb = dict ( zip ( 'ACGT' , 'TGCA' )) def triplet_index ( triplet ): a , ref , b = tuple ( list ( triplet )) s = 16 * ( ref == 'T' ) t = 4 * (( a == 'C' ) + 2 * ( a == 'G' ) + 3 * ( a == 'T' )) u = ( b == 'C' ) + 2 * ( b == 'G' ) + 3 * ( b == 'T' ) return s + t + u def sbs_format ( triplet_count ): \"\"\"Maps ref triplets to 96 SBS channel\"\"\" vector = [] for ref in 'CT' : for alt in 'ACGT' : if alt != ref : for a , b in product ( cb , repeat = 2 ): vector . append ( triplet_count [ triplet_index ( a + ref + b )]) return vector def triplets (): for ref in 'CT' : for a , b in product ( cb , repeat = 2 ): yield a + ref + b def count_triplets ( seq ): return [ seq . count ( t ) + seq . count ( rev ( t )) for t in triplets ()] def rev ( seq ): \"\"\"reverse complement of seq\"\"\" return '' . join ( list ( map ( lambda s : cb [ s ], seq [:: - 1 ]))) def get_triplet_counts_region ( regions_file_dir , reference_genome = hg38 ): \"\"\" Function to obtain the vector with 96 triplet counts given the regions file. \"\"\" regions = pd . read_csv ( regions_file_dir , sep = ' \\t ' , dtype = { 'CHROMOSOME' : 'string' }) assert ( np . all ( regions . apply ( lambda r : r [ 'END' ] - r [ 'START' ] + 1 > 0 , axis = 1 ))) regions [ 'interval' ] = regions . apply ( lambda r : ( r [ 'CHROMOSOME' ], r [ 'START' ], r [ 'END' ]), axis = 1 ) counts = np . zeros ( 32 ) for chrom , start , end in list ( regions [ 'interval' ]): try : seq = reference_genome ( chrom , start - 1 , size = end - start + 3 ) # sequence +1 nt 5' and 3' flanking nucleotides c = np . array ( count_triplets ( seq )) counts += c except Exception as e : print ( e ) return sbs_format ( list ( map ( int , counts ))) Normalization \u00b6 region_triplet_abundance = get_triplet_counts_region ( regions_file_dir ) normalized_profile = np . array ( profile ) / np . array ( region_triplet_abundance ) Reference \u00b6 Paula Gomis Ferran Mui\u00f1os","title":"Mutational profile"},{"location":"Plots%20and%20scripts/Mutational%20profile/#mutational-profile","text":"The function plot_signature will allow you to plot the mutational profile of a sample given the vector of 96 channels (see y axis in the example figure) with the frequencies of each nucleotide change. It takes as input the the vector with the mutations frequency ( profile ) and the title of the plot ( title ). The function minor_tick_labels is needed to generate the labels of the plot.","title":"Mutational profile"},{"location":"Plots%20and%20scripts/Mutational%20profile/#example","text":"","title":"Example"},{"location":"Plots%20and%20scripts/Mutational%20profile/#function","text":"import seaborn as sns import numpy as np def minor_tick_labels (): major_labels = [ 'C>A' , 'C>G' , 'C>T' , 'T>A' , 'T>C' , 'T>G' ] flanks = [ 'AA' , 'AC' , 'AG' , 'AT' , 'CA' , 'CC' , 'CG' , 'CT' , 'GA' , 'GC' , 'GG' , 'GT' , 'TA' , 'TC' , 'TG' , 'TT' ] minor_labels = [] for subs in major_labels : for flank in flanks : minor_labels . append ( flank [ 0 ] + subs [ 0 ] + flank [ 1 ]) return minor_labels def plot_signature ( profile , title = None ): \"\"\" Args: profile: 96-array in lexicographic order title: string Returns: produces the signature bar plot \"\"\" fig , ax = plt . subplots ( figsize = ( 15 , 2 )) total = np . sum ( profile ) if abs ( total - 1 ) > 0.01 : profile = profile / total sns . set ( font_scale = 1.5 ) sns . set_style ( 'white' ) # bar plot barlist = ax . bar ( range ( 96 ), profile ) color_list = [ '#72bcd4' , 'k' , 'r' , '#7e7e7e' , 'g' , '#e6add8' ] for category in range ( 6 ): for i in range ( 16 ): barlist [ category * 16 + i ] . set_color ( color_list [ category ]) ax . set_xlim ([ - 0.5 , 96 ]) ymax = np . max ( profile ) * 1.2 ax . set_ylim ( 0 , ymax ) # ax.set_ylabel('subs rel freq') labels = [ 'C>A' , 'C>G' , 'C>T' , 'T>A' , 'T>C' , 'T>G' ] major_ticks = np . arange ( 8 , 8 + 16 * 5 + 1 , 16 ) minor_ticks = np . arange ( 0.2 , 96.2 , 1 ) ax . tick_params ( length = 0 , which = 'major' , pad = 30 , labelsize = 12 ) ax . tick_params ( length = 0 , which = 'minor' , pad = 5 , labelsize = 10 ) ax . set_xticks ( major_ticks , minor = False ) ax . set_xticklabels ( labels , minor = False ) ax . set_xticks ( minor_ticks , minor = True ) ax . set_xticklabels ( minor_tick_labels (), minor = True , rotation = 90 ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'bottom' ] . set_visible ( False ) ax . spines [ 'left' ] . set_visible ( False ) ax . spines [ 'right' ] . set_visible ( False ) ax . set_title ( title , fontsize = 24 ) plt . show () Note The function normalizes the vector so that the sum of all the frequencies is equal to 1. If you want to normalize the frequencies so that the trinucleotide composition of the genomic regions from which the mutations have been obtained, you need to normalize the vector taking into account the trinucleotide composition before using the function plot_signature.","title":"Function"},{"location":"Plots%20and%20scripts/Mutational%20profile/#normalization-of-the-vector","text":"In order to normalize the vector you will need to import from bgreference the reference genome in which the data has been sequenced. You will also need the vector with the mutations frequency ( profile ) and the directory of a file with the genomic regions from which the mutations have been obtained ( regions_file_dir ), with at least the columns: CHROMOSOME , START , END .","title":"Normalization of the vector"},{"location":"Plots%20and%20scripts/Mutational%20profile/#needed-functions","text":"from itertools import product import pandas as pd import numpy as np cb = dict ( zip ( 'ACGT' , 'TGCA' )) def triplet_index ( triplet ): a , ref , b = tuple ( list ( triplet )) s = 16 * ( ref == 'T' ) t = 4 * (( a == 'C' ) + 2 * ( a == 'G' ) + 3 * ( a == 'T' )) u = ( b == 'C' ) + 2 * ( b == 'G' ) + 3 * ( b == 'T' ) return s + t + u def sbs_format ( triplet_count ): \"\"\"Maps ref triplets to 96 SBS channel\"\"\" vector = [] for ref in 'CT' : for alt in 'ACGT' : if alt != ref : for a , b in product ( cb , repeat = 2 ): vector . append ( triplet_count [ triplet_index ( a + ref + b )]) return vector def triplets (): for ref in 'CT' : for a , b in product ( cb , repeat = 2 ): yield a + ref + b def count_triplets ( seq ): return [ seq . count ( t ) + seq . count ( rev ( t )) for t in triplets ()] def rev ( seq ): \"\"\"reverse complement of seq\"\"\" return '' . join ( list ( map ( lambda s : cb [ s ], seq [:: - 1 ]))) def get_triplet_counts_region ( regions_file_dir , reference_genome = hg38 ): \"\"\" Function to obtain the vector with 96 triplet counts given the regions file. \"\"\" regions = pd . read_csv ( regions_file_dir , sep = ' \\t ' , dtype = { 'CHROMOSOME' : 'string' }) assert ( np . all ( regions . apply ( lambda r : r [ 'END' ] - r [ 'START' ] + 1 > 0 , axis = 1 ))) regions [ 'interval' ] = regions . apply ( lambda r : ( r [ 'CHROMOSOME' ], r [ 'START' ], r [ 'END' ]), axis = 1 ) counts = np . zeros ( 32 ) for chrom , start , end in list ( regions [ 'interval' ]): try : seq = reference_genome ( chrom , start - 1 , size = end - start + 3 ) # sequence +1 nt 5' and 3' flanking nucleotides c = np . array ( count_triplets ( seq )) counts += c except Exception as e : print ( e ) return sbs_format ( list ( map ( int , counts )))","title":"Needed functions"},{"location":"Plots%20and%20scripts/Mutational%20profile/#normalization","text":"region_triplet_abundance = get_triplet_counts_region ( regions_file_dir ) normalized_profile = np . array ( profile ) / np . array ( region_triplet_abundance )","title":"Normalization"},{"location":"Plots%20and%20scripts/Mutational%20profile/#reference","text":"Paula Gomis Ferran Mui\u00f1os","title":"Reference"},{"location":"Plots%20and%20scripts/Needle%20plot/","text":"Needle plot \u00b6 Description \u00b6 Reference \u00b6","title":"Needle plot"},{"location":"Plots%20and%20scripts/Needle%20plot/#needle-plot","text":"","title":"Needle plot"},{"location":"Plots%20and%20scripts/Needle%20plot/#description","text":"","title":"Description"},{"location":"Plots%20and%20scripts/Needle%20plot/#reference","text":"","title":"Reference"},{"location":"Tools/Docker/","text":"Docker \u00b6 Description \u00b6 Reference \u00b6","title":"Docker"},{"location":"Tools/Docker/#docker","text":"","title":"Docker"},{"location":"Tools/Docker/#description","text":"","title":"Description"},{"location":"Tools/Docker/#reference","text":"","title":"Reference"},{"location":"Tools/Nextflow/","text":"Nextflow \u00b6 Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of data-driven, computational pipelines written in the most common scripting languages. Usage \u00b6 To run the default installed version of Nextflow, simply load the nextflow module: $ module load nextflow $ nextflow help Usage: nextflow [ options ] COMMAND [ arg... ] For usage documentation, run nextflow help . Submitting processes as serial jobs \u00b6 Recommended for serial jobs only This section is recommended for serial jobs only. For parallel jobs, please see the Parallel jobs section below. Nextflow supports the ability to submit pipeline scripts as separate cluster jobs using the SGE executor. To enable the SGE executor, simply set to process.executor property to sge in a configuration file named nextflow.config in the job working directory. The amount of resources requested by each job submission is defined in the cluster options section, where all Univa scheduler resources are supported. For example, to run all pipeline jobs with 2 serial cores and 2GB of memory for 1 hour, create the following configuration file: process.executor='sge' process.clusterOptions='-pe smp 2 -l h_vmem=1G,h_rt=1:0:0' Setting the memory limit for serial jobs Add the -DXmx option to limit the amount of memory Nextflow can use in serial jobs. For more information regarding the Java VM memory allocation, see here. Parallel jobs \u00b6 Parallel jobs will use the in-built Apache Ignite clustering platform; Execution will be performed on the nodes requested in the submit request over MPI rather than submitting new jobs for each pipeline. Do not use the SGE executor in parallel jobs Using the SGE executor for parallel jobs causes the master job to hang until it is killed by the scheduler for exceeding walltime. This is due to Apache Ignite not being able to communicate to other pipeline scripts submitted as separate jobs. To ensure parallel jobs use Apache Ignite, add the following to the configuration file (or omit the process.executor setting): process.executor='ignite' Example jobs \u00b6 Serial job \u00b6 Here is an example job taken from the Nextflow website to submit each process in the input.nf file as a new cluster job with 1 core and 1GB of memory. Ensure the cumulative runtime across all processes does not exceed the runtime requested in the master job: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G module load nextflow nextflow -DXmx=1G \\ -C nextflow.config \\ run input.nf Parallel job \u00b6 Here is an example job taken from the Nextflow website to run each process in the input.nf file using 48 cores across 2 sdv nodes with Apache Ignite: #!/bin/bash #$ -cwd #$ -j y #$ -pe parallel 48 #$ -l infiniband=sdv-i #$ -l h_rt=240:0:0 module load nextflow openmpi mpirun --pernode \\ nextflow run input.nf \\ -with-mpi Links \u00b6 Nextflow documentation Nextflow basic pipeline example Nextflow presentation videos Nextflow community support Nextflow MPI Apache Ignite Reference \u00b6 Jordi Deu-Pons Carlos L\u00f3pez Elorduy Miguel Grau","title":"Nextflow"},{"location":"Tools/Nextflow/#nextflow","text":"Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of data-driven, computational pipelines written in the most common scripting languages.","title":"Nextflow"},{"location":"Tools/Nextflow/#usage","text":"To run the default installed version of Nextflow, simply load the nextflow module: $ module load nextflow $ nextflow help Usage: nextflow [ options ] COMMAND [ arg... ] For usage documentation, run nextflow help .","title":"Usage"},{"location":"Tools/Nextflow/#submitting-processes-as-serial-jobs","text":"Recommended for serial jobs only This section is recommended for serial jobs only. For parallel jobs, please see the Parallel jobs section below. Nextflow supports the ability to submit pipeline scripts as separate cluster jobs using the SGE executor. To enable the SGE executor, simply set to process.executor property to sge in a configuration file named nextflow.config in the job working directory. The amount of resources requested by each job submission is defined in the cluster options section, where all Univa scheduler resources are supported. For example, to run all pipeline jobs with 2 serial cores and 2GB of memory for 1 hour, create the following configuration file: process.executor='sge' process.clusterOptions='-pe smp 2 -l h_vmem=1G,h_rt=1:0:0' Setting the memory limit for serial jobs Add the -DXmx option to limit the amount of memory Nextflow can use in serial jobs. For more information regarding the Java VM memory allocation, see here.","title":"Submitting processes as serial jobs"},{"location":"Tools/Nextflow/#parallel-jobs","text":"Parallel jobs will use the in-built Apache Ignite clustering platform; Execution will be performed on the nodes requested in the submit request over MPI rather than submitting new jobs for each pipeline. Do not use the SGE executor in parallel jobs Using the SGE executor for parallel jobs causes the master job to hang until it is killed by the scheduler for exceeding walltime. This is due to Apache Ignite not being able to communicate to other pipeline scripts submitted as separate jobs. To ensure parallel jobs use Apache Ignite, add the following to the configuration file (or omit the process.executor setting): process.executor='ignite'","title":"Parallel jobs"},{"location":"Tools/Nextflow/#example-jobs","text":"","title":"Example jobs"},{"location":"Tools/Nextflow/#serial-job","text":"Here is an example job taken from the Nextflow website to submit each process in the input.nf file as a new cluster job with 1 core and 1GB of memory. Ensure the cumulative runtime across all processes does not exceed the runtime requested in the master job: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G module load nextflow nextflow -DXmx=1G \\ -C nextflow.config \\ run input.nf","title":"Serial job"},{"location":"Tools/Nextflow/#parallel-job","text":"Here is an example job taken from the Nextflow website to run each process in the input.nf file using 48 cores across 2 sdv nodes with Apache Ignite: #!/bin/bash #$ -cwd #$ -j y #$ -pe parallel 48 #$ -l infiniband=sdv-i #$ -l h_rt=240:0:0 module load nextflow openmpi mpirun --pernode \\ nextflow run input.nf \\ -with-mpi","title":"Parallel job"},{"location":"Tools/Nextflow/#links","text":"Nextflow documentation Nextflow basic pipeline example Nextflow presentation videos Nextflow community support Nextflow MPI Apache Ignite","title":"Links"},{"location":"Tools/Nextflow/#reference","text":"Jordi Deu-Pons Carlos L\u00f3pez Elorduy Miguel Grau","title":"Reference"},{"location":"Tools/BBG-tools/BGconfig/","text":"BGconfig \u00b6 Description \u00b6 Reference \u00b6","title":"BGconfig"},{"location":"Tools/BBG-tools/BGconfig/#bgconfig","text":"","title":"BGconfig"},{"location":"Tools/BBG-tools/BGconfig/#description","text":"","title":"Description"},{"location":"Tools/BBG-tools/BGconfig/#reference","text":"","title":"Reference"},{"location":"Tools/BBG-tools/BGdata/","text":"BGdata \u00b6 Description \u00b6 Reference \u00b6","title":"BGdata"},{"location":"Tools/BBG-tools/BGdata/#bgdata","text":"","title":"BGdata"},{"location":"Tools/BBG-tools/BGdata/#description","text":"","title":"Description"},{"location":"Tools/BBG-tools/BGdata/#reference","text":"","title":"Reference"},{"location":"Tools/BBG-tools/BGlogs/","text":"BGlogs \u00b6 Description \u00b6 Reference \u00b6","title":"BGlogs"},{"location":"Tools/BBG-tools/BGlogs/#bglogs","text":"","title":"BGlogs"},{"location":"Tools/BBG-tools/BGlogs/#description","text":"","title":"Description"},{"location":"Tools/BBG-tools/BGlogs/#reference","text":"","title":"Reference"},{"location":"Tools/BBG-tools/BGpack/","text":"BGpack \u00b6 Description \u00b6 Reference \u00b6","title":"BGpack"},{"location":"Tools/BBG-tools/BGpack/#bgpack","text":"","title":"BGpack"},{"location":"Tools/BBG-tools/BGpack/#description","text":"","title":"Description"},{"location":"Tools/BBG-tools/BGpack/#reference","text":"","title":"Reference"},{"location":"Tools/BBG-tools/BGreference/","text":"BgReference \u00b6 Description \u00b6 BgReference is a library to fast retrive Genome Reference partial sequences. Instalation \u00b6 conda install -c conda-forge -c bbglab bgreference or pip install bgreference Examples \u00b6 from bgreference import hg19, hg38 # Get 10 bases from chromosome one build hg19 hg19 ( '1' , 12345 , size = 10 ) # Get the sequence of the whole chromosome hg19 ( '1' , ( 1 ) , size = None ) # You can use synonymous sequence names hg19 ( 2 , 23456 ) hg19 ( '2' , 23456 ) hg19 ( 'chr2' , 23456 ) hg19 ( 'MT' , 234 , size = 3 ) hg19 ( 'chrM' , 234 , size = 3 ) hg19 ( 'chrMT' , 234 , size = 3 ) Repository \u00b6 Click here to see the repository of BgReference . Reference \u00b6 Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy Paula Gomis","title":"BgReference"},{"location":"Tools/BBG-tools/BGreference/#bgreference","text":"","title":"BgReference"},{"location":"Tools/BBG-tools/BGreference/#description","text":"BgReference is a library to fast retrive Genome Reference partial sequences.","title":"Description"},{"location":"Tools/BBG-tools/BGreference/#instalation","text":"conda install -c conda-forge -c bbglab bgreference or pip install bgreference","title":"Instalation"},{"location":"Tools/BBG-tools/BGreference/#examples","text":"from bgreference import hg19, hg38 # Get 10 bases from chromosome one build hg19 hg19 ( '1' , 12345 , size = 10 ) # Get the sequence of the whole chromosome hg19 ( '1' , ( 1 ) , size = None ) # You can use synonymous sequence names hg19 ( 2 , 23456 ) hg19 ( '2' , 23456 ) hg19 ( 'chr2' , 23456 ) hg19 ( 'MT' , 234 , size = 3 ) hg19 ( 'chrM' , 234 , size = 3 ) hg19 ( 'chrMT' , 234 , size = 3 )","title":"Examples"},{"location":"Tools/BBG-tools/BGreference/#repository","text":"Click here to see the repository of BgReference .","title":"Repository"},{"location":"Tools/BBG-tools/BGreference/#reference","text":"Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy Paula Gomis","title":"Reference"},{"location":"Tools/BBG-tools/BGsignature/","text":"BGsignature \u00b6 Description \u00b6 Reference \u00b6","title":"BGsignature"},{"location":"Tools/BBG-tools/BGsignature/#bgsignature","text":"","title":"BGsignature"},{"location":"Tools/BBG-tools/BGsignature/#description","text":"","title":"Description"},{"location":"Tools/BBG-tools/BGsignature/#reference","text":"","title":"Reference"},{"location":"Tools/BBG-tools/BGvep/","text":"BGvep \u00b6 Description \u00b6 Reference \u00b6","title":"BGvep"},{"location":"Tools/BBG-tools/BGvep/#bgvep","text":"","title":"BGvep"},{"location":"Tools/BBG-tools/BGvep/#description","text":"","title":"Description"},{"location":"Tools/BBG-tools/BGvep/#reference","text":"","title":"Reference"},{"location":"Tools/BBG-tools/OpenVariant/","text":"OpenVariant \u00b6 Description \u00b6 OpenVariant is a comprehensive Python package that provides different functionalities to read, parse and operate different multiple input file formats (e. g. tsv , csv , vcf , maf , bed ), being able to build an unified output with a proper annotation file structure. Usage \u00b6 Click here to see the installation guide and the complete documentation of OpenVariant. When working with OpenVariant, we need to distinguish 3 different types of files: input files and annotation file , which are provided by the user and output file , which will returned from the function. Input files will be the group of files in different formats (e.g. tsv, csv, vcf, maf, bed) that we want to parse. Annotation file is a YAML file which describes how the input files are processed and how the output file will look like. Output files are generated by OpenVariant and they are the result of the process. Functions \u00b6 OpenVariant has several functions to perform different tasks: find_files : Find files with a given pattern name in a given folder. Variant : Parse an input file through the annotation file. It will generate an object which you can apply different functionalities cat : It will show on the stdout (standard out) the whole parsed output. group_by : It will generate an iterator that will contain three variables: group_key (the value of each group), group_result (a list of all rows that pertain to each group) and command (if it uses the script parameter or not). It will group the parsed result for each different value of the specified key_by . count : It returns the number of rows that matches a specific conditions. Click here to see several examples of each of the functions from OpenVariant. Parameters \u00b6 The different options and parameters of these functions are specified in the annotation file , which has several required and optional parameters. Click here to learn about the parameters in the annotation file and a to see a template of the annotation file . Reference \u00b6 David Mart\u00ednez Paula Gomis","title":"OpenVariant"},{"location":"Tools/BBG-tools/OpenVariant/#openvariant","text":"","title":"OpenVariant"},{"location":"Tools/BBG-tools/OpenVariant/#description","text":"OpenVariant is a comprehensive Python package that provides different functionalities to read, parse and operate different multiple input file formats (e. g. tsv , csv , vcf , maf , bed ), being able to build an unified output with a proper annotation file structure.","title":"Description"},{"location":"Tools/BBG-tools/OpenVariant/#usage","text":"Click here to see the installation guide and the complete documentation of OpenVariant. When working with OpenVariant, we need to distinguish 3 different types of files: input files and annotation file , which are provided by the user and output file , which will returned from the function. Input files will be the group of files in different formats (e.g. tsv, csv, vcf, maf, bed) that we want to parse. Annotation file is a YAML file which describes how the input files are processed and how the output file will look like. Output files are generated by OpenVariant and they are the result of the process.","title":"Usage"},{"location":"Tools/BBG-tools/OpenVariant/#functions","text":"OpenVariant has several functions to perform different tasks: find_files : Find files with a given pattern name in a given folder. Variant : Parse an input file through the annotation file. It will generate an object which you can apply different functionalities cat : It will show on the stdout (standard out) the whole parsed output. group_by : It will generate an iterator that will contain three variables: group_key (the value of each group), group_result (a list of all rows that pertain to each group) and command (if it uses the script parameter or not). It will group the parsed result for each different value of the specified key_by . count : It returns the number of rows that matches a specific conditions. Click here to see several examples of each of the functions from OpenVariant.","title":"Functions"},{"location":"Tools/BBG-tools/OpenVariant/#parameters","text":"The different options and parameters of these functions are specified in the annotation file , which has several required and optional parameters. Click here to learn about the parameters in the annotation file and a to see a template of the annotation file .","title":"Parameters"},{"location":"Tools/BBG-tools/OpenVariant/#reference","text":"David Mart\u00ednez Paula Gomis","title":"Reference"},{"location":"Tools/Sequencing%20tools/Bedtools/","text":"Bedtools \u00b6 The Bedtools suite is a collection of tools for a wide-range of genomics analysis tasks. Installation \u00b6 You can check the installation guide here . Usage \u00b6 To run the default installed version of Bedtools, simply load the bedtools module: $ bedtools -h Usage: bedtools <subcommand> [ options ] For full usage documentation, run bedtools -h. Example job \u00b6 Serial job \u00b6 Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G # Report the base-pair overlap between the features in two BED files. bedtools intersect -a reads.bed -b genes.bed Links \u00b6 Bedtools GitHub Bedtools documentation Reference \u00b6 Hania Kranas","title":"Bedtools"},{"location":"Tools/Sequencing%20tools/Bedtools/#bedtools","text":"The Bedtools suite is a collection of tools for a wide-range of genomics analysis tasks.","title":"Bedtools"},{"location":"Tools/Sequencing%20tools/Bedtools/#installation","text":"You can check the installation guide here .","title":"Installation"},{"location":"Tools/Sequencing%20tools/Bedtools/#usage","text":"To run the default installed version of Bedtools, simply load the bedtools module: $ bedtools -h Usage: bedtools <subcommand> [ options ] For full usage documentation, run bedtools -h.","title":"Usage"},{"location":"Tools/Sequencing%20tools/Bedtools/#example-job","text":"","title":"Example job"},{"location":"Tools/Sequencing%20tools/Bedtools/#serial-job","text":"Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G # Report the base-pair overlap between the features in two BED files. bedtools intersect -a reads.bed -b genes.bed","title":"Serial job"},{"location":"Tools/Sequencing%20tools/Bedtools/#links","text":"Bedtools GitHub Bedtools documentation","title":"Links"},{"location":"Tools/Sequencing%20tools/Bedtools/#reference","text":"Hania Kranas","title":"Reference"},{"location":"Tools/Sequencing%20tools/Bowtie2/","text":"Bowtie2 \u00b6 Bowtie2 is an ultra fast and memory-efficient tool for aligning sequencing reads to long reference sequences. Installation \u00b6 You can check the installation documentation here . Conda \u00b6 conda install -c bioconda bowtie2 Package manager \u00b6 sudo apt update sudo apt install bowtie2 Manually on Ubuntu/Linux \u00b6 Create and go to install directory cd $HOME /tools/bowtie2/ Download Ubuntu/Linux version wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.4.2/bowtie2-2.4.2-sra-linux-x86_64.zip/download Decompress unzip download Add location to system PATH export PATH = $HOME /tools/bowtie2/bowtie2-2.4.2-sra-linux-x86_64: $PATH Check installation \u00b6 bowtie2 --help Usage \u00b6 $ bowtie2 -h Usage: bowtie2 [ options ] * -x <bt2-idx> { -1 <m1> -2 <m2> | -U <r> | --interleaved <i> } -S [ <sam> ] For full usage documentation, run bowtie2 -h . Example job \u00b6 Serial job \u00b6 Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G # Prepare example genomes in <inputDir> # Output is stored in <outputDir> bowtie2-build <inputDir> <outputDir> bowtie2-inspect <outputDir> Links \u00b6 Bowtie2 GitHub Bowtie2 example Reference \u00b6 Hania Kranas","title":"Bowtie2"},{"location":"Tools/Sequencing%20tools/Bowtie2/#bowtie2","text":"Bowtie2 is an ultra fast and memory-efficient tool for aligning sequencing reads to long reference sequences.","title":"Bowtie2"},{"location":"Tools/Sequencing%20tools/Bowtie2/#installation","text":"You can check the installation documentation here .","title":"Installation"},{"location":"Tools/Sequencing%20tools/Bowtie2/#conda","text":"conda install -c bioconda bowtie2","title":"Conda"},{"location":"Tools/Sequencing%20tools/Bowtie2/#package-manager","text":"sudo apt update sudo apt install bowtie2","title":"Package manager"},{"location":"Tools/Sequencing%20tools/Bowtie2/#manually-on-ubuntulinux","text":"Create and go to install directory cd $HOME /tools/bowtie2/ Download Ubuntu/Linux version wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.4.2/bowtie2-2.4.2-sra-linux-x86_64.zip/download Decompress unzip download Add location to system PATH export PATH = $HOME /tools/bowtie2/bowtie2-2.4.2-sra-linux-x86_64: $PATH","title":"Manually on Ubuntu/Linux"},{"location":"Tools/Sequencing%20tools/Bowtie2/#check-installation","text":"bowtie2 --help","title":"Check installation"},{"location":"Tools/Sequencing%20tools/Bowtie2/#usage","text":"$ bowtie2 -h Usage: bowtie2 [ options ] * -x <bt2-idx> { -1 <m1> -2 <m2> | -U <r> | --interleaved <i> } -S [ <sam> ] For full usage documentation, run bowtie2 -h .","title":"Usage"},{"location":"Tools/Sequencing%20tools/Bowtie2/#example-job","text":"","title":"Example job"},{"location":"Tools/Sequencing%20tools/Bowtie2/#serial-job","text":"Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G # Prepare example genomes in <inputDir> # Output is stored in <outputDir> bowtie2-build <inputDir> <outputDir> bowtie2-inspect <outputDir>","title":"Serial job"},{"location":"Tools/Sequencing%20tools/Bowtie2/#links","text":"Bowtie2 GitHub Bowtie2 example","title":"Links"},{"location":"Tools/Sequencing%20tools/Bowtie2/#reference","text":"Hania Kranas","title":"Reference"},{"location":"Tools/Sequencing%20tools/Cutadapt/","text":"Cutadapt \u00b6 Description \u00b6 Reference \u00b6","title":"Cutadapt"},{"location":"Tools/Sequencing%20tools/Cutadapt/#cutadapt","text":"","title":"Cutadapt"},{"location":"Tools/Sequencing%20tools/Cutadapt/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/Cutadapt/#reference","text":"","title":"Reference"},{"location":"Tools/Sequencing%20tools/Fastqc/","text":"Fastqc \u00b6 FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. Installation \u00b6 apt-get \u00b6 sudo apt-get update sudo apt-get install fastqc apt \u00b6 sudo apt update sudo apt install fastqc Usage \u00b6 $ fastqc --help FastQC - A high throughput sequence QC analysis tool SYNOPSIS fastqc seqfile1 seqfile2 .. seqfileN fastqc [ -o output dir ] [ -- ( no ) extract ] [ -f fastq | bam | sam ] [ -c contaminant file ] seqfile1 .. seqfileN Example job \u00b6 Serial job \u00b6 Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G fastqc raw_data.fastq.gz raw_data2.fastq.gz Viewing the Fastqc results To view the Fastqc results, you may open the fastqc_report.html file in a web browser or the summary.txt file (located in the zipped output archive) on the command line. For assistance copying files to your local machine, please see the Moving Data page. Links \u00b6 FastQC website FastQC manual FastQC video tutorial Reference \u00b6 Hania Kranas","title":"Fastqc"},{"location":"Tools/Sequencing%20tools/Fastqc/#fastqc","text":"FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines.","title":"Fastqc"},{"location":"Tools/Sequencing%20tools/Fastqc/#installation","text":"","title":"Installation"},{"location":"Tools/Sequencing%20tools/Fastqc/#apt-get","text":"sudo apt-get update sudo apt-get install fastqc","title":"apt-get"},{"location":"Tools/Sequencing%20tools/Fastqc/#apt","text":"sudo apt update sudo apt install fastqc","title":"apt"},{"location":"Tools/Sequencing%20tools/Fastqc/#usage","text":"$ fastqc --help FastQC - A high throughput sequence QC analysis tool SYNOPSIS fastqc seqfile1 seqfile2 .. seqfileN fastqc [ -o output dir ] [ -- ( no ) extract ] [ -f fastq | bam | sam ] [ -c contaminant file ] seqfile1 .. seqfileN","title":"Usage"},{"location":"Tools/Sequencing%20tools/Fastqc/#example-job","text":"","title":"Example job"},{"location":"Tools/Sequencing%20tools/Fastqc/#serial-job","text":"Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G fastqc raw_data.fastq.gz raw_data2.fastq.gz Viewing the Fastqc results To view the Fastqc results, you may open the fastqc_report.html file in a web browser or the summary.txt file (located in the zipped output archive) on the command line. For assistance copying files to your local machine, please see the Moving Data page.","title":"Serial job"},{"location":"Tools/Sequencing%20tools/Fastqc/#links","text":"FastQC website FastQC manual FastQC video tutorial","title":"Links"},{"location":"Tools/Sequencing%20tools/Fastqc/#reference","text":"Hania Kranas","title":"Reference"},{"location":"Tools/Sequencing%20tools/Samtools/","text":"SAMtools \u00b6 SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. Installation \u00b6 Conda \u00b6 conda install -c bioconda samtools Manual \u00b6 For the manual installation, you can find the instructions here . Usage \u00b6 samtools view -b -S -o genome_reads_aligned.bam genome_reads_aligned.sam Core Usage To ensure that SAMtools uses the correct number of cores, the -@ ${NSLOTS} option should be used on commands that support it. Example job \u00b6 Serial job \u00b6 Here is an example job running on 4 cores and 8GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 4 #$ -l h_rt=1:0:0 #$ -l h_vmem=2G samtools view -@ ${ NSLOTS } -b -S -o genome_reads_aligned.bam \\ genome_reads_aligned.sam samtools sort -@ ${ NSLOTS } genome_reads_aligned.bam \\ > genome_reads_aligned.sorted.bam samtools index genome_reads_aligned.sorted.bam samtools mpileup -g -f ref_genome_1K.fna genome_reads_aligned.sorted.bam \\ > genome_variants.bcf Links \u00b6 Samtools GitHub Samtools website Samtools documentation Reference \u00b6 Hania Kranas","title":"SAMtools"},{"location":"Tools/Sequencing%20tools/Samtools/#samtools","text":"SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments.","title":"SAMtools"},{"location":"Tools/Sequencing%20tools/Samtools/#installation","text":"","title":"Installation"},{"location":"Tools/Sequencing%20tools/Samtools/#conda","text":"conda install -c bioconda samtools","title":"Conda"},{"location":"Tools/Sequencing%20tools/Samtools/#manual","text":"For the manual installation, you can find the instructions here .","title":"Manual"},{"location":"Tools/Sequencing%20tools/Samtools/#usage","text":"samtools view -b -S -o genome_reads_aligned.bam genome_reads_aligned.sam Core Usage To ensure that SAMtools uses the correct number of cores, the -@ ${NSLOTS} option should be used on commands that support it.","title":"Usage"},{"location":"Tools/Sequencing%20tools/Samtools/#example-job","text":"","title":"Example job"},{"location":"Tools/Sequencing%20tools/Samtools/#serial-job","text":"Here is an example job running on 4 cores and 8GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 4 #$ -l h_rt=1:0:0 #$ -l h_vmem=2G samtools view -@ ${ NSLOTS } -b -S -o genome_reads_aligned.bam \\ genome_reads_aligned.sam samtools sort -@ ${ NSLOTS } genome_reads_aligned.bam \\ > genome_reads_aligned.sorted.bam samtools index genome_reads_aligned.sorted.bam samtools mpileup -g -f ref_genome_1K.fna genome_reads_aligned.sorted.bam \\ > genome_variants.bcf","title":"Serial job"},{"location":"Tools/Sequencing%20tools/Samtools/#links","text":"Samtools GitHub Samtools website Samtools documentation","title":"Links"},{"location":"Tools/Sequencing%20tools/Samtools/#reference","text":"Hania Kranas","title":"Reference"},{"location":"Tools/Sequencing%20tools/Trimmomatic/","text":"Trimmomatic \u00b6 Description \u00b6 Reference \u00b6","title":"Trimmomatic"},{"location":"Tools/Sequencing%20tools/Trimmomatic/#trimmomatic","text":"","title":"Trimmomatic"},{"location":"Tools/Sequencing%20tools/Trimmomatic/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/Trimmomatic/#reference","text":"","title":"Reference"},{"location":"Tools/Sequencing%20tools/Vep/","text":"Ensembl-VEP \u00b6 VEP determines the effect of your variants (insertions, deletions and structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions. Usage \u00b6 In order to install VEP, you can follow the installation guide . Once it is installed, simply load the ensembl-vep module: $ vep Usage: ./vep [--cache|--offline|--database] [arguments] Basic options ============= --help Display this message and quit -i | --input_file Input file -o | --output_file Output file --force_overwrite Force overwriting of output file --species [species] Species to use [default: \"human\"] --everything Shortcut switch to turn on commonly used options. See web documentation for details [default: off] --fork [num_forks] Use forking to improve script runtime For full option documentation see here . Instructions on how to download and use cached files can be found here . To enable offline mode and use of the cache, pass the --offline and --cache flags. Example job \u00b6 Serial job \u00b6 Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G vep -i homo_sapiens_GRCh38.vcf \\ --cache \\ --offline \\ --output_file results Links \u00b6 Ensembl-VEP documentation Ensembl-VEP tutorial Ensembl-VEP examples Ensembl-VEP web application Reference \u00b6 To be added","title":"Vep"},{"location":"Tools/Sequencing%20tools/Vep/#ensembl-vep","text":"VEP determines the effect of your variants (insertions, deletions and structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions.","title":"Ensembl-VEP"},{"location":"Tools/Sequencing%20tools/Vep/#usage","text":"In order to install VEP, you can follow the installation guide . Once it is installed, simply load the ensembl-vep module: $ vep Usage: ./vep [--cache|--offline|--database] [arguments] Basic options ============= --help Display this message and quit -i | --input_file Input file -o | --output_file Output file --force_overwrite Force overwriting of output file --species [species] Species to use [default: \"human\"] --everything Shortcut switch to turn on commonly used options. See web documentation for details [default: off] --fork [num_forks] Use forking to improve script runtime For full option documentation see here . Instructions on how to download and use cached files can be found here . To enable offline mode and use of the cache, pass the --offline and --cache flags.","title":"Usage"},{"location":"Tools/Sequencing%20tools/Vep/#example-job","text":"","title":"Example job"},{"location":"Tools/Sequencing%20tools/Vep/#serial-job","text":"Here is an example job running on 1 core and 1GB of memory: #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G vep -i homo_sapiens_GRCh38.vcf \\ --cache \\ --offline \\ --output_file results","title":"Serial job"},{"location":"Tools/Sequencing%20tools/Vep/#links","text":"Ensembl-VEP documentation Ensembl-VEP tutorial Ensembl-VEP examples Ensembl-VEP web application","title":"Links"},{"location":"Tools/Sequencing%20tools/Vep/#reference","text":"To be added","title":"Reference"},{"location":"Tools/Sequencing%20tools/pysam/","text":"pysam \u00b6 Description \u00b6 Reference \u00b6","title":"pysam"},{"location":"Tools/Sequencing%20tools/pysam/#pysam","text":"","title":"pysam"},{"location":"Tools/Sequencing%20tools/pysam/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/pysam/#reference","text":"","title":"Reference"},{"location":"Tools/Sequencing%20tools/Variant%20callers/MosaicForecast/","text":"MosaicForecast \u00b6 Description \u00b6 Reference \u00b6","title":"MosaicForecast"},{"location":"Tools/Sequencing%20tools/Variant%20callers/MosaicForecast/#mosaicforecast","text":"","title":"MosaicForecast"},{"location":"Tools/Sequencing%20tools/Variant%20callers/MosaicForecast/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/Variant%20callers/MosaicForecast/#reference","text":"","title":"Reference"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Mutect2/","text":"Mutect2 \u00b6 Description \u00b6 Reference \u00b6","title":"Mutect2"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Mutect2/#mutect2","text":"","title":"Mutect2"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Mutect2/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Mutect2/#reference","text":"","title":"Reference"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Platinum/","text":"Platinum \u00b6 Description \u00b6 Reference \u00b6","title":"Platinum"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Platinum/#platinum","text":"","title":"Platinum"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Platinum/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Platinum/#reference","text":"","title":"Reference"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Sarek/","text":"Sarek \u00b6 Description \u00b6 Reference \u00b6","title":"Sarek"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Sarek/#sarek","text":"","title":"Sarek"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Sarek/#description","text":"","title":"Description"},{"location":"Tools/Sequencing%20tools/Variant%20callers/Sarek/#reference","text":"","title":"Reference"},{"location":"Tools/Signature%20tools/DeconstructSigs/","text":"DeconstructSigs \u00b6 Description \u00b6 Reference \u00b6","title":"DeconstructSigs"},{"location":"Tools/Signature%20tools/DeconstructSigs/#deconstructsigs","text":"","title":"DeconstructSigs"},{"location":"Tools/Signature%20tools/DeconstructSigs/#description","text":"","title":"Description"},{"location":"Tools/Signature%20tools/DeconstructSigs/#reference","text":"","title":"Reference"},{"location":"Tools/Signature%20tools/SigProfiler/","text":"SigProfiler \u00b6 Description \u00b6 Reference \u00b6","title":"SigProfiler"},{"location":"Tools/Signature%20tools/SigProfiler/#sigprofiler","text":"","title":"SigProfiler"},{"location":"Tools/Signature%20tools/SigProfiler/#description","text":"","title":"Description"},{"location":"Tools/Signature%20tools/SigProfiler/#reference","text":"","title":"Reference"},{"location":"Tools/Signature%20tools/SigProfilerJulia/","text":"SigProfilerJulia \u00b6 Description \u00b6 Reference \u00b6","title":"SigProfilerJulia"},{"location":"Tools/Signature%20tools/SigProfilerJulia/#sigprofilerjulia","text":"","title":"SigProfilerJulia"},{"location":"Tools/Signature%20tools/SigProfilerJulia/#description","text":"","title":"Description"},{"location":"Tools/Signature%20tools/SigProfilerJulia/#reference","text":"","title":"Reference"},{"location":"Tools/Signature%20tools/mSigHDP/","text":"mSigHDP \u00b6 Description \u00b6 Reference \u00b6","title":"mSigHDP"},{"location":"Tools/Signature%20tools/mSigHDP/#msighdp","text":"","title":"mSigHDP"},{"location":"Tools/Signature%20tools/mSigHDP/#description","text":"","title":"Description"},{"location":"Tools/Signature%20tools/mSigHDP/#reference","text":"","title":"Reference"},{"location":"Tools/Signature%20tools/mSignAct/","text":"mSignAct \u00b6 Description \u00b6 Reference \u00b6","title":"mSignAct"},{"location":"Tools/Signature%20tools/mSignAct/#msignact","text":"","title":"mSignAct"},{"location":"Tools/Signature%20tools/mSignAct/#description","text":"","title":"Description"},{"location":"Tools/Signature%20tools/mSignAct/#reference","text":"","title":"Reference"},{"location":"Tools/Singularity/Building%20containers/","text":"Building containers \u00b6 Singularity containers are built from a definition file which allows the container to be built identically by anyone possessing the file. Root privileges required to build a container Note that the process of building a container requires elevated privileges One primary task per container HPC containers are designed to perform one primary task, and should consist of a main application and its dependencies, in a similar way to how module files are provided. Since containers are lightweight, you can use separate containers instead of general purpose containers containing a collection of applications. This improves supportability, performance and reproducibility. Building a Singularity container from scratch \u00b6 Building from scratch gives complete control over the contents of the container, including operating system and packages. Certain packages may only be available for a specific version of Linux (i.e. compatibility issues) so being able to build a container from scratch enhances research capability. The following example demonstrates building an Ubuntu 20 (focal) container using definition file ubuntu20_helloworld.def that installs the python3 package via the Ubuntu package manager: BootStrap: debootstrap OSVersion: focal MirrorURL: http://us.archive.ubuntu.com/ubuntu/ %post apt-get install --yes python3 %runscript python3 \"${@}\" The build process is unattended, and will not succeed if any operations require interactive input. Be sure to use -y or --yes options when installing packages. Create the image (this step requires root privileges): sudo singularity build ubuntu20_helloworld.simg ubuntu20_helloworld.def This will result in a usable image in the current working directory. Be aware that if you want a very specific version of package from a repository, that package may not be available in future, so where possible, try to future-proof your containers. Building containers for other Linux distributions \u00b6 You may build Ubuntu images using CentOS and vice versa. However to bootstrap, you will need extra packages on the host OS to build the container. CentOS hosts require the debootstrap package to create Ubuntu containers, and Ubuntu hosts require the yum package to build CentOS containers. Alternatively you may create containers from an existing Singularity or Docker image, as explained in the following section. Since this method builds upon pre-built images, the debootstrap or yum packages are not required. Using LTS for Ubuntu definitions When building an Ubuntu container we recommend that you use a release with long term support (LTS release). Non-LTS Ubuntu releases have very limited support cycles which may lead to difficulties downloading packages if used after their end-of-life date. Building containers from an existing base image \u00b6 This enables you to either build or use an existing container as a base image to build other containers. Base images must be built first if part of a dependency chain and is no longer required once all dependent containers have been built. Singularity local images \u00b6 The following example demonstrates the creation of a local base Ubuntu 20 (focal) image using definition file ubuntu20_base.def , and then creating another container with python3 installed, using the local base image: BootStrap: debootstrap OSVersion: focal MirrorURL: http://us.archive.ubuntu.com/ubuntu/ Create the base image: sudo singularity build ubuntu20_base.simg ubuntu20_base.def The non-base image container (i.e. python3 in this example) can be built using definition file ubuntu20_python3.def : Bootstrap: localimage From: ubuntu20_base.simg %post apt-get install --yes python3 %runscript python3 \"${@}\" The result will be a container almost identical to the one created from scratch. sudo singularity build ubuntu20_python3.simg ubuntu20_python3.def Docker images \u00b6 You can also bootstrap from Docker containers, although if supplied by a third party, you have less visibility or control over these images, so use with caution, as this may impact the future reproducibility of results. The below example demonstrates installing the python3 package within an Ubuntu 20 (focal) container using definition file ubuntu20_docker_python3.def , which imports the ubuntu:20.04 base container available on the Docker Hub: Bootstrap: docker From: ubuntu:20.04 %post apt-get update apt-get install --yes python3 %runscript python3 \"${@}\" Build the container. This will produce a container similar to the previous examples, but may vary slightly in overall size depending on packages installed in the base docker image: sudo singularity build ubuntu20_docker_python3.simg ubuntu20_docker_python3.def Future-proofing your containers \u00b6 When building your own containers, be sure to make them portable and future-proof. Consider whether the container will still build and produce the same results if the OS release or application version changes. If copying files from a working directory as part of setup is unavoidable, ensure that any files copied from the working directory are are available for others to download (i.e. in a git repository if not large). Perform all setup as part of the build process. If any manual steps are performed after the container is built, they should be integrated within the definition file, and the container rebuilt. Consider if the ability to rebuild your container will be impacted by package updates, or deprecation of old releases. Legacy versions of CentOS applications Outdated minor CentOS releases are moved from the main CentOS servers to vault.centos.org. If you need to use a specific Operating System or application version other than the latest, you need to future-proof your container by using the CentOS vault. Definition file sections \u00b6 The following example definition file demonstrates commonly used definition file sections: %help %post %environment %test %runscript Help section \u00b6 The %help section is designed to provide information about the container when singularity run-help is run on the container, for example: $ singularity run-help /data/containers/public/python3_helloworld.simg Purpose: Test container to print \"Hello, World!\" in Python3. Author: ITS Research / QMUL. Post section \u00b6 The %post section contains the commands used to build the container, such as package installs, file downloads, compilation and software configuration. Environment section \u00b6 Environment settings supplied at build-time in the %post section are only set during build-time and are not available at run-time. Environment settings which need to be available at run-time should be added to the %environment section. Test section \u00b6 The %test section defines a set of commands or tests which should be run to validate the container has been built successfully. Some example tests include: installed binaries are available on the PATH variable --help or --version parameter for binaries (if supported) libraries, header files and man pages exist All tests will be run during the build process, after %post has completed. To build a container without running the tests, pass the -T or --notest option to the singularity build command. To run the tests for an existing container, run the singularity test command, for example: $ singularity test /data/containers/public/python3_helloworld.simg /usr/bin/python3 Runscript section \u00b6 The %runscript section defines the default action a container will perform when ran as an executable or with singularity run . This is configured during the build process. Application parameters or arguments If the runscript calls an application which takes parameters or arguments, include \"${@}\" after the application otherwise anything passed after the container name will be ignored by Singularity. Inspecting a container \u00b6 To display information about how a container was build, use the singularity inspect command. The -d option to this command will print the definition file used to built the container and the -r option will print the runscript (if added during build-time). For example: $ singularity inspect -d /data/containers/public/python3_helloworld.simg Bootstrap: docker From: ubuntu:20.04 %help Purpose: Test container to print \"Hello, World!\" in Python3. Author: ITS Research / QMUL. %post apt-get update apt-get install --yes python3 apt-get clean && \\ rm -rf /var/lib/apt/lists/* %test which python3 %runscript python3 -c 'print(\"Hello, World!\")' The singularity help inspect command provides additional options for inspecting the container. Reference \u00b6 Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Building containers"},{"location":"Tools/Singularity/Building%20containers/#building-containers","text":"Singularity containers are built from a definition file which allows the container to be built identically by anyone possessing the file. Root privileges required to build a container Note that the process of building a container requires elevated privileges One primary task per container HPC containers are designed to perform one primary task, and should consist of a main application and its dependencies, in a similar way to how module files are provided. Since containers are lightweight, you can use separate containers instead of general purpose containers containing a collection of applications. This improves supportability, performance and reproducibility.","title":"Building containers"},{"location":"Tools/Singularity/Building%20containers/#building-a-singularity-container-from-scratch","text":"Building from scratch gives complete control over the contents of the container, including operating system and packages. Certain packages may only be available for a specific version of Linux (i.e. compatibility issues) so being able to build a container from scratch enhances research capability. The following example demonstrates building an Ubuntu 20 (focal) container using definition file ubuntu20_helloworld.def that installs the python3 package via the Ubuntu package manager: BootStrap: debootstrap OSVersion: focal MirrorURL: http://us.archive.ubuntu.com/ubuntu/ %post apt-get install --yes python3 %runscript python3 \"${@}\" The build process is unattended, and will not succeed if any operations require interactive input. Be sure to use -y or --yes options when installing packages. Create the image (this step requires root privileges): sudo singularity build ubuntu20_helloworld.simg ubuntu20_helloworld.def This will result in a usable image in the current working directory. Be aware that if you want a very specific version of package from a repository, that package may not be available in future, so where possible, try to future-proof your containers.","title":"Building a Singularity container from scratch"},{"location":"Tools/Singularity/Building%20containers/#building-containers-for-other-linux-distributions","text":"You may build Ubuntu images using CentOS and vice versa. However to bootstrap, you will need extra packages on the host OS to build the container. CentOS hosts require the debootstrap package to create Ubuntu containers, and Ubuntu hosts require the yum package to build CentOS containers. Alternatively you may create containers from an existing Singularity or Docker image, as explained in the following section. Since this method builds upon pre-built images, the debootstrap or yum packages are not required. Using LTS for Ubuntu definitions When building an Ubuntu container we recommend that you use a release with long term support (LTS release). Non-LTS Ubuntu releases have very limited support cycles which may lead to difficulties downloading packages if used after their end-of-life date.","title":"Building containers for other Linux distributions"},{"location":"Tools/Singularity/Building%20containers/#building-containers-from-an-existing-base-image","text":"This enables you to either build or use an existing container as a base image to build other containers. Base images must be built first if part of a dependency chain and is no longer required once all dependent containers have been built.","title":"Building containers from an existing base image"},{"location":"Tools/Singularity/Building%20containers/#singularity-local-images","text":"The following example demonstrates the creation of a local base Ubuntu 20 (focal) image using definition file ubuntu20_base.def , and then creating another container with python3 installed, using the local base image: BootStrap: debootstrap OSVersion: focal MirrorURL: http://us.archive.ubuntu.com/ubuntu/ Create the base image: sudo singularity build ubuntu20_base.simg ubuntu20_base.def The non-base image container (i.e. python3 in this example) can be built using definition file ubuntu20_python3.def : Bootstrap: localimage From: ubuntu20_base.simg %post apt-get install --yes python3 %runscript python3 \"${@}\" The result will be a container almost identical to the one created from scratch. sudo singularity build ubuntu20_python3.simg ubuntu20_python3.def","title":"Singularity local images"},{"location":"Tools/Singularity/Building%20containers/#docker-images","text":"You can also bootstrap from Docker containers, although if supplied by a third party, you have less visibility or control over these images, so use with caution, as this may impact the future reproducibility of results. The below example demonstrates installing the python3 package within an Ubuntu 20 (focal) container using definition file ubuntu20_docker_python3.def , which imports the ubuntu:20.04 base container available on the Docker Hub: Bootstrap: docker From: ubuntu:20.04 %post apt-get update apt-get install --yes python3 %runscript python3 \"${@}\" Build the container. This will produce a container similar to the previous examples, but may vary slightly in overall size depending on packages installed in the base docker image: sudo singularity build ubuntu20_docker_python3.simg ubuntu20_docker_python3.def","title":"Docker images"},{"location":"Tools/Singularity/Building%20containers/#future-proofing-your-containers","text":"When building your own containers, be sure to make them portable and future-proof. Consider whether the container will still build and produce the same results if the OS release or application version changes. If copying files from a working directory as part of setup is unavoidable, ensure that any files copied from the working directory are are available for others to download (i.e. in a git repository if not large). Perform all setup as part of the build process. If any manual steps are performed after the container is built, they should be integrated within the definition file, and the container rebuilt. Consider if the ability to rebuild your container will be impacted by package updates, or deprecation of old releases. Legacy versions of CentOS applications Outdated minor CentOS releases are moved from the main CentOS servers to vault.centos.org. If you need to use a specific Operating System or application version other than the latest, you need to future-proof your container by using the CentOS vault.","title":"Future-proofing your containers"},{"location":"Tools/Singularity/Building%20containers/#definition-file-sections","text":"The following example definition file demonstrates commonly used definition file sections: %help %post %environment %test %runscript","title":"Definition file sections"},{"location":"Tools/Singularity/Building%20containers/#help-section","text":"The %help section is designed to provide information about the container when singularity run-help is run on the container, for example: $ singularity run-help /data/containers/public/python3_helloworld.simg Purpose: Test container to print \"Hello, World!\" in Python3. Author: ITS Research / QMUL.","title":"Help section"},{"location":"Tools/Singularity/Building%20containers/#post-section","text":"The %post section contains the commands used to build the container, such as package installs, file downloads, compilation and software configuration.","title":"Post section"},{"location":"Tools/Singularity/Building%20containers/#environment-section","text":"Environment settings supplied at build-time in the %post section are only set during build-time and are not available at run-time. Environment settings which need to be available at run-time should be added to the %environment section.","title":"Environment section"},{"location":"Tools/Singularity/Building%20containers/#test-section","text":"The %test section defines a set of commands or tests which should be run to validate the container has been built successfully. Some example tests include: installed binaries are available on the PATH variable --help or --version parameter for binaries (if supported) libraries, header files and man pages exist All tests will be run during the build process, after %post has completed. To build a container without running the tests, pass the -T or --notest option to the singularity build command. To run the tests for an existing container, run the singularity test command, for example: $ singularity test /data/containers/public/python3_helloworld.simg /usr/bin/python3","title":"Test section"},{"location":"Tools/Singularity/Building%20containers/#runscript-section","text":"The %runscript section defines the default action a container will perform when ran as an executable or with singularity run . This is configured during the build process. Application parameters or arguments If the runscript calls an application which takes parameters or arguments, include \"${@}\" after the application otherwise anything passed after the container name will be ignored by Singularity.","title":"Runscript section"},{"location":"Tools/Singularity/Building%20containers/#inspecting-a-container","text":"To display information about how a container was build, use the singularity inspect command. The -d option to this command will print the definition file used to built the container and the -r option will print the runscript (if added during build-time). For example: $ singularity inspect -d /data/containers/public/python3_helloworld.simg Bootstrap: docker From: ubuntu:20.04 %help Purpose: Test container to print \"Hello, World!\" in Python3. Author: ITS Research / QMUL. %post apt-get update apt-get install --yes python3 apt-get clean && \\ rm -rf /var/lib/apt/lists/* %test which python3 %runscript python3 -c 'print(\"Hello, World!\")' The singularity help inspect command provides additional options for inspecting the container.","title":"Inspecting a container"},{"location":"Tools/Singularity/Building%20containers/#reference","text":"Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Reference"},{"location":"Tools/Singularity/Overview/","text":"Singularity containers \u00b6 Linux containers are self-contained execution environments that share a Linux kernel with the host, but have isolated resources for CPU, I/O, memory, etc. A container can run a completely different Linux environment, without the overhead required by virtual machines. Benefits of containers \u00b6 Reproducible science - containers can include an application and its dependencies, and be run on other systems where Singularity is installed. Version independent - run code designed for other versions of Linux e.g. Ubuntu packages on a CentOS system. Self-contained - allow isolation of complicated application installs. Singularity \u00b6 Singularity is a popular Open Source container solution designed for HPC. Unlike other container solutions such as Docker , it allows utilisation of GPUs and Infiniband interconnects for MPI jobs, and does not allow privilege escalation within a container, which would compromise the security in a multi-user environment with a shared filesystem. Using Singularity on the bbgcluster \u00b6 Singularity is available as a system package on the bbgcluster. We may update the version of Singularity installed on the cluster to address security vulnerabilities or to provide extra features as they become available. Recently, the default version of Singularity has been changed to singularity v3 , although Singularity v2 can still be used with the command singularity2 . Resources \u00b6 Containers built by ITS Research are stored in /data/containers and are supported in a similar way to the globally available supported applications. Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands. See the Singularity usage page for more information about containers provided as modules. Further reading \u00b6 Singularity website Running singularity help and singularity CMD help (replace CMD with a Singularity command, such as run ) Viewing the \"singularity\" manual page Reference \u00b6 Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Overview"},{"location":"Tools/Singularity/Overview/#singularity-containers","text":"Linux containers are self-contained execution environments that share a Linux kernel with the host, but have isolated resources for CPU, I/O, memory, etc. A container can run a completely different Linux environment, without the overhead required by virtual machines.","title":"Singularity containers"},{"location":"Tools/Singularity/Overview/#benefits-of-containers","text":"Reproducible science - containers can include an application and its dependencies, and be run on other systems where Singularity is installed. Version independent - run code designed for other versions of Linux e.g. Ubuntu packages on a CentOS system. Self-contained - allow isolation of complicated application installs.","title":"Benefits of containers"},{"location":"Tools/Singularity/Overview/#singularity","text":"Singularity is a popular Open Source container solution designed for HPC. Unlike other container solutions such as Docker , it allows utilisation of GPUs and Infiniband interconnects for MPI jobs, and does not allow privilege escalation within a container, which would compromise the security in a multi-user environment with a shared filesystem.","title":"Singularity"},{"location":"Tools/Singularity/Overview/#using-singularity-on-the-bbgcluster","text":"Singularity is available as a system package on the bbgcluster. We may update the version of Singularity installed on the cluster to address security vulnerabilities or to provide extra features as they become available. Recently, the default version of Singularity has been changed to singularity v3 , although Singularity v2 can still be used with the command singularity2 .","title":"Using Singularity on the bbgcluster"},{"location":"Tools/Singularity/Overview/#resources","text":"Containers built by ITS Research are stored in /data/containers and are supported in a similar way to the globally available supported applications. Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands. See the Singularity usage page for more information about containers provided as modules.","title":"Resources"},{"location":"Tools/Singularity/Overview/#further-reading","text":"Singularity website Running singularity help and singularity CMD help (replace CMD with a Singularity command, such as run ) Viewing the \"singularity\" manual page","title":"Further reading"},{"location":"Tools/Singularity/Overview/#reference","text":"Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Reference"},{"location":"Tools/Singularity/Using%20containers/","text":"Using containers \u00b6 Running commands inside a container \u00b6 The singularity exec command will allow you to execute any program within a given container. The singularity run command performs the action defined by the %runscript section, which is the primary task of the container. Using the singularity run command is the simpler approach for job submissions. You can even \"execute\" a container, which performs the same action as the singularity run command. For example, the following demonstrates how to inspect the runscript and execute the /data/containers/public/python3_helloworld.simg container: $ singularity inspect -r /data/containers/public/python3_helloworld.simg #!/bin/sh python3 -c 'print(\"Hello, World!\")' $ /data/containers/public/python3_helloworld.simg Hello, World! $ singularity run /data/containers/public/python3_helloworld.simg Hello, World! Execute a script from outside the container \u00b6 Using on the cluster For typical use, you want to use the singularity run or singularity exec commands, especially when submitting the work via the scheduler. The following example runs a python script hello_world2.py from the current directory using the /data/containers/public/python3_helloworld.simg container: $ singularity exec /data/containers/public/python3_helloworld.simg python3 ./hello_world2.py Hello, World! Hello, World ( again ) ! The file hello_world2.py contains the following code: print ( \"Hello, World!\" ) print ( \"Hello, World (again)!\" ) If command singularity exec was replaced by singularity run , the runscript would be called, ignoring any parameters after the container name. Customised Environments While we encourage users to customise their environment to make their workflow easier, please be aware that customisations which change the user's environment for example by setting variables in the ~/.bash_profile file, or by using python's pip to create a ~/.local folder, may cause problems with Singularity which can be difficult to troubleshoot. Using containers with Grid Engine \u00b6 One of the major benefits of Singularity is the simplicity with which it can be used in an HPC environment. Your Grid Engine submission script may not require any modules loading to run your container. The resource requirements should be very similar to native code. Simple example \u00b6 #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G singularity run /data/containers/public/python3_helloworld.simg Modules example \u00b6 Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands ( singularity run and singularity exec ). In these cases, the container name will match the runscript command. For example, to use Pandoc as a module, simply load the pandoc module to use the application. #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G module load pandoc pandoc --help Shell access to the container \u00b6 It is possible to launch a shell within the container using the shell command. Interacting directly with a shell inside the container can be useful for code debugging and running multiple commands in a single interactive session, as an alternative to writing a single script. Below demonstrates how to invoke python3 from inside the /data/containers/public/python3_helloworld.simg container using an interactive shell: $ singularity shell /data/containers/public/python3_helloworld.simg Singularity> python3 Python 3 .8.10 ( default, Sep 28 2021 , 16 :10:42 ) [ GCC 9 .3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Documentation is available on the Singularity Hub Wiki Running containers from external sources \u00b6 Use of external containers for Research For long term reproducibility of containers, we recommend that you build your own native Singularity containers from definition files instead of relying on 3rd party containers for your research. Using containers from external sources may produce undesirable results if the container is rebuilt after upstream changes such as updated or obsoleted packages. Containers created elsewhere can be copied or imported, and run on the cluster. The following example demonstrates how to import and run the latest Ubuntu official image stored in the Docker Hub : $ singularity pull ubuntu.simg docker://ubuntu:latest $ singularity exec ubuntu.simg cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 20 .04 DISTRIB_CODENAME = focal DISTRIB_DESCRIPTION = \"Ubuntu 20.04.3 LTS\" Reference \u00b6 Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Using containers"},{"location":"Tools/Singularity/Using%20containers/#using-containers","text":"","title":"Using containers"},{"location":"Tools/Singularity/Using%20containers/#running-commands-inside-a-container","text":"The singularity exec command will allow you to execute any program within a given container. The singularity run command performs the action defined by the %runscript section, which is the primary task of the container. Using the singularity run command is the simpler approach for job submissions. You can even \"execute\" a container, which performs the same action as the singularity run command. For example, the following demonstrates how to inspect the runscript and execute the /data/containers/public/python3_helloworld.simg container: $ singularity inspect -r /data/containers/public/python3_helloworld.simg #!/bin/sh python3 -c 'print(\"Hello, World!\")' $ /data/containers/public/python3_helloworld.simg Hello, World! $ singularity run /data/containers/public/python3_helloworld.simg Hello, World!","title":"Running commands inside a container"},{"location":"Tools/Singularity/Using%20containers/#execute-a-script-from-outside-the-container","text":"Using on the cluster For typical use, you want to use the singularity run or singularity exec commands, especially when submitting the work via the scheduler. The following example runs a python script hello_world2.py from the current directory using the /data/containers/public/python3_helloworld.simg container: $ singularity exec /data/containers/public/python3_helloworld.simg python3 ./hello_world2.py Hello, World! Hello, World ( again ) ! The file hello_world2.py contains the following code: print ( \"Hello, World!\" ) print ( \"Hello, World (again)!\" ) If command singularity exec was replaced by singularity run , the runscript would be called, ignoring any parameters after the container name. Customised Environments While we encourage users to customise their environment to make their workflow easier, please be aware that customisations which change the user's environment for example by setting variables in the ~/.bash_profile file, or by using python's pip to create a ~/.local folder, may cause problems with Singularity which can be difficult to troubleshoot.","title":"Execute a script from outside the container"},{"location":"Tools/Singularity/Using%20containers/#using-containers-with-grid-engine","text":"One of the major benefits of Singularity is the simplicity with which it can be used in an HPC environment. Your Grid Engine submission script may not require any modules loading to run your container. The resource requirements should be very similar to native code.","title":"Using containers with Grid Engine"},{"location":"Tools/Singularity/Using%20containers/#simple-example","text":"#!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G singularity run /data/containers/public/python3_helloworld.simg","title":"Simple example"},{"location":"Tools/Singularity/Using%20containers/#modules-example","text":"Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands ( singularity run and singularity exec ). In these cases, the container name will match the runscript command. For example, to use Pandoc as a module, simply load the pandoc module to use the application. #!/bin/bash #$ -cwd #$ -j y #$ -pe smp 1 #$ -l h_rt=1:0:0 #$ -l h_vmem=1G module load pandoc pandoc --help","title":"Modules example"},{"location":"Tools/Singularity/Using%20containers/#shell-access-to-the-container","text":"It is possible to launch a shell within the container using the shell command. Interacting directly with a shell inside the container can be useful for code debugging and running multiple commands in a single interactive session, as an alternative to writing a single script. Below demonstrates how to invoke python3 from inside the /data/containers/public/python3_helloworld.simg container using an interactive shell: $ singularity shell /data/containers/public/python3_helloworld.simg Singularity> python3 Python 3 .8.10 ( default, Sep 28 2021 , 16 :10:42 ) [ GCC 9 .3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Documentation is available on the Singularity Hub Wiki","title":"Shell access to the container"},{"location":"Tools/Singularity/Using%20containers/#running-containers-from-external-sources","text":"Use of external containers for Research For long term reproducibility of containers, we recommend that you build your own native Singularity containers from definition files instead of relying on 3rd party containers for your research. Using containers from external sources may produce undesirable results if the container is rebuilt after upstream changes such as updated or obsoleted packages. Containers created elsewhere can be copied or imported, and run on the cluster. The following example demonstrates how to import and run the latest Ubuntu official image stored in the Docker Hub : $ singularity pull ubuntu.simg docker://ubuntu:latest $ singularity exec ubuntu.simg cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 20 .04 DISTRIB_CODENAME = focal DISTRIB_DESCRIPTION = \"Ubuntu 20.04.3 LTS\"","title":"Running containers from external sources"},{"location":"Tools/Singularity/Using%20containers/#reference","text":"Jordi Deu-Pons Miguel Grau Carlos L\u00f3pez Elorduy","title":"Reference"}]}