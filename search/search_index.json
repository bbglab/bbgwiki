{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-bbg-wiki","title":"Welcome to the BBG-Wiki!","text":"<p>This website is meant to include information of all the tools and data used by the bbglab team, so that it serves both as a guide to understand them and as a place where to find information about everything.</p> <p></p> <p></p> <p></p>"},{"location":"Edit_BBG-Wiki/","title":"Edit BBG-Wiki","text":"<p>The main language of the wiki documentation is Markdown. There are several online editors which can help writting Markdown text and automatically visualize what is being written.</p> <ul> <li>StackEdit</li> <li>Editor.md</li> </ul> <p>This wiki is stored in a GitHub repository, where each section of the wiki corresponds to a single Markdown file (<code>*.md</code>). By editing these files either online or locally, the wiki can be updated by everyone.</p> <ul> <li>Markdown cheatsheet</li> <li>Mkdocs documentation</li> </ul> OnlineLocal <p>Go to the bbg-wiki repository and edit any file inside the <code>docs/</code> folder, which contains all the files of the documentation.</p> <p></p>"},{"location":"Edit_BBG-Wiki/#installation","title":"Installation","text":"<ul> <li> <p>Clone the bbgwiki repository locally:</p> <pre><code>git clone git@github.com:bbglab/bbgwiki.git\npip install -r bbgwiki/requirements.txt\n</code></pre> Error: My Github password seems to be wrong somehow... <p>It might be the case that at some point of this process, it asks for the Github user and password. However, although you should introduce your Github user, the password that it asks is not your Github password. In order to know what to introduce here, you need to generate a <code>ssh key</code> following one of these two options:</p> <ul> <li> <p>From the Github web:</p> <ul> <li>Go to Github and login</li> <li>Click on your profile on the top right</li> <li>Settings &gt; Developer settings (bottom option of the left bar) &gt; Personal access tokens &gt; Generate new token</li> <li>Introduce your Github password</li> <li>Check all the boxes of the checklist</li> <li>Click \"Generate token\".</li> <li>Copy the generated key (looks like a bunch of random letters) and paste it in your terminal where it previously asked for the password.</li> </ul> </li> <li> <p>From the terminal:</p> <ul> <li>Execute the command:</li> </ul> <pre><code>ssh-keygen -o -t rsa -C \u201cssh@github.com\u201d\n</code></pre> <ul> <li>Click \"Enter\" on all the options (unless you want to save the key in a specific file, but it is not mandatory)</li> <li>Execute the next command to see the generated key (if you selected a specific file, change the <code>id_rsa</code> in the command by the name you inputed).</li> </ul> <pre><code>cat id_rsa.pub\n</code></pre> <ul> <li>Copy the output and paste it in your terminal where it previously asked for the password.</li> </ul> </li> </ul> </li> <li> <p>Open the desired <code>.md</code> file with any IDE you prefer (vscode, pycharm, atom, etc) or any online editors above-mentioned.</p> </li> <li> <p>Check if you are happy with the edits by running the website at localhost (to try stuff before updating the main web) with the following:      <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Update web:</p> <pre><code>git add &lt;edited file or directory&gt;\ngit commit -m \"Message\"\ngit push\n</code></pre> </li> </ul>"},{"location":"Edit_BBG-Wiki/#references","title":"References","text":"<ul> <li>Carlos L\u00f3pez Elorduy</li> <li>Federica Brando</li> </ul>"},{"location":"Cluster_basics/Backups/","title":"Backups","text":""},{"location":"Cluster_basics/Backups/#description","title":"Description","text":""},{"location":"Cluster_basics/Backups/#reference","title":"Reference","text":""},{"location":"Cluster_basics/Headers/","title":"Headers","text":""},{"location":"Cluster_basics/Headers/#description","title":"Description","text":"<p>When you are in the cluster, in order to visualize the column names from a file with a table, you can use the command: <pre><code>headers name_file.tsv\n</code></pre></p> <p>This will return you the name of the columns with the number of the column, for example:</p> <pre><code>1   tumor_type\n2   gene\n3   chr\n4   pos\n5   ref\n6   alt\n</code></pre>"},{"location":"Cluster_basics/Headers/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Paula Gomis </li> </ul>"},{"location":"Cluster_basics/Interactive/","title":"Interactive","text":"<p>The <code>interactive</code> command gives to the user an interactive shell in the cluster with slurm allocation. In other words, it allocates the user to a specific node of the cluster so that the jobs can be executed there without disturbing the rest of the users.</p>"},{"location":"Cluster_basics/Interactive/#usage","title":"Usage","text":"<p>Once you enter the bbgcluster, you will see in the terminal <code>&lt;username&gt;@login01</code>. It is here where, if you want to be allocated to your own node, you can just run the command:</p> <pre><code>$ interactive\n</code></pre> <p>If the <code>login01</code> has changed to <code>bbgn###</code> where <code>###</code> is the number identifying the current node.</p> <p>Apart from the basic use, there are optional arguments/flags for extra features:</p> <pre><code>interactive [-c] [-m] [-w] [-J] [-x]\n</code></pre> <ul> <li><code>-c</code>: Number of CPU cores (default: 1)</li> <li><code>-m</code>: Total amount of memory (GB) (default: 8 [GB])</li> <li><code>-w</code>: Target node</li> <li><code>-J</code>: Job name</li> <li><code>-x</code>: Binary that you want to run interactively</li> </ul>"},{"location":"Cluster_basics/Interactive/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez Elorduy</li> </ul>"},{"location":"Cluster_basics/NewFolders/","title":"New folder in workspace","text":""},{"location":"Cluster_basics/NewFolders/#description","title":"Description","text":"<p>Appart from your own personal folder, there are four main shared folders within the <code>/workspace</code> folder in the cluster. These are:</p> <ul> <li>Projects: analysis/results files. Backup and snapshots. High safe.</li> <li>Datasets: data files downloaded from public/private repositories. Re-downloading is possible/straightforward. Backup. Medium safe.</li> <li>No backup: intermediate files generated during pipelines execution or big amount of data re-downloadable. Low safe.</li> <li>Datasafe: datasets generated by us or from collaborators, not in public repositories. Snapshots. High safe.</li> </ul> <p>If you want to create a new folder inside one of the previously mentioned, perform the following instructions:</p> <ol> <li>Go to the bbgdashboard</li> <li>Log in (or check that you are already logged in)</li> <li> <p>Go to the <code>Cluster</code> tab.</p> <p></p> </li> <li> <p>Click on one of the four options where you want to create your folder.</p> <p></p> </li> <li> <p>Write the title and a description of the new folder and click \"OK\".</p> <p></p> </li> </ol>"},{"location":"Cluster_basics/NewFolders/#possible-errors","title":"Possible errors","text":"<p>If after clicking the \"OK\" button an ERROR message is displayed:</p> <ul> <li>Make sure that you are logged in in the dashboard itself.</li> <li>Try doing everything from an incognito window.</li> </ul>"},{"location":"Cluster_basics/NewFolders/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez Elorduy</li> <li>Miguel Grau</li> <li>Jordi Deu-Pons</li> </ul>"},{"location":"Cluster_basics/Notebooks_in_cluster/","title":"Running notebooks in the cluster","text":""},{"location":"Cluster_basics/Notebooks_in_cluster/#description","title":"Description","text":"<p>Running a jupyter notebook in the cluster allows you to work with a notebook which will be running even if you disconnect from the cluster.</p> <p>This is especially useful for time-consuming/memory-consuming processes or notebooks with a high number of variables/packages needed, so that you have more computational power than your local computer, you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress and you can come back to a notebook without the need of loading all the variables/packages again.</p> <p>To run a notebook in the cluster, a screen and an interactive will be used.</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#create-a-notebook","title":"Create a notebook","text":"<p>You will need to follow the next steps:</p> <ol> <li> <p>Connect to the cluster:</p> <pre><code>ssh -p 22022 &lt;username&gt;@bbgcluster\n</code></pre> </li> <li> <p>Open a screen:</p> <pre><code>&lt;username&gt;@login01:~$ screen -S &lt;screen_name&gt;\n</code></pre> </li> <li> <p>Run an interactive job and remember the node you are assigned to (e.g. bbgn005)</p> <pre><code>[screen_name] &lt;username&gt;@login01:~$ interactive\n</code></pre> <p>Info</p> <p>If your notebook needs more than 8G and 2 cores, you can specify it here -- see interactive section.</p> </li> <li> <p>Activate conda base or the conda environment that you need in your notebook:</p> <pre><code>[screen_name] &lt;username&gt;@bbgn005:~$ conda activate &lt;your environment&gt;\n</code></pre> </li> <li> <p>Go to the folder that you wish to run the notebook:</p> <pre><code>(base)[screen_name] &lt;username&gt;@bbgn005:~$ cd /workspace/folder\n</code></pre> </li> <li> <p>Run the jupyter notebook:</p> NotebookLab <p>Copy the following command:</p> <pre><code>unset XDG_RUNTIME_DIR &amp;&amp; jupyter notebook --ip=0.0.0.0\n</code></pre> <p>Copy the following command:</p> <pre><code>unset XDG_RUNTIME_DIR &amp;&amp; jupyter lab --ip=0.0.0.0\n</code></pre> </li> <li> <p>Keep the URL with the token and the port (e.g.8888) in which the interactive is running:</p> <pre><code>[I 10:37:20.371 NotebookApp] The Jupyter Notebook is running at: http://127.0.0.1:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c\n</code></pre> </li> </ol> <p>Now, your notebook is running and you can dettach from the screen by pressing <code>Ctrl + A + D</code>. You can now close the terminal and the notebook will continue running in the cluster.</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#open-a-notebook","title":"Open a notebook","text":"<p>In order to open an already existing notebook, you'll need to know the port (e.g 8888) and the node of the cluster (e.g bbgn005) where you created it in the previous step.</p> <pre><code>ssh -L &lt;port&gt;:&lt;node&gt;:&lt;port&gt; -p 22022 &lt;username&gt;@bbgcluster\n# For example: ssh -L 8888:bbgn005:8888 -p 22022 clopeze@bbgcluster\n</code></pre> <p>Note</p> <p>If you don't want to remember these commands, you can create an alias (like a shortcut).</p> <p>Open the URL you obtain when creating the notebook in the cluster (step 7).</p> <pre><code># For example: http://127.0.0.1:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c\n</code></pre>"},{"location":"Cluster_basics/Notebooks_in_cluster/#close-a-notebook","title":"Close a notebook","text":"<p>When you don't need the notebook to continue running in the cluster, reconnect to the screen:</p> <pre><code>screen -r &lt;screen_name&gt;\n</code></pre> <p>And kill jupyter (Ctrl + C) and exit the screen (write <code>exit</code> in the terminal and press enter)</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#errors-and-solutions","title":"Errors and solutions","text":""},{"location":"Cluster_basics/Notebooks_in_cluster/#i-forgot-the-url-of-the-notebook","title":"I forgot the URL of the notebook","text":"<ol> <li> <p>Enter to the cluster and check your screens:</p> <pre><code>screen -ls\n</code></pre> </li> <li> <p>Enter the screen where you have your notebook:</p> <pre><code>screen -r &lt;screen name&gt;\n</code></pre> </li> <li> <p>Scroll up until you find the URL, which should look like:</p> <pre><code>http://127.0.0.1:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c\n</code></pre> </li> </ol>"},{"location":"Cluster_basics/Notebooks_in_cluster/#my-notebook-doesnt-open","title":"My notebook doesn't open","text":"<p>One possibility is that the running notebook in the cluster has crashed. You can check this by going to the cluster, entering the screen where you have the notebook and check if it is still running.</p> <p>If not, you should create a notebook following the steps at the beginning of this page (Create a new notebook).</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez</li> <li>Paula Gomis</li> <li>Federica Brando</li> </ul>"},{"location":"Cluster_basics/Screen/","title":"Screen","text":""},{"location":"Cluster_basics/Screen/#description","title":"Description","text":"<p>The <code>screen</code> command opens a session which will be running even if you disconnect from the cluster.</p> <p>This is especially useful for time-consuming processes, so that you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress.</p> <p>You can also open several screens for different processes, which you can detach and attach to them as you like.</p>"},{"location":"Cluster_basics/Screen/#basic-commands","title":"Basic commands","text":""},{"location":"Cluster_basics/Screen/#new-screen","title":"New screen","text":"<p>Creates a new screen with name \"custom_name\".</p> <pre><code>screen -S &lt;custom_name&gt;\n</code></pre> <p>Warning</p> <p>When opening a new screen, this should be done from the <code>login01</code> node, since this guarantees that the screen will be constantly running and not shut down (which could happen if the screen is opened in one of the other nodes).</p>"},{"location":"Cluster_basics/Screen/#list-screens","title":"List screens","text":"<p>List all the created screens.</p> <pre><code>screen -ls\n</code></pre>"},{"location":"Cluster_basics/Screen/#detach","title":"Detach","text":"<p>Detaches from a screen</p> <pre><code>Ctrl + A -&gt; D\n</code></pre>"},{"location":"Cluster_basics/Screen/#re-attach","title":"Re-attach","text":"<p>Re-attaches to a detached screen.</p> <pre><code>screen -r [#]\n</code></pre> <p>Note</p> <p>If there are multiple screens available, include the number of the screen id (or name) to identify which screen to re-attach.</p>"},{"location":"Cluster_basics/Screen/#exit-and-kill-screen","title":"Exit and kill screen","text":"<pre><code>exit\n</code></pre>"},{"location":"Cluster_basics/Screen/#kill-a-detached-screen","title":"Kill a detached screen","text":"<pre><code>screen -X -S [screen number ID or name] quit\n</code></pre>"},{"location":"Cluster_basics/Screen/#kill-all-screens","title":"Kill all screens","text":"<pre><code>pkill screen\n</code></pre>"},{"location":"Cluster_basics/Screen/#documentation","title":"Documentation","text":"<p>For a more extensive list of commands, check the screen cheatsheet.</p> <p>You can also check the full documentation.</p>"},{"location":"Cluster_basics/Screen/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez Elorduy</li> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> </ul>"},{"location":"Cluster_basics/Structure/","title":"Structure","text":""},{"location":"Cluster_basics/Structure/#description","title":"Description","text":"<p>The workspace is organized in several folders, each of them with different purposes and different security backups. </p> <ul> <li>Projects : Files from analysis or results obtained in the projects. It has backups and snapshots so it is highly safe.</li> <li>Datasets : Data files that have been downloaded from public or private repositories and re-downloading the is possible or straightforward. It only has backups, so it is medium safe.</li> <li>No backup : Intermediate files that have been generated during pipelines execution or big amounts of data that is re-downloadable. It is lowly safe.</li> <li>Datasafe : Datasets that have been generated by us or from collaborators, and it is not in public repositories. It has Snapshots and it is highly safe.</li> </ul>"},{"location":"Cluster_basics/Structure/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> </ul>"},{"location":"Cluster_basics/terminal/","title":"Terminal","text":"<p>In this section we are going to show the basic commands to navigate and manage files with the terminal.</p>"},{"location":"Cluster_basics/terminal/#creating-folders-and-files","title":"Creating folders and files","text":""},{"location":"Cluster_basics/terminal/#directories-folders","title":"Directories (folders)","text":"Command Description <code>ls</code> Show the content of the current directory <code>cd anotherdir</code> Change directory to <code>anotherdir</code> <code>cd ..</code> Change to previous directory <code>cd ~</code> Change to home folder (<code>/home/&lt;user&gt;/</code>) <code>cd /</code> Change to root folder (<code>/</code>) <code>pwd</code> Show path of current directory <code>mkdir newdirectory</code> Create new directory called <code>newdirectory</code>"},{"location":"Cluster_basics/terminal/#files","title":"Files","text":"Command Description <code>touch newfile.txt</code> Create new file called <code>newfile.txt</code> <code>echo \"message\"</code> Print <code>message</code> in the terminal <code>echo \"Hello BBGLab!\" &gt; newfile.txt</code> Save output of <code>echo</code> command into a file called <code>newfile.txt</code> <code>echo \"In the morning\" &gt;&gt; file.txt</code> Append to the end of the file <code>file.txt</code> the line output of <code>echo</code>"},{"location":"Cluster_basics/terminal/#moving-and-manipulating-files","title":"Moving and manipulating files","text":"Command Description <code>mv file.txt destination</code> Move file <code>file.txt</code> to directory <code>destination</code> <code>mv dir1/* destination</code> Move all contents of <code>dir1</code> to <code>destination</code> <code>mv file1.txt file2.txt</code> Change name of <code>file1.txt</code> to <code>file2.txt</code> <code>cp file.txt destination</code> Copy <code>file.txt</code> into directory <code>destination</code> <code>cp file1.txt file2.txt</code> Copy <code>file1.txt</code> into <code>file2.txt</code> <code>rm file.txt</code> Remove <code>file.txt</code> <code>rm -rf dir1</code> Remove directory <code>dir1</code> and ALL THE FILES INSIDE"},{"location":"Cluster_basics/terminal/#other-basic-commands","title":"Other basic commands","text":"Command Description <code>echo \"message\"</code> Print <code>message</code> in the terminal <code>cat file.txt</code> Show contents of <code>file.txt</code> <code>grep \"sentence\" file.txt</code> Search the word \"sentence\" in <code>file.txt</code> <code>find . -name \"file.txt\"</code> Find in current directory (<code>.</code>) the file <code>file.txt</code> <code>man echo</code> Manual (documentation) of command <code>echo</code>"},{"location":"Cluster_basics/terminal/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Federica Brando</li> <li>Miguel Grau</li> <li>Jordi Deu-Pons</li> </ul>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/","title":"Qmap Submit","text":""},{"location":"Cluster_basics/Submitting_jobs/Qmap/#description","title":"Description","text":"<p>How to submit jobs to the Cluster using Qmap.</p> <p>Qmap documentation: https://qmap.readthedocs.io</p>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#howto","title":"Howto","text":""},{"location":"Cluster_basics/Submitting_jobs/Qmap/#1-prepare-qmap-file-example","title":"1. Prepare .qmap file (example)","text":"<pre><code>[params]\nmemory=50G\n\n[pre]\n. \"/home/$USER/miniconda3/etc/profile.d/conda.sh\"\nconda activate sciclone-env\n\n[jobs]\nRscript run.R ../data/vafs.dat bmm 3 ./results.beta.3\nRscript run.R ../data/vafs.dat gaussian.bmm 3 ./results.gaussian.3\nRscript run.R ../data/vafs.dat binomial.bmm 3 ./results.binomial.3\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#2-run-qmap-submit-from-the-login-node","title":"2. Run \"qmap submit\" from the login node","text":"<pre><code>$ qmap submit filename.qmap\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> </ul>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/","title":"SLURM","text":""},{"location":"Cluster_basics/Submitting_jobs/SLURM/#description","title":"Description","text":""},{"location":"Cluster_basics/Submitting_jobs/SLURM/#reference","title":"Reference","text":""},{"location":"Datasets/General_datasets/BeatAML/","title":"BeatAML","text":""},{"location":"Datasets/General_datasets/BeatAML/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/BeatAML/#reference","title":"Reference","text":""},{"location":"Datasets/General_datasets/CGCI/","title":"CGCI","text":""},{"location":"Datasets/General_datasets/CGCI/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/CGCI/#reference","title":"Reference","text":""},{"location":"Datasets/General_datasets/CPTAC/","title":"CPTAC","text":""},{"location":"Datasets/General_datasets/CPTAC/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/CPTAC/#data-access","title":"Data access","text":"<p>Website</p> <p></p>"},{"location":"Datasets/General_datasets/CPTAC/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":""},{"location":"Datasets/General_datasets/CPTAC/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/GENIE/","title":"GENIE","text":""},{"location":"Datasets/General_datasets/GENIE/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/GENIE/#reference","title":"Reference","text":""},{"location":"Datasets/General_datasets/Hartwig/","title":"Hartwig","text":""},{"location":"Datasets/General_datasets/Hartwig/#description","title":"Description","text":"<p>The Hartwig Medical Foundation (HMF) is aiming to make whole genome sequencing WGS) the future standard of care in the Netherlands ( https://www.hartwigmedicalfoundation.nl/en/). The HMF maintains and develops a growing database with &gt;5,000 metastatic cancer patients with rich clincial and genomic data (https://www.hartwigmedicalfoundation.nl/en/data/database/) - the largest such database in the entire world. The only way to truly to understand the HMF data is to look through the very detailed provided github repos. Most of the bioinformatics pipeline in HMF is made with their own in-house tools. </p>"},{"location":"Datasets/General_datasets/Hartwig/#data-access","title":"Data access","text":"<p>To use data from Hartwig Medical Foundation Database you need special permission. Contact Martina or Paula if you need to use this data. </p> <p>Once you obtains permissions, you can find the data from Hartwig Medical Foundation Database on bbgcluster here: <pre><code>/workspace/datasets/hartwig\n</code></pre></p>"},{"location":"Datasets/General_datasets/Hartwig/#processed-data","title":"Processed data","text":"<p>In our lab several projects have used HMF data. From these projects the HMF data was pre-processed in away that could be re-used for other projects.  </p> <p>For one such project (immunobiomarkers) a pipeline to extract and format HMF biomarkers was created (https://bitbucket.org/bbglab/hartwig_biomarkers/src/master/). This pipeline creates pre-processed data files of different data types. The HMF processed clinical data is output on our cluster here: <pre><code>/workspace/datasets/hartwig/20220809/biomarkers/tmp/clinical_ready.csv\n</code></pre> The full output from the biomarkers pipline collects more than 60,000 curated columns of biomarkers into a table ready for analysis (located in folder below): <pre><code>/workspace/datasets/hartwig/20220809/biomarkers/clean/biomarkers_AdjTPM.csv\n</code></pre></p>"},{"location":"Datasets/General_datasets/Hartwig/#exhaustive-study","title":"Exhaustive Study","text":"<p>For the immunobiomarkers project an exhaustive study of biomarkers was run. The exhaustive analysis is found within this repo - https://bitbucket.org/bbglab/immune_biomarkers/src/main/ - and the code could be re-purposed in the future to study other classes of treatmetns (e.g. Chemo, Targeted, Hormonal therapies).</p>"},{"location":"Datasets/General_datasets/Hartwig/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Researchers making use of data provided by Hartwig Medical Foundation must acknowledge this in every publication, by using at least the text below:</p> <p>This publication and the underlying research are partly facilitated by Hartwig Medical Foundation and the Center for Personalized Cancer Treatment (CPCT) which have generated, analysed and made available data for this research.</p> <p>Read this document to learn more about how to cite Hartwig Medical Foundation Database.</p>"},{"location":"Datasets/General_datasets/Hartwig/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> <li>Joseph Usset</li> </ul>"},{"location":"Datasets/General_datasets/ICGC/","title":"ICGC","text":""},{"location":"Datasets/General_datasets/ICGC/#description","title":"Description","text":"<p>The International Cancer Genome Consortium (ICGC) is a global initiative to build a comprehensive catalog of mutational abnormalities in the major tumor types. ICGC\u2019s Data Portal is a user-friendly platform for efficient visualization, analysis and interpretation of large, diverse cancer datasets. </p> <p>The portal currently contains data from 84 worldwide cancer projects, collectively representing about 77 million somatic mutations and molecular data from over 20,000 contributors.</p> <p>The ICGC Data Portal provides many tools for visualizing, querying, and downloading cancer data, which is released on a quarterly schedule.</p>"},{"location":"Datasets/General_datasets/ICGC/#data-access","title":"Data access","text":"<p>You can find the data from ICGC in the folder: </p> <p>Website</p> <p>https://dcc.icgc.org</p>"},{"location":"Datasets/General_datasets/ICGC/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>To cite the ICGC 25K Data Portal, please cite this publication:</p> <ul> <li>Zhang J, Bajari R, Andric D, et al. The International Cancer Genome Consortium Data Portal. Nat Biotechnol. 2019;37(4):367\u2010369. doi:10.1038/s41587-019-0055-9</li> </ul>"},{"location":"Datasets/General_datasets/ICGC/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/PCAWG/","title":"PCAWG","text":""},{"location":"Datasets/General_datasets/PCAWG/#description","title":"Description","text":"<p>The Pan-Cancer Analysis of Whole Genomes (PCAWG) study is an international collaboration to identify common patterns of mutation in more than 2,600 cancer whole genomes from the International Cancer Genome Consortium.</p> <p>Building upon previous work which examined cancer coding regions (Cancer Genome Atlas Research Network, The Cancer Genome Atlas Pan-Cancer analysis project), this project explored the nature and consequences of somatic and germline variations in both coding and non-coding regions, with specific emphasis on cis-regulatory sites, non-coding RNAs, and large-scale structural alterations.</p>"},{"location":"Datasets/General_datasets/PCAWG/#data-access","title":"Data access","text":"<p>You can find the data from PCAWG in the folder:  <pre><code>/workspace/datasets/intogen_datasets/genomes/pcawg_20160110/filtered\n</code></pre></p> <p>You can find clinical data from samples from PCAWG in the file: <pre><code>/workspace/datasets/intogen_datasets/genomes/pcawg_20160110/original_data/pcawg_donor_clinical_August2016_v9.csv\n</code></pre></p> <p>You can find clinical data from samples from Hartwig in the file: <pre><code>/workspace/datasets/intogen_datasets/genomes/pcawg_20160110/original_data/pcawg_specimen_histology_August2016_v9.csv\n</code></pre></p> <p>Additional data about ... from PCAWG is available in the folder (you will need special permission to use this data): <pre><code>/workspace/datasets/pcawg\n</code></pre></p> <p>Website</p> <p>https://dcc.icgc.org/pcawg</p>"},{"location":"Datasets/General_datasets/PCAWG/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>When using this dataset, please cite reference: The ICGC/TCGA Pan-Cancer Analysis of Whole Genomes Network. Pan-cancer analysis of whole genomes. Nature (2020).</p> <p>Click here to learn more about how to cite PCAWG.</p>"},{"location":"Datasets/General_datasets/PCAWG/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/PedcBioPortal/","title":"PedcBioPortal","text":""},{"location":"Datasets/General_datasets/PedcBioPortal/#description","title":"Description","text":"<p>The PedcBioPortal for Childhood Cancer Genomics is an instance of cBioPortal supporting the curation and pan-cancer integration of public, pediatric cancer genomics data sets as well as 'open science' initiatives integrated within the Kids First Data Resource Center as well as data from consortia-based efforts including the Children's Brain Tumor Tissue Consortium (CBTTC), the Pediatric NeuroOncology Consortium (PNOC), the St. Baldrick Pediatric Stand Up 2 Cancer Dream Team, and the Pediatric Preclinical Testing Consortium (PPTC). </p>"},{"location":"Datasets/General_datasets/PedcBioPortal/#data-access","title":"Data access","text":"<p>Website</p> <p></p>"},{"location":"Datasets/General_datasets/PedcBioPortal/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":""},{"location":"Datasets/General_datasets/PedcBioPortal/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/StJude/","title":"St. Jude","text":""},{"location":"Datasets/General_datasets/StJude/#description","title":"Description","text":"<p>In 2010, St. Jude Children\u2019s Research Hospital and Washington University School of Medicine launched a $65 million, three-year project (St. Jude\u2014Washington University Pediatric Cancer Genome Project) to define the genomic landscape of pediatric cancer, including some of the least understood and most challenging cancers, as an effort to discover the origins of pediatric cancer and seek new treatments. It included whole exome and whole transcriptome sequencing of an additional 1,200 patients, which included more than 20 different cancers.</p> <p>Click here to visualize the list of tumor types available in St. Jude Cloud. </p>"},{"location":"Datasets/General_datasets/StJude/#data-access","title":"Data access","text":"<p>You can find the data from St. Jude Cloud in the folder: <pre><code>/workspace/datasets/stjude\n</code></pre></p> <p>Permission</p> <p>To use data from St. Jude  you need special permission. Contact Martina or Paula if you need to use this data. </p> <p>Website</p> <p>https://www.stjude.cloud/studies/pediatric-cancer-genome-project/</p>"},{"location":"Datasets/General_datasets/StJude/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Click here to learn how to cite St. Jude data.</p>"},{"location":"Datasets/General_datasets/StJude/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/StJudeLife/","title":"St. Jude LIFE","text":""},{"location":"Datasets/General_datasets/StJudeLife/#description","title":"Description","text":"<p>The objective of the St. Jude LIFE study is to establish a lifetime cohort of childhood cancer survivors to facilitate longitudinal clinical evaluation of health outcomes in aging adults surviving pediatric cancer.</p> <p>The main aims of the project are:</p> <ul> <li>determine prevalence and latency of late effects;</li> <li>identify multifactorial predictors of adverse outcomes;</li> <li>develop risk profiles for adverse health outcomes across the age spectrum; </li> <li>use data to guide health screening and risk-reducing interventions.</li> </ul> <p>Click here to visualize a complete report of the characteristics of the cohort from St. Jude LIFE. </p>"},{"location":"Datasets/General_datasets/StJudeLife/#data-access","title":"Data access","text":"<p>You can find the data from St. Jude LIFE in the folder: <pre><code>/workspace/datasets/stjudelife\n</code></pre></p> <p>Website</p> <p>https://sjlife.stjude.org/</p>"},{"location":"Datasets/General_datasets/StJudeLife/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":""},{"location":"Datasets/General_datasets/StJudeLife/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/TARGET/","title":"TARGET","text":""},{"location":"Datasets/General_datasets/TARGET/#description","title":"Description","text":"<p>The Therapeutically Applicable Research to Generate Effective Treatments (TARGET) program applies a comprehensive genomic approach to determine molecular changes that drive childhood cancers. The goal of the program is to use data to guide the development of effective, less toxic therapies. </p>"},{"location":"Datasets/General_datasets/TARGET/#data-access","title":"Data access","text":"<p>You can find TARGET pediatric cancer data in the folder: </p> <p>Website</p> <p>https://ocg.cancer.gov/programs/target</p>"},{"location":"Datasets/General_datasets/TARGET/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Researchers making use of TARGET pediatric cancer data must acknowledge this in every publication, by using at least the text below:</p> <p>\"The results published here are in whole or part based upon data generated by the Therapeutically Applicable Research to Generate Effective Treatments (https://ocg.cancer.gov/programs/target) initiative, phs000218. The data used for this analysis are available at https://portal.gdc.cancer.gov/projects. Information about TARGET can be found at http://ocg.cancer.gov/programs/target.\"</p> <p>Click here to learn more about how to cite TARGET pediatric cancer data.</p>"},{"location":"Datasets/General_datasets/TARGET/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/TCGA/","title":"TCGA","text":""},{"location":"Datasets/General_datasets/TCGA/#description","title":"Description","text":"<p>The Cancer Genome Atlas (TCGA), a landmark cancer genomics program, molecularly characterized over 20,000 primary cancer and matched normal samples spanning 33 cancer types. This joint effort between NCI and the National Human Genome Research Institute began in 2006, bringing together researchers from diverse disciplines and multiple institutions.</p> <p>TCGA has generated over 2.5 petabytes of genomic, epigenomic, transcriptomic, and proteomic data. The data, which has already led to improvements in our ability to diagnose, treat, and prevent cancer, will remain publicly available for anyone in the research community to use.</p> <p>Click here to visualize the list of tumor types available in TCGA. For each cancer type, TCGA published an overview of the characterizations performed and an initial analysis of the data.  </p>"},{"location":"Datasets/General_datasets/TCGA/#data-access","title":"Data access","text":"<p>You can find the data from TCGA in the folder: <pre><code>/workspace/datasets/intogen_datasets/genomes/tcga_20171006/filtered\n</code></pre></p> <p>You can find clinical data from samples from TCGA in the folder: <pre><code>/workspace/datasets/intogen_datasets/genomes/tcga_20171006/metadata\n</code></pre></p> <p>Website</p> <p>https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga</p>"},{"location":"Datasets/General_datasets/TCGA/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Click here to learn how to cite TCGA.</p>"},{"location":"Datasets/General_datasets/TCGA/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/UK_Biobank/","title":"UK Biobank","text":""},{"location":"Datasets/General_datasets/UK_Biobank/#description","title":"Description","text":"<p>UK Biobank (UKB) is a large-scale biomedical database and research resource, containing in-depth genetic and health information from half a million UK participants.  It is a very large and detailed prospective study with over 500,000 participants aged 40\u201369 years when recruited in 2006\u20132010.</p> <p>The study has collected and continues to collect extensive phenotypic and genotypic detail about its participants, including data from questionnaires, physical measures, sample assays, accelerometry, multimodal imaging, genome-wide genotyping and longitudinal follow-up for a wide range of health-related outcomes. It is accessible to approved researchers undertaking vital research into the most common and life-threatening diseases. </p> <p>Website</p> <p>https://www.ukbiobank.ac.uk/</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#genomic-data","title":"Genomic data","text":"<p>UKB includes Whole Exome Sequencing data obtained from blood samples from nearly the whole cohort (about 470,000 individuals). Moreover, it also has Whole Genome Sequencing data obtained also from blood from about 200,000 individuals, and, by the end of 2023, it is expected to be extended to the whole cohort (500,000 individuals). More info here.</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#data-access","title":"Data access","text":"<p>Access to UKB data is restricted to authorized users that have to submit a research proposal to UKB. As BBGLab we can use the data on the study of the genetic basis of clonal hematopoiesis. Only some people in the lab have access to the data, you can ask Martina or Paula for more information.</p> <p>Originally, the data was downloadable, so we have in our cluster a lot of clinical data from the entire cohort and also the WES from 200,000 individuals in the folder: <pre><code>/workspace/datasets/ukbiobank_ch/\n</code></pre></p> <p>More recently, the UK Biobank Research Analysis Platform (UKB RAP) was created, so the data can not be downloaded anymore. The UKB RAP is a platform in the cloud based on DNA Nexus. The WES data from the full cohort and the WGS data are only accessible through this platform.</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>All publications should include the acknowledgement: \u201cThis research has been conducted using data from UK Biobank, a major biomedical database\u201d and where appropriate, include a link to the UK Biobank website: www.ukbiobank.ac.uk.</p> <p>More information here.</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#reference","title":"Reference","text":"<ul> <li>Santi Demajo</li> <li>Joan Enric Ramis</li> <li>Miguel Grau</li> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/cBioPortal/","title":"cBioPortal","text":""},{"location":"Datasets/General_datasets/cBioPortal/#description","title":"Description","text":"<p>The cBioPortal for Cancer Genomics is an open-access, open-source resource for interactive exploration of multidimensional cancer genomics data sets including molecular profiles and clinical attributes from large-scale cancer genomics projects.</p> <p>The portal supports and stores non-synonymous mutations, DNA copy-number data (putative, discrete values per gene, e.g. \"deeply deleted\" or \"amplified\", as well as log2 or linear copy number data), mRNA and microRNA expression data, protein-level and phosphoprotein level data (RPPA or mass spectrometry based), DNA methylation data, and de-identified clinical data. However, for many studies only somatic mutation data and limited clinical data are available. For TCGA studies, the other data types are also available. Germline mutations are supported by cBioPortal, but are, with a few exceptions, not available in the public instance.</p>"},{"location":"Datasets/General_datasets/cBioPortal/#data-access","title":"Data access","text":"<p>Click here to visualize the list of all the available datasets in cBioPortal and download the data.</p> <p>Website</p> <p>https://www.cbioportal.org/</p>"},{"location":"Datasets/General_datasets/cBioPortal/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>When using data from cBioPortal, please cite the following portal papers:</p> <ul> <li>Cerami et al. The cBio Cancer Genomics Portal: An Open Platform for Exploring Multidimensional Cancer Genomics Data. Cancer Discovery. May 2012 2; 401. PubMed.</li> <li>Gao et al. Integrative analysis of complex cancer genomics and clinical profiles using the cBioPortal. Sci. Signal. 6, pl1 (2013). PubMed.</li> </ul> <p>Remember to also cite the source of the data if you are using a publicly available dataset.</p> <p>Click here to learn more about how to cite cBioPortal.</p>"},{"location":"Datasets/General_datasets/cBioPortal/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/Inhouse_datasets/ALL_cohort/","title":"ALL cohort","text":""},{"location":"Datasets/Inhouse_datasets/ALL_cohort/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/ALL_cohort/#reference","title":"Reference","text":""},{"location":"Datasets/Inhouse_datasets/Damage_maps/","title":"Damage maps","text":""},{"location":"Datasets/Inhouse_datasets/Damage_maps/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/Damage_maps/#reference","title":"Reference","text":""},{"location":"Datasets/Inhouse_datasets/Nanopore_data/","title":"Nanopore data","text":""},{"location":"Datasets/Inhouse_datasets/Nanopore_data/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/Nanopore_data/#reference","title":"Reference","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric%20Secondary%20neoplasms/","title":"Pediatric Secondary neoplasms","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric%20Secondary%20neoplasms/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric%20Secondary%20neoplasms/#reference","title":"Reference","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric_Rhabdoid_cohort/","title":"Pediatric Rhabdoid cohort","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric_Rhabdoid_cohort/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric_Rhabdoid_cohort/#reference","title":"Reference","text":""},{"location":"Datasets/Other_data/Canonical_transcripts/","title":"Canonical transcripts","text":""},{"location":"Datasets/Other_data/Canonical_transcripts/#description","title":"Description","text":""},{"location":"Datasets/Other_data/Canonical_transcripts/#reference","title":"Reference","text":""},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/","title":"Encode Epigenetic data","text":"<p>Cluster location:  <code>/workspace/datasets/encode/</code></p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#description","title":"Description","text":"<p>Encode genomic data downloaded from https://www.encodeproject.org/</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#contents","title":"Contents:","text":"<p>bigWig files of epigenetic tracks across the genome</p> <p>Reference genome: hg38</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#chromatin-files-801-files","title":"Chromatin files (801 files)","text":"<ul> <li>Are large-scale chromatin modifications such as: </li> <li>H3K27ac</li> <li>H3K9me2</li> </ul> <p>Full download link present in chromatin_files.txt Metadata: Chromatin_metadata.tsv</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#tfs-442-files","title":"TFs (442 files)","text":"<p>Although the name does not fully characterize its content this folder contains:  - CTCF-sites - ATAC sequencing - DNase-seq</p> <p>Metadata: TFs_metadata_2022_12_12.tsv</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#references","title":"References","text":"<p>Information for citing the most recent version of the Encode dataset: https://www.encodeproject.org/help/citing-encode/</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#author","title":"Author","text":"<p>Axel Rosendahl Huber - 16-03-2023</p>"},{"location":"Datasets/Other_data/Genomic_regions/","title":"Genomic regions","text":"<p>Genomic regions annotations generated by BBGLab Data includes:</p> <ul> <li><code>3utr</code></li> <li><code>introns</code></li> <li><code>mirna_mat</code></li> <li><code>tfbs</code></li> <li><code>5utr</code></li> <li><code>lncrna_distal_promoters</code></li> <li><code>mirna_pre</code></li> <li><code>utr</code></li> <li><code>cds</code></li> <li><code>lncrna_exons</code></li> <li><code>other_ncrnas</code></li> <li><code>distal_promoters</code></li> <li><code>lncrna_proximal_promoters</code></li> <li><code>proximal_promoters</code></li> <li><code>enhancer</code></li> <li><code>lncrna_splice_sites</code></li> <li><code>splice_sites</code></li> </ul>"},{"location":"Datasets/Other_data/Genomic_regions/#releases","title":"Releases","text":"<ul> <li> <p>Release 2 (30-09-2020): The coordinates are extracted from the <code>gtf3</code> annotation file.</p> <ul> <li>CDS coordinates are generated in two flavours: with and without (default) STOP codon.</li> <li>Gencode v35</li> <li>Ensembl canonical transcripts v101</li> </ul> </li> <li> <p>Release 1 (2019): The coordinates are extracted from the <code>gtf</code> annotation file.</p> <ul> <li>CDS coordinates are generated without STOP codon.</li> <li>Gencode v31</li> <li>Ensembl canonical transcripts v97</li> </ul> </li> </ul>"},{"location":"Datasets/Other_data/Genomic_regions/#description","title":"Description","text":"<p>You can find the data in the folder: <code>/workspace/projects/genomic_regions/</code></p> <ul> <li><code>./raw_data</code>: contains databases from which raw data has been downloaded</li> <li><code>./scripts</code>: contains code to parse raw data</li> <li><code>./hg19</code>: contains genomic annotations in hg19 reference genome</li> <li><code>./hg38</code>: contains genomic annotations in hg38 reference genome</li> </ul>"},{"location":"Datasets/Other_data/Genomic_regions/#reference","title":"Reference","text":"<ul> <li>Joan Enric</li> </ul> <p>Created on 2019-07-08 by claudia.arnedo@irbbarcelona.org</p>"},{"location":"Datasets/Other_data/Nmdetective/","title":"Nmdetective","text":""},{"location":"Datasets/Other_data/Nmdetective/#description","title":"Description","text":""},{"location":"Datasets/Other_data/Nmdetective/#reference","title":"Reference","text":""},{"location":"Datasets/Other_data/Reference_genomes/","title":"Reference genomes","text":""},{"location":"Datasets/Other_data/Reference_genomes/#description","title":"Description","text":"<p>The same reference genomes are used across many different projects. Although each user may prefer to have its own copy of the genome sequence, there is a <code>genomes</code> folder in datasets that can serve as the collective storage of reference genomes.</p> <p>You can find the full path to the directory here: <code>/workspace/datasets/genomes/</code></p> <p>The contents of this folder are the following: <pre><code>GRCh37\nGRCh38\niGenomes\niGenomes_2022\npurple_resources\nREADME.txt\nSaccharomyces_cerevisiae\nSageGermlinePon.hg38.98x.vcf.gz\nSAGE_resources\n</code></pre></p> <p>GRCh37 -&gt; contains only the FASTA file of the genome assembly GRCh37</p> <p>GRCh38 -&gt; contains the genomic sequence of GRCh38 human genome assembly with several index files, as well as some annotation files.</p> <p>iGenomes -&gt; contains the illumina Homo sapiens genomes in different versions and from different origins (NCBI, GATK, UCSC)</p> <p>iGenomes_2022 -&gt; contains the illumina Homo sapiens genome for version GRCh38 and from NCBI</p> <p>purple_resources -&gt; contains two files seemingly related to a panel of normals.</p> <p>Saccharomyces_cerevisiae -&gt; contains information of the genome of this species and it was downloaded from UCSC iGenomes.</p> <p>SageGermlinePon... and SAGE resources are resources related to running SAGE.</p>"},{"location":"Datasets/Other_data/Reference_genomes/#recommendations","title":"Recommendations","text":"<p>It would be good to get used to use the reference files that are here avoiding the creation of multiple copies of the same files all along the cluster.</p>"},{"location":"Datasets/Other_data/Reference_genomes/#reference","title":"Reference","text":"<p>Ferriol Calvet</p> <p>2023/03/16</p>"},{"location":"IRB/VPN/","title":"VPN","text":"<p>When working from home or outside the PCB, connecting to the VPN is neeeded in order to have access to resources such as the cluster.</p>"},{"location":"IRB/VPN/#linux-ubuntu-2004","title":"Linux (Ubuntu 20.04)","text":"<p>1 - Install \"Openconnect\"</p> <pre><code>sudo apt update\nsudo apt install openconnect\n</code></pre> <p>2 - Connect to the VPN</p> <pre><code>sudo openconnect --protocol=gp vpnirb.pcb.ub.es\n# You can use the parameter --user to automatically enter the username.\n</code></pre> <p>You can disconnect by just pressing <code>Ctrl+C</code></p>"},{"location":"IRB/VPN/#full-instructions","title":"Full instructions","text":"<p>VPN tutorial - pdf</p>"},{"location":"IRB/VPN/#references","title":"References","text":"<ul> <li>Miguel Grau</li> <li>Carlos L\u00f3pez Elorduy</li> </ul>"},{"location":"IRB/eduroam/","title":"Eduroam","text":""},{"location":"IRB/eduroam/#configuration","title":"Configuration","text":"<ul> <li>Security: WPA &amp; WPA2 Enterprise</li> <li>Authentication: Protected EAP (PEAP)</li> <li>Anonymous identity: Empty</li> <li>Check the checkbox \"No CA certificate is required\"</li> <li>PEAP version: Automatic</li> <li>Inner authentication: MSCHAPv2</li> <li>Username: clusteruser@irbbarcelona.org</li> <li>Password: IRB password</li> </ul>"},{"location":"IRB/eduroam/#example","title":"Example","text":""},{"location":"IRB/eduroam/#reference","title":"Reference","text":"<ul> <li>Martina Gasull</li> <li>Carlos L\u00f3pez Elorduy</li> <li>Miguel Grau</li> </ul>"},{"location":"Plots_and_scripts/HierarchicalClustering/","title":"Hierarchical Clustering","text":""},{"location":"Plots_and_scripts/HierarchicalClustering/#python","title":"Python","text":"<pre><code>import numpy as np\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\n# condensed distance array\nydist = np.array([662., 877., 255., 412., 996., 295., 468., 268., 400., 754., 564., 138., 219., 869., 669.])\n# linkage object\nZ = hierarchy.linkage(ydist, 'single')\n# compute and plot dendrogram\ndn = hierarchy.dendrogram(Z)\n</code></pre>"},{"location":"Plots_and_scripts/HierarchicalClustering/#reference","title":"Reference","text":"<ul> <li>Ferran Mui\u00f1os</li> </ul>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/","title":"IntOGen - boostDM plots","text":"<p>Examples of how to generate different types of plots found in the IntOGen + boostDM suite.</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#intogen","title":"IntOGen","text":"<p>(Pending)</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#boostdm","title":"boostDM","text":"<p>All the code for figures of the boostDM paper are included in the following public repo: boostDM paper analyses repo.  Find below references to the publicly available repo as well as to example scripts stored in the bbg-cluster <code>/workspace</code>.</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#precision-recall-curves","title":"Precision-Recall Curves","text":""},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#public-repo","title":"Public repo","text":"<p>Figure 1 code, Manuscript Figure 1 PDF</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#bbg-workspace","title":"BBG Workspace","text":"<p>Notebook: <code>/workspace/projects/boostdm_ch/notebooks/benchmarks-precision-recall.ipynb</code></p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#radar-plots","title":"Radar plots","text":""},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#public-repo_1","title":"Public repo","text":"<p>Figure 2 code, Manuscript Figure 2 PDF</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#bbg-workspace_1","title":"BBG Workspace","text":"<p>Notebook: <code>/workspace/projects/boostdm_ch/notebooks/run8_20220705_feature-landscape/BoostDM-CH_run20220705_blueprints_SDM_JER.ipynb</code></p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#blueprints","title":"Blueprints","text":""},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#public-repo_2","title":"Public repo","text":"<p>Figure 3 code, Manuscript Figure 3 PDF</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#bbg-workspace_2","title":"BBG Workspace","text":"<p>Notebook: <code>/workspace/projects/boostdm_ch/notebooks/run8_20220705_feature-landscape/BoostDM-CH_run20220705_blueprints_SDM_JER.ipynb</code></p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#reference","title":"Reference","text":"<ul> <li>Ferran Mui\u00f1os</li> <li>Santi Demajo</li> <li>Joan Enric Ramis</li> </ul>"},{"location":"Plots_and_scripts/Mutational_profile/","title":"Mutational profile","text":"<p>The function <code>plot_signature</code>  will allow you to plot the mutational profile of a sample given the vector of 96 channels (see y axis in the example figure) with the frequencies of each nucleotide change. It takes as input the the vector with the mutations frequency  (<code>profile</code>) and the title of the plot (<code>title</code>).</p> <p>The function <code>minor_tick_labels</code>  is needed to generate the labels of the plot.</p>"},{"location":"Plots_and_scripts/Mutational_profile/#example","title":"Example","text":""},{"location":"Plots_and_scripts/Mutational_profile/#function","title":"Function","text":"<pre><code>import seaborn as sns\nimport numpy as np\ndef minor_tick_labels():\nmajor_labels = ['C&gt;A', 'C&gt;G', 'C&gt;T', 'T&gt;A', 'T&gt;C', 'T&gt;G']\nflanks = ['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT',\n'GA', 'GC', 'GG', 'GT', 'TA', 'TC', 'TG', 'TT']\nminor_labels = []\nfor subs in major_labels:\nfor flank in flanks:\nminor_labels.append(flank[0] + subs[0] + flank[1])\nreturn minor_labels\ndef plot_signature(profile, title=None):\n\"\"\"\n    Args:\n        profile: 96-array in lexicographic order\n        title: string\n    Returns:\n        produces the signature bar plot\n    \"\"\"\nfig, ax = plt.subplots(figsize=(15, 2))\ntotal = np.sum(profile)\nif abs(total - 1) &gt; 0.01:\nprofile = profile / total\nsns.set(font_scale=1.5)\nsns.set_style('white')\n# bar plot\nbarlist = ax.bar(range(96), profile)\ncolor_list = ['#72bcd4', 'k', 'r', '#7e7e7e', 'g', '#e6add8']\nfor category in range(6):\nfor i in range(16):\nbarlist[category * 16 + i].set_color(color_list[category])\nax.set_xlim([-0.5, 96])\nymax = np.max(profile) * 1.2\nax.set_ylim(0, ymax)\n# ax.set_ylabel('subs rel freq')\nlabels = ['C&gt;A', 'C&gt;G', 'C&gt;T', 'T&gt;A', 'T&gt;C', 'T&gt;G']\nmajor_ticks = np.arange(8, 8 + 16 * 5 + 1, 16)\nminor_ticks = np.arange(0.2, 96.2, 1)\nax.tick_params(length=0, which='major', pad=30, labelsize=12)\nax.tick_params(length=0, which='minor', pad=5, labelsize=10)\nax.set_xticks(major_ticks, minor=False)\nax.set_xticklabels(labels, minor=False)\nax.set_xticks(minor_ticks, minor=True)\nax.set_xticklabels(minor_tick_labels(), minor=True, rotation=90)\nax.spines['top'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_title(title, fontsize=24)\nplt.show()\n</code></pre> <p>Note</p> <ul> <li>The function normalizes the vector so that the sum of all the frequencies is equal to 1.</li> <li>If you want to normalize the frequencies so that the trinucleotide composition of the genomic regions from which the mutations have been obtained, you need to normalize the vector taking into account the trinucleotide composition before using the function plot_signature.</li> </ul>"},{"location":"Plots_and_scripts/Mutational_profile/#normalization-of-the-vector","title":"Normalization of the vector","text":"<p>In order to normalize the vector you will need to import from bgreference the reference genome in which the data has been sequenced.</p> <p>You will also need the vector with the mutations frequency (<code>profile</code>) and the directory of a file with the genomic regions from which the mutations have been obtained (<code>regions_file_dir</code>), with at least the columns: <code>CHROMOSOME</code>, <code>START</code>, <code>END</code>.</p>"},{"location":"Plots_and_scripts/Mutational_profile/#needed-functions","title":"Needed functions","text":"<pre><code>from itertools import product\nimport pandas as pd\nimport numpy as np\ncb = dict(zip('ACGT','TGCA'))\ndef triplet_index(triplet):\na, ref, b = tuple(list(triplet))\ns = 16 * (ref == 'T')\nt = 4 * ((a == 'C') + 2 * (a == 'G') + 3 * (a == 'T'))\nu = (b == 'C') + 2 * (b == 'G') + 3 * (b == 'T')\nreturn s + t + u\ndef sbs_format(triplet_count):\n\"\"\"Maps ref triplets to 96 SBS channel\"\"\"\nvector = []\nfor ref in 'CT':\nfor alt in 'ACGT':\nif alt != ref:\nfor a, b in product(cb, repeat=2):\nvector.append(triplet_count[triplet_index(a + ref + b)])\nreturn vector\ndef triplets():\nfor ref in 'CT':\nfor a, b in product(cb, repeat=2):\nyield a + ref + b\ndef count_triplets(seq):\nreturn [seq.count(t) + seq.count(rev(t)) for t in triplets()]\ndef rev(seq):\n\"\"\"reverse complement of seq\"\"\"\nreturn ''.join(list(map(lambda s: cb[s], seq[::-1]))) \ndef get_triplet_counts_region(regions_file_dir,reference_genome=hg38):\n\"\"\"\n    Function to obtain the vector with 96 triplet counts given the regions file.\n    \"\"\"\nregions=pd.read_csv(regions_file_dir,sep='\\t',dtype={'CHROMOSOME':'string'})\nassert(np.all(regions.apply(lambda r: r['END'] - r['START'] + 1 &gt; 0, axis=1)))\nregions['interval'] = regions.apply(lambda r: (r['CHROMOSOME'], r['START'], r['END']), axis=1)\ncounts = np.zeros(32)\nfor chrom, start, end in list(regions['interval']):\ntry:\nseq = reference_genome(chrom, start-1, size=end-start+3)  # sequence +1 nt 5' and 3' flanking nucleotides\nc = np.array(count_triplets(seq))\ncounts += c\nexcept Exception as e:\nprint(e)\nreturn sbs_format(list(map(int, counts)))\n</code></pre>"},{"location":"Plots_and_scripts/Mutational_profile/#normalization","title":"Normalization","text":"<pre><code>region_triplet_abundance=get_triplet_counts_region(regions_file_dir)\nnormalized_profile = np.array(profile)/np.array(region_triplet_abundance)\n</code></pre>"},{"location":"Plots_and_scripts/Mutational_profile/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Ferran Mui\u00f1os</li> </ul>"},{"location":"Plots_and_scripts/Needle_plot/","title":"Needle plot","text":""},{"location":"Plots_and_scripts/Needle_plot/#description","title":"Description","text":"<p>A needle plot displays vertical lines that connect the data points to a horizontal baseline.  Needle plots are useful when you want to plot frequency of mutations on the protein body.</p> <p>The <code>needle_plot</code> function allows you to perform the needle plot with the representation of the protein body including domains. It requires an input matrix  including gene name column called <code>'gene'</code>, nucleotide position column called <code>'Protein_position'</code> and the number of mutations affecting the same position <code>'number_observed_muts'</code>.  The function also needs the gene name and the transcript used. </p>"},{"location":"Plots_and_scripts/Needle_plot/#function","title":"Function","text":"<pre><code>def needle_plot(data, gene, transcript):\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.rcParams['font.sans-serif'] = ['arial']\nplt.rcParams['font.size'] = 6\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['svg.fonttype'] = 'none'\nplt.rcParams['mathtext.fontset'] = 'custom'\nplt.rcParams['mathtext.cal'] = 'arial'\nplt.rcParams['mathtext.rm'] = 'arial'\nmpl.rcParams['figure.dpi']= 200\n# get PFAM domains and subset the mutation data\nsubset_data_pfam = get_PFAMs_per_transcript(PFAM_files, PFAM_info, transcript)\n# define figure layout\nfig = plt.figure(figsize=(8, 2.25))\n# ! SDM change\ngs = gridspec.GridSpec(11, 3, figure=fig)\nax1 = plt.subplot(gs[1:-1, :2])\nax2 = plt.subplot(gs[-1, :2], sharex=ax1)\n# plot for each axes\nplot_gene_full_nucleotide(subset_data_pfam, data, gene, transcript, path_coord, ax1, ax2)\nax2.set_xlabel('CDS base position')\nplt.show()\n</code></pre>"},{"location":"Plots_and_scripts/Needle_plot/#example","title":"Example","text":"<p><pre><code>needle_plot(data, 'DNMT3A', 'ENST00000264709')\n</code></pre> </p>"},{"location":"Plots_and_scripts/Needle_plot/#dependencies","title":"Dependencies","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib import collections as mc\nfrom matplotlib import gridspec\nfrom collections import defaultdict\nfrom scipy.stats import norm\nimport matplotlib as mpl\n### PFAM info\nPFAM_files = \"/workspace/datasets/intogen/runs/20200703_oriolRun/CH_IMPACT_out/intogen_merge_20220325/pfam/pfam_biomart.tsv.gz\"\nPFAM_info = \"/workspace/datasets/intogen/runs/20200703_oriolRun/CH_IMPACT_out/intogen_merge_20220325/pfam/pfam_names.info.csv\"\n### CDS coordinates\npath_coord =  \"/workspace/datasets/intogen/runs/20200703_oriolRun/CH_IMPACT_out/intogen_merge_20220325/cds_biomart.tsv\"\n## Functions\ndef get_PFAMs_per_transcript(PFAM_files, PFAM_info, transcript):\ndf_pfam = pd.read_csv(PFAM_files, sep=\"\\t\", names=[\"ENSEMBL_GENE\", \"ENSEMBL_TRANSCRIPT\", \"START\", \"END\", \"DOMAIN\"])\ndf_names = pd.read_csv(PFAM_info, sep=\"\\t\", names=[\"DOMAIN\", \"CLAN\", \"CLAN_NAME\", \"DOMAIN_NAME\", \"Long Name\"])\n# Get domains\ndf_pfam_gene = df_pfam[(df_pfam[\"ENSEMBL_TRANSCRIPT\"] == transcript)]\ndf_pfam_gene = df_pfam_gene[[\"ENSEMBL_TRANSCRIPT\", \"START\", \"END\", \"DOMAIN\"]].drop_duplicates()\ndf_pfam_gene = pd.merge(df_pfam_gene, df_names[[\"DOMAIN\", \"DOMAIN_NAME\"]].drop_duplicates(), how=\"left\")\ndf_pfam_gene[\"POS\"] = df_pfam_gene.apply(lambda row: row[\"START\"] + ((row[\"END\"] - row[\"START\"]) // 2), axis=1)\ndf_pfam_gene[\"SIZE\"] = df_pfam_gene.apply(lambda row: row[\"END\"] - row[\"START\"] + 1, axis=1)\ndf_pfam_gene[\"Color\"] = \"#998ec3\"\nreturn df_pfam_gene\ndef get_positions_in_CDS(transcript, path_coord):\ndf = pd.read_csv(path_coord, sep='\\t', low_memory=False,\nnames=['gene', 'gene_symbol', 'prot', 'chr', 's', 'e', 'aa', 'cds', 'genpos',\n'strand', 'transcript'])\ntoappend = []\nstrand = ''\nfor i, row in df[df['transcript'] == transcript].sort_values(by='s').iterrows():\ntoappend.extend([i for i in range(row['s'], row['e'] + 1)])\nstrand = row['strand']\nif strand == -1:\ntoappend = toappend[::-1]\nreturn toappend\ndef plot_gene_full_nucleotide(subset_data_pfam, df, gene, transcript, path_coord, ax0, ax1):\n# remove those mutations not falling in CDS:\ndf = df[df['AA'] != 'n']\ndf = df[df['gene'] == gene]\n# Configure the axis\nax0.set_title('Observed mutations')\nax0.set_ylabel(\"mutation count\")\nax0.spines['bottom'].set_visible(False)\nax0.spines['left'].set_linewidth(1)\nax0.spines['right'].set_visible(False)\nax0.spines['top'].set_visible(False)\nax0.tick_params(axis='y', labelsize=6, pad=0.25, width=0.25, length=1.5)\nax1.tick_params(axis='x', length=0)\nax1.set_yticks([])\n#ax0.set_xticks([])\n# set equivalent coordinates for the three possible mutations\nset_coordinates = get_positions_in_CDS(transcript, path_coord)\n# we need to get the set of equivalent coordinates per gene\nequivalent_coordinates = {coord: i for i, coord in enumerate(set_coordinates)}\nvals_coord = list(equivalent_coordinates.values())\naxs = [ax0]\nfor ax in axs:\nax.set_xlim(np.min(vals_coord), np.max(vals_coord))\n# plot observed mutations\npos_list = df[\"pos\"].tolist()\nys = df[\"number_observed_muts\"].values\ncoordinates_mutations = []\nx_axis = []\ny_axis = []\n# for each of the positions\nfor i, p in enumerate(pos_list):\nif ys[i] &gt; 0:\ncoordinates_mutations.append([(equivalent_coordinates[p], 0),\n(equivalent_coordinates[p], ys[i] - 0.1)])\nx_axis.append(equivalent_coordinates[p])\ny_axis.append(ys[i])\nlc = mc.LineCollection(coordinates_mutations, colors='black', linewidths=1, alpha=0.3)\nax0.add_collection(lc)\nsize = 12\nax0.scatter(x_axis, y_axis, s=size, c='red', alpha=0.7)\nax1.set_ylim(0, 1)\nfor i, r in subset_data_pfam.iterrows():\nstart_base = 3 * r['START']\nsize_base = 3 * r['SIZE']\nrect = patches.Rectangle(xy=(start_base, 0), width=size_base, height=5, color=r[\"Color\"], alpha=0.5, zorder=2)\nax1.annotate(s=r[\"DOMAIN_NAME\"], xy=(start_base + 1, 0.3), fontsize=7)\nax1.add_patch(rect)\n</code></pre>"},{"location":"Plots_and_scripts/Needle_plot/#reference","title":"Reference","text":"<p>Joan Enric</p>"},{"location":"Tools/Conda/","title":"Conda","text":""},{"location":"Tools/Conda/#description","title":"Description","text":"<p>Conda is an open source package management system and environment management system that runs on Windows, macOS, Linux and z/OS. Conda quickly installs, runs and updates packages and their dependencies.</p>"},{"location":"Tools/Conda/#get-started","title":"Get started","text":"<pre><code>conda create -n &lt;env name&gt; &lt;package[=&lt;version&gt;]&gt;\nconda activate &lt;env name&gt;\nconda install &lt;package[=version]&gt;\n</code></pre>"},{"location":"Tools/Conda/#cheatsheet","title":"Cheatsheet","text":""},{"location":"Tools/Conda/#environments","title":"Environments","text":"<p>Activate an environment:</p> <pre><code>conda activate &lt;environment name&gt;\n</code></pre> <p>Deactivates an environment. If in <code>base</code>, closes conda.</p> <pre><code>conda deactivate\n</code></pre> <p>List all environments:</p> <pre><code>conda env list\n</code></pre> <pre><code>conda info --envs\n</code></pre> <p>Create a new virtual environment with  <pre><code>conda create --name &lt;environment name&gt; [&lt;packages[=&lt;version&gt;]&gt;]\n</code></pre> <p>Export active environment to a file</p> <pre><code>conda env export &gt; environment.yml\n</code></pre> <p>Export all environments to its own file:</p> <pre><code>for env in $(conda env list | cut -d\" \" -f1); do if [[ ${env:0:1} == \"#\" ]] ; then continue; fi;\nconda env export -n $env &gt; ${env}.yml\ndone\n</code></pre> <p>Create environment from file</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Clone an environment</p> <pre><code>conda create --name &lt;environment name&gt; --clone &lt;original environment&gt;\n</code></pre> <p>Remove an environment</p> <pre><code>conda env remove --name &lt;environment name&gt;\n</code></pre> <pre><code>conda remove --name &lt;environment name&gt; --all\n</code></pre> <p>List all packages installed (in current environment)</p> <pre><code>conda list\n</code></pre> <p>List all packages installed with path</p> <pre><code>conda list --explicit\n</code></pre> <p>Show history of changes in packages</p> <pre><code>conda list --revisions\n</code></pre>"},{"location":"Tools/Conda/#packages","title":"Packages","text":"<p>Install a package (use <code>-f</code> to force the installation)</p> <pre><code>conda install &lt;package&gt;\n</code></pre> <p>Install package(s) specified in a file (like a Python requirements file)</p> <pre><code>conda install --file &lt;file&gt;\n</code></pre> <p>Uninstall a package</p> <pre><code>conda remove &lt;package&gt;\n</code></pre> <p>Search for a package</p> <pre><code>conda search &lt;package&gt;\n</code></pre>"},{"location":"Tools/Conda/#configuration","title":"Configuration","text":"<p>Show configuration</p> <pre><code>conda config --show\n</code></pre> <p>Add channels (use <code>add</code> instead of <code>append</code> to put the channel on the top of the list)</p> <pre><code>conda config --append channels &lt;channel name&gt;\n</code></pre>"},{"location":"Tools/Conda/#building-packages","title":"Building Packages","text":"<p>Install conda build</p> <pre><code>conda install conda-build\n</code></pre> <p>Build package</p> <pre><code>conda build &lt;directory with the files&gt;\n</code></pre> <p>Build for other platforms</p> <pre><code>conda convert --platform all &lt;path to package&gt;\n</code></pre> <p>Install built package</p> <pre><code>conda install --use-local &lt;package&gt;\n</code></pre>"},{"location":"Tools/Conda/#from-pypi","title":"From Pypi","text":"<p>Create files</p> <pre><code>conda skeleton pypi &lt;package&gt;\n</code></pre> <p>Build for different Python version</p> <pre><code>conda build --python &lt;version&gt; &lt;directory with the files&gt;\n</code></pre>"},{"location":"Tools/Conda/#custom-channel","title":"Custom Channel","text":"<p>Add channel</p> <pre><code>conda config --append channels file://&lt;path to folder&gt;\n</code></pre> <p>(re)build the index</p> <pre><code>conda index &lt;channel folder&gt;/&lt;platform&gt;\n</code></pre>"},{"location":"Tools/Conda/#reference","title":"Reference","text":"<ul> <li>Jordi Deu Pons</li> <li>Miguel Grau</li> <li>Federica Brando</li> <li>Carlos L\u00f3pez</li> </ul>"},{"location":"Tools/Docker/","title":"Docker","text":""},{"location":"Tools/Docker/#description","title":"Description","text":""},{"location":"Tools/Docker/#reference","title":"Reference","text":""},{"location":"Tools/Nextflow/","title":"Nextflow","text":"<p>Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of data-driven, computational pipelines written in the most common scripting languages.</p>"},{"location":"Tools/Nextflow/#usage","title":"Usage","text":"<p>To run the default installed version of Nextflow, simply load the <code>nextflow</code> module:</p> <pre><code>$ module load nextflow\n$ nextflow help\nUsage: nextflow [options] COMMAND [arg...]\n</code></pre> <p>For usage documentation, run <code>nextflow help</code>.</p>"},{"location":"Tools/Nextflow/#submitting-processes-as-serial-jobs","title":"Submitting processes as serial jobs","text":"<p>Recommended for serial jobs only</p> <p>This section is recommended for serial jobs only. For parallel jobs, please see the Parallel jobs section below.</p> <p>Nextflow supports the ability to submit pipeline scripts as separate cluster jobs using the SGE executor.</p> <p>To enable the SGE executor, simply set to <code>process.executor</code> property to sge in a configuration file named <code>nextflow.config</code> in the job working directory. The amount of resources requested by each job submission is defined in the cluster options section, where all Univa scheduler resources are supported.</p> <p>For example, to run all pipeline jobs with 2 serial cores and 2GB of memory for 1 hour, create the following configuration file:</p> <pre><code>process.executor='sge'\nprocess.clusterOptions='-pe smp 2 -l h_vmem=1G,h_rt=1:0:0'\n</code></pre> <p>Setting the memory limit for serial jobs</p> <p>Add the <code>-DXmx</code> option to limit the amount of memory Nextflow can use in serial jobs. For more information regarding the Java VM memory allocation, see here.</p>"},{"location":"Tools/Nextflow/#parallel-jobs","title":"Parallel jobs","text":"<p>Parallel jobs will use the in-built Apache Ignite clustering platform; Execution will be performed on the nodes requested in the submit request over MPI rather than submitting new jobs for each pipeline.</p> <p>Do not use the SGE executor in parallel jobs</p> <p>Using the SGE executor for parallel jobs causes the master job to hang until it is killed by the scheduler for exceeding walltime. This is due to Apache Ignite not being able to communicate to other pipeline scripts submitted as separate jobs.</p> <p>To ensure parallel jobs use Apache Ignite, add the following to the configuration file (or omit the process.executor setting):</p> <pre><code>process.executor='ignite'\n</code></pre>"},{"location":"Tools/Nextflow/#example-jobs","title":"Example jobs","text":""},{"location":"Tools/Nextflow/#serial-job","title":"Serial job","text":"<p>Here is an example job taken from the Nextflow website to submit each process in the <code>input.nf</code> file as a new cluster job with 1 core and 1GB of memory. Ensure the cumulative runtime across all processes does not exceed the runtime requested in the master job:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\nmodule load nextflow\n\nnextflow -DXmx=1G \\\n         -C nextflow.config \\\n         run input.nf\n</code></pre>"},{"location":"Tools/Nextflow/#parallel-job","title":"Parallel job","text":"<p>Here is an example job taken from the Nextflow website to run each process in the <code>input.nf</code> file using 48 cores across 2 sdv nodes with Apache Ignite:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe parallel 48\n#$ -l infiniband=sdv-i\n#$ -l h_rt=240:0:0\n\nmodule load nextflow openmpi\n\nmpirun --pernode \\\n       nextflow run input.nf \\\n       -with-mpi\n</code></pre>"},{"location":"Tools/Nextflow/#links","title":"Links","text":"<ul> <li>Nextflow documentation</li> <li>Nextflow basic pipeline example</li> <li>Nextflow presentation videos</li> <li>Nextflow community support</li> <li>Nextflow MPI</li> <li>Apache Ignite</li> </ul>"},{"location":"Tools/Nextflow/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Carlos L\u00f3pez Elorduy</li> <li>Miguel Grau</li> </ul>"},{"location":"Tools/BBG-tools/BGconfig/","title":"BGconfig","text":""},{"location":"Tools/BBG-tools/BGconfig/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGconfig/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGdata/","title":"BGdata","text":""},{"location":"Tools/BBG-tools/BGdata/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGdata/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGlogs/","title":"BGlogs","text":""},{"location":"Tools/BBG-tools/BGlogs/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGlogs/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGpack/","title":"BGpack","text":""},{"location":"Tools/BBG-tools/BGpack/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGpack/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGreference/","title":"BgReference","text":""},{"location":"Tools/BBG-tools/BGreference/#description","title":"Description","text":"<p>BgReference is a library to fast retrive Genome Reference partial sequences.</p>"},{"location":"Tools/BBG-tools/BGreference/#instalation","title":"Instalation","text":"<p><pre><code>conda install -c conda-forge -c bbglab bgreference\n</code></pre> or <pre><code>pip install bgreference\n</code></pre></p>"},{"location":"Tools/BBG-tools/BGreference/#examples","title":"Examples","text":"<pre><code>from bgreference import hg19, hg38\n# Get 10 bases from chromosome one build hg19\nhg19('1', 12345, size=10)\n# Get the sequence of the whole chromosome\nhg19('1',(1), size=None)\n# You can use synonymous sequence names\nhg19(2, 23456)\nhg19('2', 23456)\nhg19('chr2', 23456)\nhg19('MT', 234, size=3)\nhg19('chrM', 234, size=3)\nhg19('chrMT', 234, size=3)\n</code></pre>"},{"location":"Tools/BBG-tools/BGreference/#repository","title":"Repository","text":"<p>Click here to see the repository of BgReference.</p>"},{"location":"Tools/BBG-tools/BGreference/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Paula Gomis </li> </ul>"},{"location":"Tools/BBG-tools/BGsignature/","title":"BGsignature","text":""},{"location":"Tools/BBG-tools/BGsignature/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGsignature/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGvep/","title":"BGvep","text":""},{"location":"Tools/BBG-tools/BGvep/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGvep/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/OpenVariant/","title":"OpenVariant","text":""},{"location":"Tools/BBG-tools/OpenVariant/#description","title":"Description","text":"<p>OpenVariant is a comprehensive Python package that provides different functionalities to read, parse and operate different multiple input file formats (e. g. <code>tsv</code>, <code>csv</code>, <code>vcf</code>, <code>maf</code>, <code>bed</code>), being able to build an unified output with  a proper annotation file structure.</p>"},{"location":"Tools/BBG-tools/OpenVariant/#usage","title":"Usage","text":"<p>Click here to see the installation guide and the complete documentation of OpenVariant.</p> <p>When working with OpenVariant, we need to distinguish 3 different types of files: <code>input files</code> and <code>annotation file</code>, which are provided by the user and <code>output file</code>, which will returned from the function.</p> <ul> <li><code>Input files</code> will be the group of files in different formats (e.g. tsv, csv, vcf, maf, bed) that we want to parse. </li> <li><code>Annotation file</code> is a YAML file which describes how the <code>input files</code> are processed and how the <code>output file</code> will look like.</li> <li><code>Output files</code> are generated by OpenVariant and they are the result of the process.</li> </ul>"},{"location":"Tools/BBG-tools/OpenVariant/#functions","title":"Functions","text":"<p>OpenVariant has several functions to perform different tasks: </p> <ul> <li><code>find_files</code>: Find files with a given pattern name in a given folder.</li> <li><code>Variant</code>: Parse an input file through the annotation file. It will generate an object which you can apply different functionalities</li> <li><code>cat</code>: It will show on the stdout (standard out) the whole parsed output. </li> <li><code>group_by</code>: It will generate an iterator that will contain three variables: <code>group_key</code> (the value of each group), <code>group_result</code> (a list of all rows that pertain to each group) and <code>command</code> (if it uses the <code>script</code> parameter or not). It will group the parsed result for each different value of the specified <code>key_by</code>.</li> <li><code>count</code>: It returns the number of rows that matches a specific conditions.</li> </ul> <p>Click here to see several examples of each of the functions from OpenVariant.</p>"},{"location":"Tools/BBG-tools/OpenVariant/#parameters","title":"Parameters","text":"<p>The different options and parameters of these functions are specified in the <code>annotation file</code>, which has several required and optional parameters.   Click here to learn about the parameters in the <code>annotation file</code> and a to see a template of the <code>annotation file</code>.</p>"},{"location":"Tools/BBG-tools/OpenVariant/#reference","title":"Reference","text":"<ul> <li>David Mart\u00ednez</li> <li>Paula Gomis</li> </ul>"},{"location":"Tools/Sequencing_tools/Bedtools/","title":"Bedtools","text":"<p>The Bedtools suite is a collection of tools for a wide-range of genomics analysis tasks.</p>"},{"location":"Tools/Sequencing_tools/Bedtools/#installation","title":"Installation","text":"<p>You can check the installation guide here.</p>"},{"location":"Tools/Sequencing_tools/Bedtools/#usage","title":"Usage","text":"<p>To run the default installed version of Bedtools, simply load the bedtools module:</p> <pre><code>$ bedtools -h\n\nUsage:   bedtools &lt;subcommand&gt; [options]\nFor full usage documentation, run bedtools -h.\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bedtools/#example-job","title":"Example job","text":""},{"location":"Tools/Sequencing_tools/Bedtools/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n# Report the base-pair overlap between the features in two BED files.\nbedtools intersect -a reads.bed -b genes.bed\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bedtools/#links","title":"Links","text":"<ul> <li>Bedtools GitHub</li> <li>Bedtools documentation</li> </ul>"},{"location":"Tools/Sequencing_tools/Bedtools/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Tools/Sequencing_tools/Bowtie2/","title":"Bowtie2","text":"<p>Bowtie2 is an ultra fast and memory-efficient tool for aligning sequencing reads to long reference sequences.</p>"},{"location":"Tools/Sequencing_tools/Bowtie2/#installation","title":"Installation","text":"<p>You can check the installation documentation here.</p>"},{"location":"Tools/Sequencing_tools/Bowtie2/#conda","title":"Conda","text":"<pre><code>conda install -c bioconda bowtie2\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bowtie2/#package-manager","title":"Package manager","text":"<pre><code>sudo apt update\nsudo apt install bowtie2\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bowtie2/#manually-on-ubuntulinux","title":"Manually on Ubuntu/Linux","text":"<p>Create and go to install directory</p> <pre><code>cd $HOME/tools/bowtie2/\n</code></pre> <p>Download Ubuntu/Linux version</p> <pre><code>wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.4.2/bowtie2-2.4.2-sra-linux-x86_64.zip/download\n</code></pre> <p>Decompress</p> <pre><code>unzip download\n</code></pre> <p>Add location to system PATH</p> <pre><code>export PATH=$HOME/tools/bowtie2/bowtie2-2.4.2-sra-linux-x86_64:$PATH\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bowtie2/#check-installation","title":"Check installation","text":"<pre><code>bowtie2 --help\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bowtie2/#usage","title":"Usage","text":"<pre><code>$ bowtie2 -h\n\nUsage:   bowtie2 [options]* -x &lt;bt2-idx&gt; {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt; |\n--interleaved &lt;i&gt;} -S [&lt;sam&gt;]\n</code></pre> <p>For full usage documentation, run <code>bowtie2 -h</code>.</p>"},{"location":"Tools/Sequencing_tools/Bowtie2/#example-job","title":"Example job","text":""},{"location":"Tools/Sequencing_tools/Bowtie2/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n# Prepare example genomes in &lt;inputDir&gt;\n# Output is stored in &lt;outputDir&gt;\nbowtie2-build &lt;inputDir&gt; &lt;outputDir&gt;\nbowtie2-inspect &lt;outputDir&gt;\n</code></pre>"},{"location":"Tools/Sequencing_tools/Bowtie2/#links","title":"Links","text":"<ul> <li>Bowtie2 GitHub</li> <li>Bowtie2 example</li> </ul>"},{"location":"Tools/Sequencing_tools/Bowtie2/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Tools/Sequencing_tools/Cutadapt/","title":"Cutadapt","text":""},{"location":"Tools/Sequencing_tools/Cutadapt/#description","title":"Description","text":""},{"location":"Tools/Sequencing_tools/Cutadapt/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Fastqc/","title":"Fastqc","text":"<p>FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines.</p>"},{"location":"Tools/Sequencing_tools/Fastqc/#installation","title":"Installation","text":""},{"location":"Tools/Sequencing_tools/Fastqc/#apt-get","title":"apt-get","text":"<pre><code>sudo apt-get update\nsudo apt-get install fastqc\n</code></pre>"},{"location":"Tools/Sequencing_tools/Fastqc/#apt","title":"apt","text":"<pre><code>sudo apt update\nsudo apt install fastqc\n</code></pre>"},{"location":"Tools/Sequencing_tools/Fastqc/#usage","title":"Usage","text":"<pre><code>$ fastqc --help\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\nfastqc seqfile1 seqfile2 .. seqfileN\n\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam]\n[-c contaminant file] seqfile1 .. seqfileN\n</code></pre>"},{"location":"Tools/Sequencing_tools/Fastqc/#example-job","title":"Example job","text":""},{"location":"Tools/Sequencing_tools/Fastqc/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\nfastqc raw_data.fastq.gz raw_data2.fastq.gz\n</code></pre> <p>Viewing the Fastqc results</p> <p>To view the Fastqc results, you may open the fastqc_report.html file in a web browser or the summary.txt file (located in the zipped output archive) on the command line. For assistance copying files to your local machine, please see the Moving Data page.</p>"},{"location":"Tools/Sequencing_tools/Fastqc/#links","title":"Links","text":"<ul> <li>FastQC website</li> <li>FastQC manual</li> <li>FastQC video tutorial</li> </ul>"},{"location":"Tools/Sequencing_tools/Fastqc/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Tools/Sequencing_tools/Samtools/","title":"SAMtools","text":"<p>SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments.</p>"},{"location":"Tools/Sequencing_tools/Samtools/#installation","title":"Installation","text":""},{"location":"Tools/Sequencing_tools/Samtools/#conda","title":"Conda","text":"<pre><code>conda install -c bioconda samtools\n</code></pre>"},{"location":"Tools/Sequencing_tools/Samtools/#manual","title":"Manual","text":"<p>For the manual installation, you can find the instructions here.</p>"},{"location":"Tools/Sequencing_tools/Samtools/#usage","title":"Usage","text":"<pre><code>samtools view -b -S -o genome_reads_aligned.bam genome_reads_aligned.sam\n</code></pre> <p>Core Usage</p> <p>To ensure that SAMtools uses the correct number of cores, the <code>-@ ${NSLOTS}</code> option should be used on commands that support it.</p>"},{"location":"Tools/Sequencing_tools/Samtools/#example-job","title":"Example job","text":""},{"location":"Tools/Sequencing_tools/Samtools/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 4 cores and 8GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 4\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=2G\nsamtools view -@ ${NSLOTS} -b -S -o genome_reads_aligned.bam \\\ngenome_reads_aligned.sam\n\nsamtools sort -@ ${NSLOTS} genome_reads_aligned.bam \\\n&gt; genome_reads_aligned.sorted.bam\n\nsamtools index genome_reads_aligned.sorted.bam\n\nsamtools mpileup -g -f ref_genome_1K.fna genome_reads_aligned.sorted.bam \\\n&gt; genome_variants.bcf\n</code></pre>"},{"location":"Tools/Sequencing_tools/Samtools/#links","title":"Links","text":"<ul> <li>Samtools GitHub</li> <li>Samtools website</li> <li>Samtools documentation</li> </ul>"},{"location":"Tools/Sequencing_tools/Samtools/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Tools/Sequencing_tools/Trimmomatic/","title":"Trimmomatic","text":""},{"location":"Tools/Sequencing_tools/Trimmomatic/#description","title":"Description","text":""},{"location":"Tools/Sequencing_tools/Trimmomatic/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Vep/","title":"Vep","text":""},{"location":"Tools/Sequencing_tools/Vep/#ensembl-vep","title":"Ensembl-VEP","text":"<p>VEP determines the effect of your variants (insertions, deletions and structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions.</p>"},{"location":"Tools/Sequencing_tools/Vep/#usage","title":"Usage","text":"<p>In order to install VEP, you can follow the installation guide.</p>"},{"location":"Tools/Sequencing_tools/Vep/#where-to-install-it","title":"Where to install it?","text":"<p>There is a shared folder in datasets where there are several vep cache versions. If you are planning to download one, make sure to store it in this location.</p> <p><code>/workspace/datasets/vep</code></p> <p>More information will come in order to document the best way of downloading the vep cache.</p>"},{"location":"Tools/Sequencing_tools/Vep/#after-installing","title":"After installing","text":"<p>Simply load the <code>ensembl-vep</code> module:</p> <pre><code>$ vep\n\nUsage:\n./vep [--cache|--offline|--database] [arguments]\n\nBasic options\n=============\n\n--help                 Display this message and quit\n\n-i | --input_file      Input file\n-o | --output_file     Output file\n--force_overwrite      Force overwriting of output file\n--species [species]    Species to use [default: \"human\"]\n\n--everything           Shortcut switch to turn on commonly used options. See web\n                       documentation for details [default: off]\n--fork [num_forks]     Use forking to improve script runtime\n</code></pre> <p>For full option documentation see here.</p> <p>Instructions on how to download and use cached files can be found here.</p> <p>To enable offline mode and use of the cache, pass the <code>--offline</code> and <code>--cache</code> flags.</p>"},{"location":"Tools/Sequencing_tools/Vep/#example-job","title":"Example job","text":""},{"location":"Tools/Sequencing_tools/Vep/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\nvep -i homo_sapiens_GRCh38.vcf \\\n--cache \\\n--offline \\\n--output_file results\n</code></pre>"},{"location":"Tools/Sequencing_tools/Vep/#additional-comments","title":"Additional comments","text":"<p>Be careful when running VEP with the TAB output and then merging again the variants from a VCF file, some indels are reformated in VEP and you cannot pair them with the original mutations.</p>"},{"location":"Tools/Sequencing_tools/Vep/#links","title":"Links","text":"<ul> <li>Ensembl-VEP documentation</li> <li>Ensembl-VEP tutorial</li> <li>Ensembl-VEP examples</li> <li>Ensembl-VEP web application</li> </ul>"},{"location":"Tools/Sequencing_tools/Vep/#reference","title":"Reference","text":"<ul> <li>To be added</li> </ul>"},{"location":"Tools/Sequencing_tools/pysam/","title":"pysam","text":""},{"location":"Tools/Sequencing_tools/pysam/#description","title":"Description","text":""},{"location":"Tools/Sequencing_tools/pysam/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/","title":"HMFtools","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#description","title":"Description","text":"<p>HMFtools is a pipeline that can be run on the BBGlab cluster to perform variant calling and analysis. It resembles the exact pipeline that it is used in Google Cloud Platform (see Platinum) but it does not run in parallel and it is not optimized for HPC computing.</p> <p>The scripts, the tools and the reference data are provided by HMF on their GitHub and Google Cloud.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#installation","title":"Installation","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#create-conda-env-with-requirements","title":"create conda env with requirements","text":"<p>Move to the hmftools pipeline folder:</p> <pre><code>cd /workspace/projects/hartwig/hmftools/pipeline/\n</code></pre> <p>create a conda envirnoment with the <code>environment.yml</code> file.</p> <pre><code>conda env create --file environment.yml\n</code></pre> <p>activate the environment</p> <pre><code>conda activate hmftpipe\n</code></pre> <p>Warning</p> <p>You need specific permission to access the <code>/workspace/projects/hartwig/</code> following folder. Please contact Martina or Paula if you need to use this data. More info here.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#run","title":"Run","text":"<p>It can be run with a <code>.bam</code> file or a <code>.fastq</code> file.</p> BAMFASTQ.gz <p>Run mode</p> WGS modePanel mode <p>to run the pipeline the command is the following: </p> <pre><code>./scripts/run_pipeline &lt;sample_data&gt; \"tumorID,referenceID\" &lt;gen_version&gt; &lt;run_mode&gt; &lt;threads&gt; &lt;memory&gt;\n</code></pre> <p>to run the pipeline the command is the following: </p> <pre><code>./scripts/run_pipeline &lt;sample_data&gt; \"tumorID\" &lt;gen_versio&gt; &lt;run mode&gt; &lt;threads&gt; &lt;memory&gt;\n</code></pre> <p>This option is going to take longer since we are running the alignment step.</p> <p>Run mode</p> WGS modePanel mode <p>to run the pipeline the command is the following: </p> <pre><code>./scripts/run_pipeline &lt;sample_data&gt; \"tumorID,referenceID\" &lt;gen_version&gt; &lt;run_mode&gt; &lt;threads&gt; &lt;memory&gt;\n</code></pre>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#sample-data-directory-output-directory","title":"<code>sample data</code> directory = output directory","text":"<p>The <code>sample_data</code> directory must have:</p> <ul> <li>a directory named as the sample's tumorId</li> <li>tumor and reference BAM and BAM index files in the sample's directory, named as tumorId and referenceId.bam</li> </ul> <p>i.e. see folder tree below</p> <pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 referenceId.bam\n      \u251c\u2500\u2500 referenceId.bam.bai\n      \u251c\u2500\u2500 tumorId.bam\n      \u2514\u2500\u2500 tumorId.bam.bai\n</code></pre> <p>In the tumorID folder the pipeline will make a directory for each of the steps and store the results in subfolders.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#gen_version-parameter","title":"<code>gen_version</code> parameter","text":"Genome version Genome file V38 <code>./ref_data_dir/V38/ref_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna</code> V37 <code>./ref_data_dir/V37/ref_genome/Homo_sapiens.GRCh37.GATK.illumina.fasta</code>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#threads-and-memory-parameters","title":"<code>threads</code> and <code>memory</code> parameters","text":"<ul> <li><code>threads</code> number of threads used for each component</li> <li><code>memory</code>: GB allocated to each component (default=12GB)</li> </ul>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#example","title":"Example","text":"<p>my directory:</p> <p><pre><code>pipeline/test_data/\n \u2514\u2500\u2500 COLO928T\n      \u251c\u2500\u2500 COLO928R.bam\n      \u251c\u2500\u2500 COLO928R.bam.bai\n      \u251c\u2500\u2500 COLO928T.bam\n      \u2514\u2500\u2500 COLO928T.bam.bai\n</code></pre> Run: </p> <pre><code>./scripts/run_pipeline test_data \"COLO928T,COLO928R\" V37 WGS 8 16\n</code></pre> Example output <pre><code>pipeline/test_data/COLO829T\n\u251c\u2500\u2500 amber\n\u2502   \u251c\u2500\u2500 amber.version\n\u2502   \u251c\u2500\u2500 COLO829R.amber.homozygousregion.tsv\n\u2502   \u251c\u2500\u2500 COLO829R.amber.snp.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829R.amber.snp.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.pcf\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.tsv.gz\n\u2502   \u251c\u2500\u2500 COLO829T.amber.contamination.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.amber.contamination.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.amber.contamination.vcf.gz.tbi\n\u2502   \u2514\u2500\u2500 COLO829T.amber.qc\n\u251c\u2500\u2500 cobalt\n\u2502   \u251c\u2500\u2500 cobalt.version\n\u2502   \u251c\u2500\u2500 COLO829R.cobalt.gc.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829R.cobalt.ratio.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829R.cobalt.ratio.pcf\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.gc.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.ratio.pcf\n\u2502   \u2514\u2500\u2500 COLO829T.cobalt.ratio.tsv.gz\n\u251c\u2500\u2500 COLO829R.bam\n\u251c\u2500\u2500 COLO829R.bam.bai\n\u251c\u2500\u2500 COLO829T.bam\n\u251c\u2500\u2500 COLO829T.bam.bai\n\u251c\u2500\u2500 gridss\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.bam\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.junctions.csv\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.bai\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.fragment_lengths\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.junctions.csv\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.bai\n\u2502   \u2514\u2500\u2500 gridss\n\u2502       \u251c\u2500\u2500 COLO829R.bam.gridss.working\n\u2502       \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.computesamtags.changes.tsv\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.insert_size_histogram.pdf\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.insert_size_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.sv.bam.csi\n\u2502       \u2502   \u2514\u2500\u2500 COLO829R.sv_prep.sorted.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 COLO829T.bam.gridss.working\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.alignment_summary_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.coverage.blacklist.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.downsampled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.excluded_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.subsetCalled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam.bai\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.gridss.working\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.allocated.vcf.idx\n\u2502       \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.computesamtags.changes.tsv\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_histogram.pdf\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam.csi\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.sv_prep.sorted.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 gridss.full.20221116_120546.bbgn004.49271.log\n\u2502       \u251c\u2500\u2500 gridss.timing.20221116_120546.bbgn004.49271.log\n\u2502       \u2514\u2500\u2500 libsswjni.so\n\u251c\u2500\u2500 gripss_germline\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.germline.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.germline.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.germline.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.gripss.germline.vcf.gz.tbi\n\u251c\u2500\u2500 gripss_somatic\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.somatic.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.gripss.somatic.vcf.gz.tbi\n\u251c\u2500\u2500 lilac\n\u2502   \u251c\u2500\u2500 COLO829T.candidates.coverage.csv\n\u2502   \u251c\u2500\u2500 COLO829T.lilac.csv\n\u2502   \u2514\u2500\u2500 COLO829T.lilac.qc.csv\n\u251c\u2500\u2500 linx_germline\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.clusters.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.disruption.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.driver.catalog.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.links.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.svs.tsv\n\u2502   \u2514\u2500\u2500 linx.version\n\u251c\u2500\u2500 linx_somatic\n\u2502   \u251c\u2500\u2500 COLO829T.linx.breakend.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.clusters.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.driver.catalog.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.drivers.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.fusion.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.links.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.svs.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_copy_number.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_fusion.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_gene_exon.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_protein_domain.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_segments.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_sv_data.tsv\n\u2502   \u251c\u2500\u2500 linx.version\n\u2502   \u251c\u2500\u2500 plot_data\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.003.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.003.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.chromosome.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.circos.003.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.cna.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.connector.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.distance.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.exon.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.exon.rank.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.fragile.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.gene.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.gene.name.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.karyotype.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.line_element.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.position.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.scatter.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.scatter.sgl.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.segment.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.003.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.003.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.chromosome.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.circos.003.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.cna.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.connector.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.distance.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.exon.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.exon.rank.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.fragile.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.gene.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.gene.name.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.karyotype.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.line_element.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.position.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.scatter.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.scatter.sgl.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.segment.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.003.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.003.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.chromosome.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.circos.003.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.cna.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.connector.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.distance.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.exon.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.exon.rank.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.fragile.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.gene.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.gene.name.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.karyotype.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.line_element.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.position.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.scatter.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.scatter.sgl.circos\n\u2502   \u2502   \u2514\u2500\u2500 COLO829T.cluster-1.sv1.debug.segment.circos\n\u2502   \u2514\u2500\u2500 plots\n\u2502       \u251c\u2500\u2500 COLO829T.chr10.debug.003.png\n\u2502       \u251c\u2500\u2500 COLO829T.chr18.debug.003.png\n\u2502       \u2514\u2500\u2500 COLO829T.cluster-1.sv1.debug.003.png\n\u251c\u2500\u2500 pave_germline\n\u2502   \u251c\u2500\u2500 COLO829T.sage.germline.pave.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.germline.pave.vcf.gz.tbi\n\u251c\u2500\u2500 pave_somatic\n\u2502   \u251c\u2500\u2500 COLO829T.sage.somatic.pave.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.somatic.pave.vcf.gz.tbi\n\u251c\u2500\u2500 purple\n\u2502   \u251c\u2500\u2500 circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829R.ratio.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.baf.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cnv.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.indel.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.ratio.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.snp.circos\n\u2502   \u2502   \u2514\u2500\u2500 gaps.txt\n\u2502   \u251c\u2500\u2500 COLO829T.driver.catalog.germline.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.driver.catalog.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.gene.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.germline.deletion.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.germline.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.germline.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.range.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.qc\n\u2502   \u251c\u2500\u2500 COLO829T.purple.segment.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.clonality.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.hist.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 plot\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.copynumber.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.map.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.purity.range.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.segment.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.clonality.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.png\n\u2502   \u2502   \u2514\u2500\u2500 COLO829T.somatic.rainfall.png\n\u2502   \u2514\u2500\u2500 purple.version\n\u251c\u2500\u2500 run005_37_wgs.log\n\u251c\u2500\u2500 sage_germline\n\u2502   \u251c\u2500\u2500 COLO829T.sage.germline.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.germline.vcf.gz.tbi\n\u2514\u2500\u2500 sage_somatic\n    \u251c\u2500\u2500 COLO829R.sage.bqr.tsv\n    \u251c\u2500\u2500 COLO829T.sage.bqr.tsv\n    \u251c\u2500\u2500 COLO829T.sage.exon.medians.tsv\n    \u251c\u2500\u2500 COLO829T.sage.gene.coverage.tsv\n    \u251c\u2500\u2500 COLO829T.sage.somatic.vcf.gz\n    \u2514\u2500\u2500 COLO829T.sage.somatic.vcf.gz.tbi\n</code></pre>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#sample-data-directory-output-directory_1","title":"<code>sample data</code> directory = output directory","text":"<p>The <code>sample_data</code> must have:</p> <ul> <li>a directory named as the sample's tumorId</li> <li>tumor BAM and BAM index files in the sample's directory, named as tumorId and referenceId.bam</li> </ul> <p>i.e. see folder tree below</p> <pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 tumorId.bam\n      \u2514\u2500\u2500 tumorId.bam.bai\n</code></pre> <p>In the tumorID folder the pipeline will make a directory for each of the steps and store the results in subfolders.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#gen_version-parameter_1","title":"<code>gen_version</code> parameter","text":"Genome version Genome file V38 <code>./ref_data_dir/V38/ref_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna</code> V37 <code>./ref_data_dir/V37/ref_genome/Homo_sapiens.GRCh37.GATK.illumina.fasta</code>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#threads-and-memory-parameters_1","title":"<code>threads</code> and <code>memory</code> parameters","text":"<ul> <li><code>threads</code> number of threads used for each component</li> <li><code>memory</code>: GB allocated to each component (default=12GB)</li> </ul>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#example_1","title":"Example","text":"<p>my directory:</p> <p><pre><code>pipeline/test_data/\n \u2514\u2500\u2500 COLO928T\n      \u251c\u2500\u2500 COLO928T.bam\n      \u2514\u2500\u2500 COLO928T.bam.bai\n</code></pre> Run: </p> <pre><code>./scripts/run_pipeline test_data COLO928T V37 PANEL 8 16\n</code></pre> Example output <pre><code>pipeline/test_data/COLO829T/\n\u251c\u2500\u2500 amber\n\u2502   \u251c\u2500\u2500 amber.version\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.pcf\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.tsv.gz\n\u2502   \u2514\u2500\u2500 COLO829T.amber.qc\n\u251c\u2500\u2500 cobalt\n\u2502   \u251c\u2500\u2500 cobalt.version\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.gc.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.ratio.pcf\n\u2502   \u2514\u2500\u2500 COLO829T.cobalt.ratio.tsv.gz\n\u251c\u2500\u2500 COLO829T.bam\n\u251c\u2500\u2500 COLO829T.bam.bai\n\u251c\u2500\u2500 gridss\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.fragment_lengths\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.junctions.csv\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.bai\n\u2502   \u2514\u2500\u2500 gridss\n\u2502       \u251c\u2500\u2500 COLO829T.bam.gridss.working\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.alignment_summary_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.coverage.blacklist.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.downsampled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.excluded_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.subsetCalled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam.bai\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.gridss.working\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.allocated.vcf.idx\n\u2502       \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.computesamtags.changes.tsv\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_histogram.pdf\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam.csi\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.sv_prep.sorted.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 fbrando\n\u2502       \u251c\u2500\u2500 gridss.full.20230113_110106.login01.6901.log\n\u2502       \u2514\u2500\u2500 libsswjni.so\n\u251c\u2500\u2500 gripss\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.somatic.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.gripss.somatic.vcf.gz.tbi\n\u251c\u2500\u2500 lilac\n\u2502   \u251c\u2500\u2500 COLO829T.candidates.coverage.csv\n\u2502   \u251c\u2500\u2500 COLO829T.lilac.csv\n\u2502   \u2514\u2500\u2500 COLO829T.lilac.qc.csv\n\u251c\u2500\u2500 linx\n\u2502   \u251c\u2500\u2500 plot_data\n\u2502   \u2514\u2500\u2500 plots\n\u251c\u2500\u2500 pave\n\u2502   \u251c\u2500\u2500 COLO829T.sage.pave.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.pave.vcf.gz.tbi\n\u251c\u2500\u2500 purple\n\u2502   \u251c\u2500\u2500 circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.baf.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cnv.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.indel.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.ratio.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.snp.circos\n\u2502   \u2502   \u251c\u2500\u2500 gaps.txt\n\u2502   \u2502   \u2514\u2500\u2500 null.ratio.circos\n\u2502   \u251c\u2500\u2500 COLO829T.driver.catalog.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.gene.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.range.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.qc\n\u2502   \u251c\u2500\u2500 COLO829T.purple.segment.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.clonality.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.hist.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 plot\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.clonality.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.png\n\u2502   \u2502   \u2514\u2500\u2500 COLO829T.somatic.rainfall.png\n\u2502   \u2514\u2500\u2500 purple.version\n\u2514\u2500\u2500 sage\n    \u251c\u2500\u2500 COLO829T.sage.bqr.tsv\n    \u251c\u2500\u2500 COLO829T.sage.exon.medians.tsv\n    \u251c\u2500\u2500 COLO829T.sage.gene.coverage.tsv\n    \u251c\u2500\u2500 COLO829T.sage.vcf.gz\n    \u2514\u2500\u2500 COLO829T.sage.vcf.gz.tbi\n</code></pre>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#sample-data-directory-output-directory_2","title":"<code>sample data</code> directory = output directory","text":"<p>The <code>sample_data</code> directory must have:</p> <ul> <li>a directory named as the sample's tumorId</li> <li>tumor and reference fastq files in the sample's directory, named as tumorId and referenceId.fastq.gz</li> </ul> <p>i.e. see folder tree below</p> <pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 referenceId.fastq.gz\n      \u2514\u2500\u2500 tumorId.fastq.gz\n</code></pre> <p>The pipeline searches for the files that match the regex <code>tumorId.*.fastq.gz</code> and <code>referenceId.*.fastq.gz</code> in order to perform the alignment. Therefore, you could provide several fastq.gz files to align as in the example below: </p> <p><pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 referenceId_L001_R1_001.fastq.gz\n      \u251c\u2500\u2500 referenceId_L001_R2_001.fastq.gz\n      \u251c\u2500\u2500 tumorId_L001_R1_001.fastq.gz\n      \u2514\u2500\u2500 tumorId_L001_R2_001.fastq.gz\n</code></pre> the pipeline will match all the fastq files that start with the <code>referenceId</code> or <code>tumorId</code> and end with <code>fastq.gz</code>, then it aligns them to the reference genome.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#gen_version-parameter_2","title":"<code>gen_version</code> parameter","text":"Genome version Genome file V38 <code>./ref_data_dir/V38/ref_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna</code> V37 <code>./ref_data_dir/V37/ref_genome/Homo_sapiens.GRCh37.GATK.illumina.fasta</code>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#threads-and-memory-parameters_2","title":"<code>threads</code> and <code>memory</code> parameters","text":"<ul> <li><code>threads</code> number of threads used for each component</li> <li><code>memory</code>: GB allocated to each component (default=12GB)</li> </ul>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#example_2","title":"Example","text":"<p>my directory:</p> <p><pre><code>pipeline/test_data/\n \u2514\u2500\u2500 ALL-280622\n      \u251c\u2500\u2500 ALL-280622-DIAGNOSTIC-DNA_1.fastq.gz\n      \u2514\u2500\u2500 ALL-280622-DIAGNOSTIC-DNA_2.fastq.gz\n</code></pre> Run: </p> <pre><code>./scripts/run_pipeline test_data \"ALL-280622\" V37 WGS 8 16\n</code></pre> <p>WGS without reference</p> <p>as this example, it is possible to use WGS without a non-tumoral reference. the output is the following: </p> Example <pre><code>TBC\n</code></pre>"},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Hmftools_pipeline/#source","title":"Source","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Tools/Sequencing_tools/Variant_callers/MosaicForecast/","title":"MosaicForecast","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/MosaicForecast/#description","title":"Description","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/MosaicForecast/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Mutect2/","title":"Mutect2","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Mutect2/#description","title":"Description","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Mutect2/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Platinum/","title":"Platinum","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Platinum/#description","title":"Description","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Platinum/#reference","title":"Reference","text":""},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/","title":"Sarek","text":"<p>Sarek is a pipeline built using Nextflow designed to detect variants in whole or targeted sequencing data. It works on any species with a reference genome and allows for germline, tumor only and tumor-normal pair variant calling</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#description","title":"Description","text":"<p>Sarek workflow consists of the following steps:</p> <ul> <li>Preprocessing (based on GATK4 Best Practices): includes sequencing quality control, reads alignment (BWA), mark duplicates and base recalibration.</li> <li>Variant calling. You can choose any caller, but here are some recommendations:<ul> <li>HaplotypeCaller for germline SNVs and indels.</li> <li>Mutect2 and Strelka for somatic SNVs and indels.</li> <li>Manta for structural variants (SVs).</li> <li>ASCAT for copy number variants (CNVs)</li> </ul> </li> <li>Annotation: you can choose snpEff, VEP or merge both.</li> </ul> <p>Note sarek allows to run the pipeline starting from any step.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#installation","title":"Installation","text":"<p>You can follow the instructions in Sarek's webpage. Take this into account if you plan to use sarek in the cluster:</p> <ol> <li>Create a conda environment to install the latest java version (openjdk specifically).</li> <li>Preferably, use Singularity as container. It is not necessary to install it as it is already installed in the cluster.</li> <li> <p>Better download and run the pipeline test specifying the version, entering the <code>-r</code> argument:</p> <pre><code>nextflow run nf-core/sarek -r 3.1.2 -profile test,singularity\n</code></pre> </li> <li> <p>Create a config file (you can name it <code>sarek.conf</code>) to specify the executor and Singularity cache directory. In this file you can also add any changes in the configuration specification of the pipeline steps (e.g.: memory limit). Example:</p> <pre><code>executor {\nname = 'slurm'\nqueueSize = 25\n}\nsingularity {\ncacheDir = '/home/$USER/singcache'\n}\nprocess {\nwithName: 'NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES' {\ncpus = 28\nmemory = 200.GB\n    }\n}\n</code></pre> </li> </ol>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#usage","title":"Usage","text":"<p>To run the pipeline you can follow the instruction in sarek's webpage, first activating the conda environment with java installed. Here is an example of a command to run the analysis from alignment generation to variant annotation, calling germline and somatic SNVs, SVs and CNVs:</p> <pre><code>conda activate java\nnextflow run nf-core/sarek -r 3.1.2 -profile singularity -c /path/to/sarek.conf \\\n--input input.csv --genome GATK.GRCh38 --igenomes_base /path/to/igenomes/ \\\n--tools 'haplotypecaller,strelka,ascat,manta,mutect2,vep' </code></pre> <p>To start the pipeline from a different step you will have to include the <code>--step</code> flag in the command. If you only want to generate the alignments, omit the <code>--tools</code> flag. To avoid pipeline freezing, it is highly recommended to use a local version of iGenomes instead of pulling it from AWS during pipeline execution.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#create-inputcsv","title":"Create <code>input.csv</code>","text":"<p>Sarek uses a comma-separated samplesheet with information about the samples to run the pipeline. Here you specify information as the patient-id, sex, status (normal/tumor), etc. In sarek's webpage you can find examples of the mandatory fields required in the samplesheet depending on the pipeline step from which you start the analysis.</p>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#main-outputs","title":"Main outputs","text":"<p>If you run the entire pipeline, you will end with many files. From those, we will highlight:</p> <ul> <li>In  <code>results/preprocessing/recalibrated/</code> the final alignments in CRAM format per sample.</li> <li>In <code>results/variant_calling/</code> the VCFs per caller and sample.</li> <li>In <code>results/annotation/</code> the annotated VCFs per caller and sample.</li> </ul>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#links","title":"Links","text":"<ul> <li>Sarek official webpage</li> <li>As an output example, the data in <code>/workspace/datasets/all_aecc_pediatric</code> was generated using sarek version 3.1.1. Feel free to give it a look if needed.</li> </ul>"},{"location":"Tools/Sequencing_tools/Variant_callers/Sarek/#reference","title":"Reference","text":"<ul> <li>Raquel Blanco</li> <li>Monica Sanchez</li> <li>Miguel Grau</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/","title":"Mosaic - python package","text":"<p>This package provides a set of tools to analyze data produced by Tapestri instruments (<code>.h5</code> files). It is NOT open source, although data can be exctrated and analysis can be held either using mosaic functions or custom functions.</p> <p>package version</p> <p>This wiki is based on Mosaic v2.2, the package is mantained and constantly updated by missionbio, therefore some things maybe be changed. If you are using a new version of the package please refer to the official documentation.</p>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/#installation","title":"Installation","text":"<p>The package is available in conda.</p> <pre><code>conda create --name mosaic -c missionbio -c plotly -c conda-forge missionbio.mosaic notebook\nconda activate mosaic\n</code></pre> Tip - Installing from exported environment <p>Another way of installing mosaic with some other packages used during the first analysis of the data is with the exported env: <code>/workspace/projects/scell_tall/mosaic180123.txt</code>.</p> <pre><code>conda create --name mosaic --file /workspace/projects/scell_tall/mosaic180123.txt\n</code></pre>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/#jupyter-tutorials","title":"Jupyter tutorials","text":"<p>How do I access a jupyter notebook? </p> <p>please, if you are having hard time accessing a Jupyter notebook, refer to this guide.</p> <p>Data accessibility</p> <p>You might need additional accessibility to run some of the analysis available in the jupyter notebooks.</p> <p>In the folder <code>/workspace/projects/scell_tall/LOPEBIG_44_analysis/</code> there are four notebooks that were used to do some of the initial analysis on data. In these notebooks you can find some of mosaic built-in functions and some custom functions.</p> <p>Additionally you can find some other notebooks in the section below: Link and in the folder <code>/workspace/projects/scell_tall/LOPEBIG_44_analysis/</code></p>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/#faq","title":"FAQ","text":"<p>Since there is no section with FAQ on mosaic documentation, here we collect our questions and the answers missionbio support provided. If you intend to ask more questions to their support, please update this section.</p> How <code>min_prc_cells</code> and <code>min_mut_prct_cells</code> are computed? <p>In mosaic they are calculated on the total numbers of cells in the dataset. Differently from Tapestri Insights where they are computed on the percentage of genotyped.</p> Why do I get different number of variants when uploading samples in Tapestri Insight and when loading a merged <code>.h5</code> file in Mosaic? <p>Still waiting for a response</p> How is the data filtered by the <code>ms.load(\"path/to/.h5\", raw=False, apply_filter=True)</code> function? <ul> <li>The <code>raw</code> parameter set to False discards empty barcodes</li> <li>The <code>apply_filter</code> parameter set to True loads only the variants that meets the Tapestri Insights advanced filters.</li> </ul> <p>We have a final matrix that excludes all those cells or variants that do not meet the filters and a filter layer called <code>FILTER_MASK</code> filled with 1 and 0 that excludes the genotypes (single cell in the matrix) that do not meet the filters. </p> <p></p>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/#additional-resources","title":"Additional resources","text":"<p>MissionBio provides some video tutorials on their website: Mosaic tutorials</p> <p>Additionally, the company provided a personal training course, the video lessons can be found here:</p> <pre><code>/workspace/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-3_1-Mosaic.mp4\n\n/workspace/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-3_2-Mosaic.mp4\n</code></pre>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/#links","title":"Links","text":"<ul> <li>Mosaic documentation</li> <li>Mosaic GitHub repo</li> <li>Mosaic Jupyter tutorial notebook</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Mosaic/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> <li>Raquel Blanco</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_insight/","title":"Tapestri insights","text":"<p>Compatibiliy</p> <p>The software is NOT available for Linux, you need a Windows machine or a VM running windows on your Linux machine in order to use it.</p> <p>Don't panic! This software is not a mandatory step to analyse the data. Indeed, one can produce the same analysis by using Mosaic package</p>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_insight/#installation","title":"Installation","text":"<p>Download and installation guide is available here.</p>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_insight/#usage","title":"Usage","text":"<p>Tapestri insight has a GUI and does not require command line nor programming skills to use it.</p> <p>The file type supported are <code>.loom</code> files, that can be found in the list of ouput file from the Tapestri pipeline summary page of the analysis, in the user private area.</p> <p>An optional input could be a whitelist file with known chromosomal locations with pathogenic variants that, even if they do not pass the built-in quality filters, they are still carried on for the analysis.</p>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_insight/#additional-resources","title":"Additional resources","text":"<p>MissionBio provided a personal training course, the video lesson on Tapestri Insights can be found here:</p> <pre><code>/workspace/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-2-TapestriInsight.mp4\n</code></pre>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_insight/#links","title":"Links","text":"<ul> <li>Tapestri Insights Demo</li> <li>Tapestri Insights: Visualize Clones</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_insight/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/","title":"Tapestri pipeline","text":"<p>Tapestri pipeline is designed for processing single-cell DNA and DNA+protein sequencing data generated on the Tapestri Platform</p>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#accessing-the-platform","title":"Accessing the platform","text":"<p>The pipeline is currently used online, accessible from the personal user portal on their website.</p>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#usage","title":"Usage","text":"<p>Clicking on the <code>Start Run</code> button here initializes the Run set-up.</p> <p></p>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#options-and-files","title":"Options and files","text":"<p>There are three different options that requires different files:</p> Options Reference Genome Panel FASTQ Notes 1. DNA v2.0.2 yes 1 DNA DNA Possibility to stop the run after cells.bam file is produced 2. DNA + Protein v2.0.2 yes 1 DNA + 1 Protein DNA + Protein The number of DNA and Protein FASTQ files must be the same 3. Merge Runs v1.0 no no no at least two .h5 files (recommended no more than 5) <ul> <li>Reference Genome. Custom reference file must be in <code>.fa.zip</code> format. As of now, there are h19 and mouse reference genomes on Tapestri portal.</li> <li>Panels. Both DNA and Protein can be either customed or taken from their Designer catalog. Custom Panels files must be in <code>zip</code> format.</li> <li>FASTQ files. They must be in <code>.fastq</code>, <code>.fastq.gz</code>, <code>.fq</code> or <code>.fq.gz</code> format. The number of files must be even.</li> <li>.h5 files . Tapestri Pipeline output file of one run. i.e. If we have multiple runs of the sample patients but with different stage of the disease we can merge the runs to produce an .h5 file with both samples.</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#output","title":"Output","text":"<p>At the end of the run we end up with a list of files. A detailed list of output files is found here. Here we present three files:</p> File Extension Notes report.html <code>.html</code> It contains the run details with a summary of the most important specs. Merge Runs and DNA only option do not produce this report. cells.loom <code>.loom</code> It consists of a numerical genotype (NGT) matrix that has 4 other layers. It contains metadata for all the variants. It's the file used as input for Tapestri Insight. dna+protein.h5 <code>.h5</code> It contsins data for one or more run. Each run contains data for one or more assays (eg. DNA assay, CNV assay, Protein assay) associated with all the barcodes. It's the file used as input for Mosaic package."},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#additional-resources","title":"Additional resources","text":"<p>MissionBio provided a personal training course, the video lesson on Tapestri Pipeline can be found here:</p> <pre><code>/workspace/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-1-TapestriPipeline.mp4\n</code></pre>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#links","title":"Links","text":"<ul> <li>Tapestri Pipeline user guide</li> <li>Pipeline output files overview</li> <li>Video - Tapestri Pipeline: Processing DNA + Protein sequencing data</li> <li>FAQs</li> </ul>"},{"location":"Tools/Sequencing_tools/singleCell/Tapestri_pipeline/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Tools/Signature_tools/DeconstructSigs/","title":"DeconstructSigs","text":""},{"location":"Tools/Signature_tools/DeconstructSigs/#description","title":"Description","text":"<p>The deconstructSigs package is an extension for R that allows to quantify presence and prevalence of known mutational signatures in individual tumor samples. It determines the linear combination of pre-defined signatures that most accurately reconstructs the mutational profile of the input tumor sample. This method uses a multiple linear regression model. </p>"},{"location":"Tools/Signature_tools/DeconstructSigs/#getting-started","title":"Getting started","text":"<pre><code>library(deconstructSigs)\n</code></pre>"},{"location":"Tools/Signature_tools/DeconstructSigs/#input","title":"Input","text":"<p>The input for the method is a dataframe with the mutational data containing the following colunns: - sample identifier (sample.id) - chromosome (chr) - base position (pos) - reference base (ref) - alternate base (alt)</p>"},{"location":"Tools/Signature_tools/DeconstructSigs/#main-functions","title":"Main functions","text":"<p>mut.to.sigs.input()</p> <p>Converts input dataframe to a dataframe with 3nt context mutational frequencies. The output is a data frame with n rows (corresponding to the number of samples analyzed) and 96-columns (corresponding to all possible 96 3nt mutational contexts).</p> <pre><code>sigs.input &lt;- mut.to.sigs.input(mut.ref = input_dataframe, sample.id = \"Sample\", chr = \"chr\", pos = \"pos\", ref = \"ref\", alt = \"alt\")\n</code></pre> <p>whichSignatures()</p> <p>Reconstruct mutational profile of a given tumour using input set of signatures. It requires two data frames as an input. The first one containing the frequencies of mutations in the sample (created by mut.to.sigs.input() or generated by the user manually). The second one with the mutational profiles of the reference set of signatures that should be used for reconstruction. Two sets of reference signatures are supplied by the package (signatures.nature2013 and signatures.cosmic) but it is also possible to use custom reference set.</p> <pre><code>sigs.output = whichSignatures(tumor.ref = sigs.input, signatures.ref = signatures.cosmic, sample.id = 2,\n                       contexts.needed = TRUE)\n</code></pre> <p>Important! If the input data frame contains the raw number of mutations thean it shoul be normalized. The minimum required normalization for the function to work - is the relative frequency of the mutations not the raw counts (so that the sum in the row is equal to 1). For this you can simply set contexts.needed = TRUE when running whichSignatures(). Further normalization for the 3nt context of target region can be done using tri.counts.method parameter. Possible values are 'exome', 'genome', 'exome2genome' or the data frame with the corresponding numbers of contexts. </p> <p>Optional parameters for reconstruction: associated -- vector of signatures (limits the reconstruction to listed signatures) signatures.limit -- number (limit the number of signatures present in the reconstruction) signature.cutoff -- proportion (cutoff to discard signatures with weight less than this amount)</p> <p>Main output - data frame with the weights of input reference signatures in the tumour sample.</p> <p>plotSignatures()</p> <p>Visualize the result from the whichSignatures().</p> <pre><code>plotSignatures(plot_example, sub = 'example')\n</code></pre>"},{"location":"Tools/Signature_tools/DeconstructSigs/#reference","title":"Reference","text":"<p>Rosenthal, R., McGranahan, N., Herrero, J. et al. deconstructSigs: delineating mutational processes in single tumors distinguishes DNA repair deficiencies and patterns of carcinoma evolution. Genome Biol 17, 31 (2016). https://doi.org/10.1186/s13059-016-0893-4</p>"},{"location":"Tools/Signature_tools/SigProfiler/","title":"SigProfiler","text":""},{"location":"Tools/Signature_tools/SigProfiler/#description","title":"Description","text":""},{"location":"Tools/Signature_tools/SigProfiler/#reference","title":"Reference","text":""},{"location":"Tools/Signature_tools/SigProfilerJulia/","title":"SigProfilerJulia","text":""},{"location":"Tools/Signature_tools/SigProfilerJulia/#description","title":"Description","text":""},{"location":"Tools/Signature_tools/SigProfilerJulia/#reference","title":"Reference","text":""},{"location":"Tools/Signature_tools/mSigHDP/","title":"mSigHDP","text":""},{"location":"Tools/Signature_tools/mSigHDP/#description","title":"Description","text":""},{"location":"Tools/Signature_tools/mSigHDP/#reference","title":"Reference","text":""},{"location":"Tools/Signature_tools/mSignAct/","title":"mSignAct","text":""},{"location":"Tools/Signature_tools/mSignAct/#description","title":"Description","text":""},{"location":"Tools/Signature_tools/mSignAct/#reference","title":"Reference","text":""},{"location":"Tools/Singularity/Building_containers/","title":"Building containers","text":"<p>Singularity containers are built from a definition file which allows the container to be built identically by anyone possessing the file.</p> <p>Root privileges required to build a container</p> <p>Note that the process of building a container requires elevated privileges</p> <p>One primary task per container</p> <p>HPC containers are designed to perform one primary task, and should consist of a main application and its dependencies, in a similar way to how module files are provided. Since containers are lightweight, you can use separate containers instead of general purpose containers containing a collection of applications. This improves supportability, performance and reproducibility.</p>"},{"location":"Tools/Singularity/Building_containers/#building-a-singularity-container-from-scratch","title":"Building a Singularity container from scratch","text":"<p>Building from scratch gives complete control over the contents of the container, including operating system and packages. Certain packages may only be available for a specific version of Linux (i.e. compatibility issues) so being able to build a container from scratch enhances research capability.</p> <p>The following example demonstrates building an Ubuntu 20 (focal) container using definition file <code>ubuntu20_helloworld.def</code> that installs the <code>python3</code> package via the Ubuntu package manager:</p> <pre><code>BootStrap: debootstrap\nOSVersion: focal\nMirrorURL: http://us.archive.ubuntu.com/ubuntu/\n\n%post\n  apt-get install --yes python3\n\n%runscript\n  python3 \"${@}\"\n</code></pre> <p>The build process is unattended, and will not succeed if any operations require interactive input. Be sure to use <code>-y</code> or <code>--yes</code> options when installing packages.</p> <p>Create the image (this step requires root privileges):</p> <pre><code>sudo singularity build ubuntu20_helloworld.simg ubuntu20_helloworld.def\n</code></pre> <p>This will result in a usable image in the current working directory. Be aware that if you want a very specific version of package from a repository, that package may not be available in future, so where possible, try to future-proof your containers.</p>"},{"location":"Tools/Singularity/Building_containers/#building-containers-for-other-linux-distributions","title":"Building containers for other Linux distributions","text":"<p>You may build Ubuntu images using CentOS and vice versa. However to bootstrap, you will need extra packages on the host OS to build the container. CentOS hosts require the <code>debootstrap</code> package to create Ubuntu containers, and Ubuntu hosts require the <code>yum</code> package to build CentOS containers. Alternatively you may create containers from an existing Singularity or Docker image, as explained in the following section. Since this method builds upon pre-built images, the <code>debootstrap</code> or <code>yum</code> packages are not required.</p> <p>Using LTS for Ubuntu definitions</p> <p>When building an Ubuntu container we recommend that you use a release with long term support (LTS release). Non-LTS Ubuntu releases have very limited support cycles which may lead to difficulties downloading packages if used after their end-of-life date.</p>"},{"location":"Tools/Singularity/Building_containers/#building-containers-from-an-existing-base-image","title":"Building containers from an existing base image","text":"<p>This enables you to either build or use an existing container as a base image to build other containers. Base images must be built first if part of a dependency chain and is no longer required once all dependent containers have been built.</p>"},{"location":"Tools/Singularity/Building_containers/#singularity-local-images","title":"Singularity local images","text":"<p>The following example demonstrates the creation of a local base Ubuntu 20 (focal) image using definition file <code>ubuntu20_base.def</code>, and then creating another container with <code>python3</code> installed, using the local base image:</p> <pre><code>BootStrap: debootstrap\nOSVersion: focal\nMirrorURL: http://us.archive.ubuntu.com/ubuntu/\n</code></pre> <p>Create the base image:</p> <pre><code>sudo singularity build ubuntu20_base.simg ubuntu20_base.def\n</code></pre> <p>The non-base image container (i.e. <code>python3</code> in this example) can be built using definition file <code>ubuntu20_python3.def</code>:</p> <pre><code>Bootstrap: localimage\nFrom: ubuntu20_base.simg\n\n%post\n  apt-get install --yes python3\n\n%runscript\n  python3 \"${@}\"\n</code></pre> <p>The result will be a container almost identical to the one created from scratch.</p> <pre><code>sudo singularity build ubuntu20_python3.simg ubuntu20_python3.def\n</code></pre>"},{"location":"Tools/Singularity/Building_containers/#docker-images","title":"Docker images","text":"<p>You can also bootstrap from Docker containers, although if supplied by a third party, you have less visibility or control over these images, so use with caution, as this may impact the future reproducibility of results.</p> <p>The below example demonstrates installing the <code>python3</code> package within an Ubuntu 20 (focal) container using definition file <code>ubuntu20_docker_python3.def</code>, which imports the <code>ubuntu:20.04</code> base container available on the Docker Hub:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  apt-get update\n  apt-get install --yes python3\n\n%runscript\n  python3 \"${@}\"\n</code></pre> <p>Build the container. This will produce a container similar to the previous examples, but may vary slightly in overall size depending on packages installed in the base docker image:</p> <pre><code>sudo singularity build ubuntu20_docker_python3.simg ubuntu20_docker_python3.def\n</code></pre>"},{"location":"Tools/Singularity/Building_containers/#future-proofing-your-containers","title":"Future-proofing your containers","text":"<p>When building your own containers, be sure to make them portable and future-proof.</p> <ul> <li>Consider whether the container will still build and produce the same results if the OS release or application version changes.</li> <li>If copying files from a working directory as part of setup is unavoidable, ensure that any files copied from the working directory are are available for others to download (i.e. in a git repository if not large).</li> <li>Perform all setup as part of the build process. If any manual steps are performed after the container is built, they should be integrated within the definition file, and the container rebuilt.</li> <li>Consider if the ability to rebuild your container will be impacted by package updates, or deprecation of old releases.</li> </ul> <p>Legacy versions of CentOS applications</p> <p>Outdated minor CentOS releases are moved from the main CentOS servers to vault.centos.org. If you need to use a specific Operating System or application version other than the latest, you need to future-proof your container by using the CentOS vault.</p>"},{"location":"Tools/Singularity/Building_containers/#definition-file-sections","title":"Definition file sections","text":"<p>The following example definition file demonstrates commonly used definition file sections:</p> <ul> <li><code>%help</code></li> <li><code>%post</code></li> <li><code>%environment</code></li> <li><code>%test</code></li> <li><code>%runscript</code></li> </ul>"},{"location":"Tools/Singularity/Building_containers/#help-section","title":"Help section","text":"<p>The <code>%help</code> section is designed to provide information about the container when singularity run-help is run on the container, for example:</p> <pre><code>$ singularity run-help /data/containers/public/python3_helloworld.simg\nPurpose: Test container to print \"Hello, World!\" in Python3.\nAuthor:  ITS Research / QMUL.\n</code></pre>"},{"location":"Tools/Singularity/Building_containers/#post-section","title":"Post section","text":"<p>The <code>%post</code> section contains the commands used to build the container, such as package installs, file downloads, compilation and software configuration.</p>"},{"location":"Tools/Singularity/Building_containers/#environment-section","title":"Environment section","text":"<p>Environment settings supplied at build-time in the <code>%post</code> section are only set during build-time and are not available at run-time. Environment settings which need to be available at run-time should be added to the <code>%environment</code> section.</p>"},{"location":"Tools/Singularity/Building_containers/#test-section","title":"Test section","text":"<p>The <code>%test</code> section defines a set of commands or tests which should be run to validate the container has been built successfully. Some example tests include:</p> <ul> <li>installed binaries are available on the <code>PATH</code> variable</li> <li><code>--help</code> or <code>--version</code> parameter for binaries (if supported)</li> <li>libraries, header files and man pages exist</li> </ul> <p>All tests will be run during the build process, after <code>%post</code> has completed. To build a container without running the tests, pass the <code>-T</code> or <code>--notest</code> option to the singularity build command.</p> <p>To run the tests for an existing container, run the singularity test command, for example:</p> <pre><code>$ singularity test /data/containers/public/python3_helloworld.simg\n/usr/bin/python3\n</code></pre>"},{"location":"Tools/Singularity/Building_containers/#runscript-section","title":"Runscript section","text":"<p>The <code>%runscript</code> section defines the default action a container will perform when ran as an executable or with <code>singularity run</code>. This is configured during the build process.</p> <p>Application parameters or arguments</p> <p>If the runscript calls an application which takes parameters or arguments, include \"${@}\" after the application otherwise anything passed after the container name will be ignored by Singularity.</p>"},{"location":"Tools/Singularity/Building_containers/#inspecting-a-container","title":"Inspecting a container","text":"<p>To display information about how a container was build, use the <code>singularity inspect</code> command. The <code>-d</code> option to this command will print the definition file used to built the container and the <code>-r</code> option will print the runscript (if added during build-time). For example:</p> <pre><code>$ singularity inspect -d /data/containers/public/python3_helloworld.simg\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%help\n  Purpose: Test container to print \"Hello, World!\" in Python3.\n  Author:  ITS Research / QMUL.\n\n%post\n  apt-get update\n  apt-get install --yes python3\n\napt-get clean &amp;&amp; \\\nrm -rf /var/lib/apt/lists/*\n\n%test\n  which python3\n\n%runscript\n  python3 -c 'print(\"Hello, World!\")'\n</code></pre> <p>The <code>singularity help inspect</code> command provides additional options for inspecting the container.</p>"},{"location":"Tools/Singularity/Building_containers/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez Elorduy</li> </ul>"},{"location":"Tools/Singularity/Overview/","title":"Overview","text":""},{"location":"Tools/Singularity/Overview/#singularity-containers","title":"Singularity containers","text":"<p>Linux containers are self-contained execution environments that share a Linux kernel with the host, but have isolated resources for CPU, I/O, memory, etc. A container can run a completely different Linux environment, without the overhead required by virtual machines.</p>"},{"location":"Tools/Singularity/Overview/#benefits-of-containers","title":"Benefits of containers","text":"<ul> <li>Reproducible science - containers can include an application and its dependencies, and be run on other systems where Singularity is installed.</li> <li>Version independent - run code designed for other versions of Linux e.g. Ubuntu packages on a CentOS system.</li> <li>Self-contained - allow isolation of complicated application installs.</li> </ul>"},{"location":"Tools/Singularity/Overview/#singularity","title":"Singularity","text":"<p>Singularity is a popular Open Source container solution designed for HPC. Unlike other container solutions such as Docker, it allows utilisation of GPUs and Infiniband interconnects for MPI jobs, and does not allow privilege escalation within a container, which would compromise the security in a multi-user environment with a shared filesystem.</p>"},{"location":"Tools/Singularity/Overview/#using-singularity-on-the-bbgcluster","title":"Using Singularity on the bbgcluster","text":"<p>Singularity is available as a system package on the bbgcluster. We may update the version of Singularity installed on the cluster to address security vulnerabilities or to provide extra features as they become available. Recently, the default version of Singularity has been changed to singularity v3, although Singularity v2 can still be used with the command <code>singularity2</code>.</p>"},{"location":"Tools/Singularity/Overview/#resources","title":"Resources","text":"<p>Containers built by ITS Research are stored in <code>/data/containers</code> and are supported in a similar way to the globally available supported applications. Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands. See the Singularity usage page for more information about containers provided as modules.</p>"},{"location":"Tools/Singularity/Overview/#further-reading","title":"Further reading","text":"<ul> <li>Singularity website</li> <li>Running <code>singularity help</code> and <code>singularity CMD help</code> (replace <code>CMD</code> with a Singularity command, such as <code>run</code>)</li> <li>Viewing the \"singularity\" manual page</li> </ul>"},{"location":"Tools/Singularity/Overview/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez Elorduy</li> </ul>"},{"location":"Tools/Singularity/Using_containers/","title":"Using containers","text":""},{"location":"Tools/Singularity/Using_containers/#running-commands-inside-a-container","title":"Running commands inside a container","text":"<p>The <code>singularity exec</code> command will allow you to execute any program within a given container. The <code>singularity run</code> command performs the action defined by the <code>%runscript</code> section, which is the primary task of the container. Using the <code>singularity run</code> command is the simpler approach for job submissions.</p> <p>You can even \"execute\" a container, which performs the same action as the <code>singularity run</code> command. For example, the following demonstrates how to inspect the runscript and execute the <code>/data/containers/public/python3_helloworld.simg</code> container:</p> <pre><code>$ singularity inspect -r /data/containers/public/python3_helloworld.simg\n#!/bin/sh\npython3 -c 'print(\"Hello, World!\")'\n$ /data/containers/public/python3_helloworld.simg\nHello, World!\n\n$ singularity run /data/containers/public/python3_helloworld.simg\nHello, World!\n</code></pre>"},{"location":"Tools/Singularity/Using_containers/#execute-a-script-from-outside-the-container","title":"Execute a script from outside the container","text":"<p>Using on the cluster</p> <p>For typical use, you want to use the singularity run or singularity exec commands, especially when submitting the work via the scheduler.</p> <p>The following example runs a python script <code>hello_world2.py</code> from the current directory using the <code>/data/containers/public/python3_helloworld.simg</code> container:</p> <pre><code>$ singularity exec /data/containers/public/python3_helloworld.simg python3 ./hello_world2.py\nHello, World!\nHello, World (again)!\n</code></pre> <p>The file hello_world2.py contains the following code:</p> <pre><code>print(\"Hello, World!\")\nprint(\"Hello, World (again)!\")\n</code></pre> <p>If command <code>singularity exec</code> was replaced by <code>singularity run</code>, the runscript would be called, ignoring any parameters after the container name.</p> <p>Customised Environments</p> <p>While we encourage users to customise their environment to make their workflow easier, please be aware that customisations which change the user's environment for example by setting variables in the ~/.bash_profile file, or by using python's pip to create a ~/.local folder, may cause problems with Singularity which can be difficult to troubleshoot.</p>"},{"location":"Tools/Singularity/Using_containers/#using-containers-with-grid-engine","title":"Using containers with Grid Engine","text":"<p>One of the major benefits of Singularity is the simplicity with which it can be used in an HPC environment. Your Grid Engine submission script may not require any modules loading to run your container. The resource requirements should be very similar to native code.</p>"},{"location":"Tools/Singularity/Using_containers/#simple-example","title":"Simple example","text":"<pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\nsingularity run /data/containers/public/python3_helloworld.simg\n</code></pre>"},{"location":"Tools/Singularity/Using_containers/#modules-example","title":"Modules example","text":"<p>Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands (<code>singularity run</code> and <code>singularity exec</code>). In these cases, the container name will match the runscript command. For example, to use Pandoc as a module, simply load the <code>pandoc</code> module to use the application.</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\nmodule load pandoc\npandoc --help\n</code></pre>"},{"location":"Tools/Singularity/Using_containers/#shell-access-to-the-container","title":"Shell access to the container","text":"<p>It is possible to launch a shell within the container using the <code>shell</code> command. Interacting directly with a shell inside the container can be useful for code debugging and running multiple commands in a single interactive session, as an alternative to writing a single script. Below demonstrates how to invoke python3 from inside the <code>/data/containers/public/python3_helloworld.simg</code> container using an interactive shell:</p> <pre><code>$ singularity shell /data/containers/public/python3_helloworld.simg\nSingularity&gt; python3\nPython 3.8.10 (default, Sep 28 2021, 16:10:42)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>Documentation is available on the Singularity Hub Wiki</p>"},{"location":"Tools/Singularity/Using_containers/#running-containers-from-external-sources","title":"Running containers from external sources","text":"<p>Use of external containers for Research</p> <p>For long term reproducibility of containers, we recommend that you build your own native Singularity containers from definition files instead of relying on 3rd party containers for your research. Using containers from external sources may produce undesirable results if the container is rebuilt after upstream changes such as updated or obsoleted packages.</p> <p>Containers created elsewhere can be copied or imported, and run on the cluster. The following example demonstrates how to import and run the latest Ubuntu official image stored in the Docker Hub:</p> <pre><code>$ singularity pull ubuntu.simg docker://ubuntu:latest\n$ singularity exec ubuntu.simg cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=20.04\nDISTRIB_CODENAME=focal\nDISTRIB_DESCRIPTION=\"Ubuntu 20.04.3 LTS\"\n</code></pre>"},{"location":"Tools/Singularity/Using_containers/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez Elorduy</li> </ul>"},{"location":"Tools/VSCode/cluster_node/","title":"VSCode in interactive node","text":""},{"location":"Tools/VSCode/cluster_node/#description","title":"Description","text":"<p>These are the instructions to use Visual Studio Code to run and debug scripts/notebooks within an interactive node from the cluster.</p>"},{"location":"Tools/VSCode/cluster_node/#requirements","title":"Requirements","text":"<ul> <li>Conda or Mamba installed</li> <li>Have an account on ngrok</li> </ul>"},{"location":"Tools/VSCode/cluster_node/#create-a-conda-environment","title":"Create a conda environment","text":"<p>The conda environment must include the packages <code>code-server</code>, <code>pyngrok</code> and <code>screen</code></p> <pre><code>conda create -n vsc_node -c conda-forge code-server screen pyngrok -y\nconda activate vsc_node\n</code></pre>"},{"location":"Tools/VSCode/cluster_node/#run-code-server-in-a-screen-inside-login01","title":"Run code server in a screen (inside <code>login01</code>)","text":"<pre><code>screen -S vscode\n</code></pre> <p>Right after creating the screen, create an interactive session and remember on which node you are allocated.</p> <pre><code>interactive\nconda activate vsc_node\ncode-server\n</code></pre> <p>Exit the screen with <code>Ctrl + A + D</code></p>"},{"location":"Tools/VSCode/cluster_node/#run-a-ngrok-tunnel-in-a-screen-inside-login01","title":"Run a Ngrok tunnel in a screen (inside <code>login01</code>)","text":"<p>Note</p> <p>If this is your first time doing this step, you'll first need to setup your authentification token for ngrok.</p> <ol> <li>Log in to your ngrok home page.</li> <li>On the left-hand side bar: <code>Getting Started &gt; Your Authtoken</code></li> <li>On the <code>Command Line</code> section, copy only the key, which is the big string with random letters and numbers.</li> <li> <p>Go back to the terminanl in the cluster (with the <code>vsc_pyngrok</code> environment activated) and add your authentification token with the following command:</p> <p><code>ngrok authtoken &lt;the_token_you_copied_in_the_previous_step&gt;</code></p> </li> </ol> <p>This setup only has to be done once.</p> <pre><code>screen -S pyngrok\n</code></pre> <pre><code>interactive -w &lt;bbgnXXX&gt; # Node of previous step\nconda activate vsc_node\npyngrok http 8080\n</code></pre> <p>Copy the URL to your browser and exit the session <code>Ctrl + A + D</code></p>"},{"location":"Tools/VSCode/cluster_node/#check-your-vscode-password","title":"Check your VSCode password","text":"<pre><code>cat ~/.config/code-server/config.yaml\n</code></pre>"},{"location":"Tools/VSCode/cluster_node/#browse-your-vscode-remotely","title":"Browse your VSCode remotely","text":"<p>When entering the URL in your browser, click <code>Visit site</code> and introduce the password you obtained in the previous step in order to be able to use VSCode.</p> <p></p>"},{"location":"Tools/VSCode/cluster_node/#reference","title":"Reference","text":"<ul> <li>Jordi Deu Pons</li> <li>Carlos L\u00f3pez Elorduy</li> <li>Federica Brando</li> </ul>"}]}