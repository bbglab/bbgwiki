{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-bbg-wiki","title":"Welcome to the BBG-Wiki!","text":"<p>This website is meant to include information of all the tools and data used by the bbglab team, so that it serves both as a guide to understand them and as a place where to find information about everything.</p> <p></p>"},{"location":"BBGWelcome/","title":"Welcome to the BBG Lab","text":"<p>Here you can find some resources to help you start smoothly.</p>"},{"location":"BBGWelcome/#communication","title":"Communication","text":"<ul> <li>Gmail</li> <li>Hangouts</li> <li>Calendar</li> </ul>"},{"location":"BBGWelcome/#files","title":"Files","text":"<ul> <li>NextCloud</li> <li>Google Drive</li> </ul>"},{"location":"BBGWelcome/#social","title":"Social","text":"<ul> <li>Website</li> <li>LinkedIn</li> <li>Bluesky</li> <li>Twitter</li> </ul>"},{"location":"BBGWelcome/#repositories","title":"Repositories","text":"<ul> <li>Github</li> <li>Bitbucket</li> </ul>"},{"location":"BBGWelcome/#cheatsheets","title":"Cheatsheets","text":"<ul> <li>Summary cheatsheet - Summary cheatsheet with the basis</li> <li>Screen - Working with screen</li> <li>Git - Manage git version control system<ul> <li>Basic user</li> <li>Advanced</li> </ul> </li> <li>Python packages - Manage Python packages with pip and conda</li> <li>Conda - Manage software with conda</li> </ul>"},{"location":"BBGWelcome/#learning","title":"Learning","text":"<ul> <li>Get started with Python Python tutorial covering the basis of Python.</li> <li>Python classes  Tutorial on Python classes.</li> <li>Python decorators  Introduction to Python decorators.</li> <li>Git Understand git version control system. Here the link  to the Agora about Git and GitHub.</li> <li>Write docs Create documentation for your project using Sphinx.</li> <li>Points of view Summary 38 Points of View articles. Points of view are columns on data visualization published in Nature Methods containing practical advices on visualizing scientific data.</li> </ul>"},{"location":"BBGWelcome/#reference","title":"Reference","text":"<ul> <li>Stefano Pellegrini</li> <li>Davide Scarpetta</li> </ul>"},{"location":"Edit_BBG-Wiki/","title":"Edit BBG-Wiki","text":"<p>The main language of the wiki documentation is Markdown. There are several online editors which can help writting Markdown text and automatically visualize what is being written.</p> <ul> <li>StackEdit</li> <li>Editor.md</li> </ul> <p>This wiki is stored in a GitHub repository, where each section of the wiki corresponds to a single Markdown file (<code>*.md</code>). By editing these files either online or locally, the wiki can be updated by everyone.</p> <ul> <li>Markdown cheatsheet</li> <li>Mkdocs documentation</li> </ul> OnlineLocal <p>Go to the bbg-wiki repository and edit any file inside  the <code>docs/</code> folder, which contains all the files of the documentation.</p> <p></p>"},{"location":"Edit_BBG-Wiki/#installation","title":"Installation","text":"<ul> <li> <p>Clone the bbgwiki repository locally:</p> <pre><code>git clone git@github.com:bbglab/bbgwiki.git\npip install -r bbgwiki/requirements.txt\n</code></pre> Error: My Github password seems to be wrong somehow... <p>It might be the case that at some point of this process, it asks for the Github user and password.  However, although you should introduce your Github user, the password that it asks  is not your Github password. In order to know what to introduce here, you need to generate a <code>ssh key</code> following one of these two options:</p> <ul> <li> <p>From the Github web:</p> <ul> <li>Go to Github and login</li> <li>Click on your profile on the top right</li> <li>Settings &gt; Developer settings (bottom option of the left bar) &gt; Personal access tokens &gt; Generate new token</li> <li>Introduce your Github password</li> <li>Check all the boxes of the checklist</li> <li>Click \"Generate token\".</li> <li>Copy the generated key (looks like a bunch of random letters) and paste it in your terminal where it previously asked for the password.</li> </ul> </li> <li> <p>From the terminal:</p> <ul> <li>Execute the command:</li> </ul> <pre><code>ssh-keygen -o -t rsa -C \u201cssh@github.com\u201d\n</code></pre> <ul> <li>Click \"Enter\" on all the options (unless you want to save the key in a specific file,  but it is not mandatory)</li> <li>Execute the next command to see the generated key (if you selected a specific file, change the <code>id_rsa</code> in the command by the name you inputed).</li> </ul> <pre><code>cat id_rsa.pub\n</code></pre> <ul> <li>Copy the output and paste it in your terminal where it previously asked for the password.</li> </ul> </li> </ul> </li> <li> <p>Open the desired <code>.md</code> file with any IDE you prefer (vscode, pycharm, atom, etc) or  any online editors above-mentioned.</p> </li> <li> <p>Check if you are happy with the edits by running the website at localhost (to try  stuff before updating the main web) with the following: </p> <pre><code>mkdocs serve\n</code></pre> </li> <li> <p>Update web:</p> <pre><code>git add &lt;edited file or directory&gt;\ngit commit -m \"Message\"\ngit push\n</code></pre> </li> </ul>"},{"location":"Edit_BBG-Wiki/#references","title":"References","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Federica Brando</li> </ul>"},{"location":"BBGProtocols/BBGlab%20Printer/","title":"BBGlab Printer Instructions","text":"<p>Instructions to print quick and fast from the BBGlab printer. Please note the actual printer model should appear in your PC as: Colour-LaserJet-Pro-MFP-4302 (avoid the one with D86AAA!)</p> <p>Important</p> <p>Even if you set the BBGlab printer as the default in your computer's printer settings, the printer in the closest lab is often automatically recognized. This may result in accidentally printing to the printer in another lab. Please double-check before printing!</p> <p>Disclaimer: While we hope these instructions help improve the printing process, we cannot guarantee they will resolve all issues. Still, we believe it's better than nothing!</p>"},{"location":"BBGProtocols/BBGlab%20Printer/#printing-files-directly-from-a-usb-device","title":"Printing files directly from a USB device","text":"<ol> <li>Insert USB to printer</li> <li>Now in the screen select: \"Print\".  Note that by default it prints 1 sided page in Black&amp;White. This might be</li> <li>different if a previous user has recently changed the configuration of the paper side or the colour.</li> </ol> <p>If you need to change the default configuration follow this:</p> <ul> <li>To change Paper Side:<ol> <li>First you need to change the paper size in the printer screen:  Options &gt;&gt; More Options &gt;&gt; Paper Selection &gt;&gt; A4  </li> <li>Then you will be able to change to 2-sided:  Options &gt;&gt; More Options &gt; Sides &gt; 2-sided</li> </ol> </li> <li>To change the Colour:  In the printer screen go to:     Options &gt; More Options &gt; Color &gt; choose color Then in the main menu go to Print.</li> </ul>"},{"location":"BBGProtocols/BBGlab%20Printer/#reference","title":"Reference","text":"<ul> <li>Elisabet Figuerola</li> <li>Olivia Dove</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/","title":"BBGlab best practices","text":"<p>We present some best practices to make your life easier and more efficient and when day comes to be able to set up the BBGlab Exit protocol.</p>"},{"location":"BBGProtocols/BBglab_data_organization/#organizing-the-data","title":"Organizing the data","text":""},{"location":"BBGProtocols/BBglab_data_organization/#project-compilation","title":"Project Compilation","text":"<p>You MUST add all the relevant information about your finished or ongoing project in:</p> <ol> <li>ProjectCompilation  google spreadsheet. It should be one file per BBGlab member.</li> <li>BBGlab datasets google spreadsheet.</li> </ol> <p>It is essential to fill all these files so that all your project data is updated and stored. It is the responsibility of ALL the users involved in the project to keep it updated!</p> <p>Organize your project directories with this basic structure:</p> <ul> <li>Data</li> <li>Scripts</li> <li>Environments</li> <li>Notebooks</li> <li> <p>Software (useful)</p> <ul> <li>Local in your computer<ul> <li>Tools not installable in the cluster</li> <li>Custom scripts</li> </ul> </li> <li>Environments in the cluster<ul> <li>Tools installed in your user in the cluster (some /bin folder somewhere) linked to source code</li> </ul> </li> </ul> </li> <li> <p>Figures</p> </li> <li>Abstracts, Slides, Posters, Manuscripts</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#google-drive","title":"Google Drive","text":"<p>You should have one directory per project only shared with the people involved in the project. This directory must contain the following:</p>"},{"location":"BBGProtocols/BBglab_data_organization/#slides","title":"Slides","text":"<p>It is good to include links to where plots were generated as much as possible. This directory would contain:</p> <ul> <li>FormalPresentations (not linked to conferences)</li> <li>WorkingMeetings / Project follow-up presentations</li> <li>NuriaSlides (link to Nextcloud or copy from there)</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#conferences","title":"Conferences","text":"<ul> <li>Abstracts</li> <li>Posters</li> <li>Slides</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#manuscripts","title":"Manuscripts","text":"<ul> <li>Mature versions of the figures including<ul> <li>Images</li> <li>Link to code (i.e. GitHub repo)</li> </ul> </li> <li>Text If different versions, make sure that the date is updated. Do not remove older versions, either make sure that the history is available or duplicate the file and store it in the OLD_versions folder.</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#relevant-papers","title":"Relevant papers","text":"<ul> <li>PDFs of those papers relevant for the project</li> <li>If there are any notes or summary from the paper, include it as well. (BUT NAME IT PROPERLY)</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#communicationscontacts","title":"Communications/Contacts","text":"<ul> <li>Meeting notes (if relevant, usually it is)</li> <li>Content of relevant emails or slack conversations.</li> <li>Relevant contacts (including older collaborations). It is a good idea to have them in a contact compilation sheet. </li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#naming-files-folders","title":"Naming files / folders / \u2026","text":"<p>Everywhere where you store files (Cluster, Drive, Cloud, Computer)</p> <ul> <li>Use dates in YYYYMMDD format or YYMMDD. Always year first\u2026</li> <li>Use informative words</li> <li>No spaces replace them by \"_\"</li> <li>For more official guidelines or recommendations check these links: Harvard File Naming Convention and How To Name Files.</li> </ul>"},{"location":"BBGProtocols/BBglab_data_organization/#avoid-accumulating-unnecessary-data","title":"Avoid accumulating unnecessary data","text":"<p>Store only essential files in the cluster by ensuring you erase intermediate or temporary files that are no longer needed. Archive the essential files from completed projects to keep the cluster clean and manageable (ask Miguel). Instructions on how to archive files in the BBGcluster here.</p>"},{"location":"BBGProtocols/BBglab_data_organization/#track-and-manage-your-code-with-github","title":"Track and Manage Your Code with GitHub","text":"<p>It is highly recommended to create a GitHub repository for your project code and regularly update it to track changes, share it with others, review it... Find all documentation on how to work with GitHub repositories here.</p>"},{"location":"BBGProtocols/BBglab_data_organization/#be-environmentally-friendly","title":"Be environmentally friendly","text":"<p>Did you know it is estimated that the emission of a biology project (both wet and dry lab) has an estimated cost of 29 tCO2e/person? Or that a single analysis of RNA seq data of 10 million 100bp reads aligned to P.falciparum (13Gb, 1h 30) cost 240g CO2e? Also data storage has an environmental impact: a data center uses as much water as three average-sized hospitals. So our work also leaves a carbon footprint that we should be aware of and try to reduce it as much as possible. What can we do?</p> <ul> <li>Reduce electricity<ul> <li>Turn off the screens when you are not in the lab. Also the mouse and the keyboard</li> <li>Lock/Suspend the computer when it is not used</li> <li>Turn off the light of the lab when you are the last one leaving</li> </ul> </li> <li>Reduce the CO2 cost of your coding: you can estimate the CO2 footprint of your scripts or pipelines and make it more efficiently with many tools like:<ul> <li>nf-co2footprint</li> <li>CodeCarbon</li> <li>carbontracker</li> <li>Check out more here</li> </ul> </li> </ul> <p>Check Lo\u00efc Lannealongue talk  in Nextflow Submit 2024 in Barcelona to learn more about this.</p>"},{"location":"BBGProtocols/BBglab_data_organization/#reference","title":"Reference","text":"<ul> <li>Elisabet Figuerola</li> <li>Ferriol Calvet</li> </ul>"},{"location":"BBGProtocols/coffee_system/","title":"Info for bbg-coffee drinkers","text":""},{"location":"BBGProtocols/coffee_system/#system-for-the-coffee","title":"System for the coffee","text":"<ul> <li>Every week there is one person responsible for cleaning the coffee machine.</li> <li>The list is here in the same group meetings &amp; agora sheet, on the right side.</li> <li>The same list works for cleaning the coffee machine (weekly, our names are on it) and buying coffee (every time we buy we write our name).</li> <li>Anytime there is one package left of coffee, we tell the next person in the list, and once this person buys coffee we will write the name on the same week this person bought the coffee. Once there is the last coffee package open, we tell the next person, and so on.</li> <li>Each one of us will buy the coffee according to our coffee consumption.</li> </ul> <p>The ratio is:</p> <p>5 coffees per week - buy 1 kg of coffee (and scale accordingly)</p>"},{"location":"BBGProtocols/coffee_system/#important-notes","title":"Important notes","text":"<ul> <li>The 5 coffees are average, just try to make an approximation, doesn't need to be super exact. You can use the amount of coffees you've been taking last month, for instance, and make the mean per week.</li> <li>It is a shared responsibility to know if we are using the last coffee package. I recommend telling the next person in the list whenever we are opening the last package.</li> <li>It is a shared responsibility to clean the coffee machine every week. I recommend you to annotate in your calendar your turn to clean the coffee machine. The list is repeated several times, so you can annotate it for any time.</li> <li>If you can't clean the coffee machine that week, you can exchange your turn with another person.</li> <li>Only Monica, Erika and Martina have permissions to edit the spreadsheet. But you can put comments on the cells.</li> <li>Some brands that are OK:<ul> <li>Nestle Bonka.</li> <li>Bon preu</li> <li>Bellarom (the brand from LidL) </li> </ul> </li> <li>With this system we would buy like once a year, enough time to find where to buy these brands</li> <li>If you drink coffee very rarely, you don't need to enter the system, we can invite you </li> <li>Feel free to add or remove yourself from the list whenever you change your coffee consumption routine. Just tell Monica and she will modify the list in the spreadsheet.</li> </ul>"},{"location":"BBGProtocols/coffee_system/#how-to-clean-the-coffee-machine","title":"How to clean the coffee machine","text":"<ol> <li> <p>Turn off the coffee machine and unsamble all the parts shown in the pictures below, in that order:</p> <p>1 2 </p> <p>3 4 </p> <p>5 </p> <p>To extract this inner part (coffee grinder), press the red buttons and pull out.</p> </li> <li> <p>Wash with soap all the parts EXCEPT the grinder. You can use the sink in the EBL, ask any member of the wet lab. The grinder is cleaned just by running water (NO SOAP).</p> </li> <li>Rinse all pieces and dry it with paper.</li> <li>Put back all the pieces, in the inverted order in which you took them off.</li> </ol> <p>Remember to tidy up a little bit, for any remaining coffee leftovers on the table.</p> <p>Thanks a lot!</p>"},{"location":"BBGProtocols/coffee_system/#slack-coffee-bot","title":"Slack coffee bot","text":"<p>It is easy to forget when it is your turn to clean the coffee machine, many of us have been there. So, to help you remember, we have a Slack bot user called <code>@Coffee reminder BOT</code> that sends a reminder every Monday of the week to the person who is responsible for cleaning the coffee machine. This bot sends a direct message to the individual person.</p> <p>The source code as well as the instructions for using it can be found in the Coffee bot GitHub repository, maintained by @CarlosLopezElorduy.</p> <p>Every time a new person joins or exits the lab, a specific file within the code of the bot needs to be updated. If this happens, please contact any of the people in the references below, and they will update the required information.</p>"},{"location":"BBGProtocols/coffee_system/#references","title":"References","text":"<ul> <li>Monica</li> <li>Martina</li> <li>Erika</li> <li>Carlos</li> </ul>"},{"location":"Cluster_basics/Backups/","title":"Backups","text":"<p>As explained in the Structure section, there are different partitions with different levels of backups. Basically, there are two options of backup:  </p> <ul> <li>Snapshots : It is the state of the system at a particular point in time. It is usefull for human mistakes, ie. if you delete or edit the wrong file. For HW errors or big catastrophes (e.g. a fire) (unlikely) is not useful, because the backup data is stored in the same disk/location. We can recover the data by ourselves.</li> <li>Standard backup : Useful for human mistakes and HW/catastrophes. To recover the data from a backup, we need to contact IT and it could take a few days for recovering.</li> </ul> <p>Depending the partition, the safety level is different:</p> <ul> <li><code>home/</code> : Snapshots: 3 per day last 1.5 days, one for each of the last 5 days and one for each of the last 4 weeks. medium-high safe.</li> <li><code>projects/</code> : Standard backups and snapshots. Backup: every day during the last 15 days and every week during the last 12 weeks. Snapshots: 3 per day last 5 days, one for each of the last 15 days and one for each of the last 12 weeks. high safe.</li> <li><code>datasets/</code> : Standard backups. Backup: every Sunday, replaced every week. medium safe.</li> <li><code>datasafe/</code> : Snapshots: 3 per day last 5 days, one for each of the last 15 days and one for each of the last 12 weeks. medium-high safe.</li> <li><code>nobackup/</code>/ <code>nobackup2/</code> : No backup at all...</li> </ul>"},{"location":"Cluster_basics/Backups/#example-of-recovering-data","title":"Example of recovering data","text":"<p>Let's say we have deleted or edited by mistake a file in a partition with snapshots (e.g. <code>/data/bbg/projects/</code>). If we check the content of the <code>.snapshot/</code> folder:</p> <pre><code>mgrau@login01:/data/bbg/projects$ ls /data/bbg/projects/.snapshot/\ndaily_at_23_noSun.2023-06-05_2300  daily_at_23_noSun.2023-06-15_2300        hourly_mon2fri_11_15_19.2023-06-16_1900  hourly_mon2fri_11_15_19.2023-06-21_1900  Sun_at_23.2023-05-14_2300\ndaily_at_23_noSun.2023-06-06_2300  daily_at_23_noSun.2023-06-16_2300        hourly_mon2fri_11_15_19.2023-06-19_1100  hourly_mon2fri_11_15_19.2023-06-22_1100  Sun_at_23.2023-05-21_2300\ndaily_at_23_noSun.2023-06-07_2300  daily_at_23_noSun.2023-06-17_2300        hourly_mon2fri_11_15_19.2023-06-19_1500  hourly_mon2fri_11_15_19.2023-06-22_1500  Sun_at_23.2023-05-28_2300\ndaily_at_23_noSun.2023-06-08_2300  daily_at_23_noSun.2023-06-19_2300        hourly_mon2fri_11_15_19.2023-06-19_1900  Sun_at_23.2023-04-02_2300                Sun_at_23.2023-06-04_2300\ndaily_at_23_noSun.2023-06-09_2300  daily_at_23_noSun.2023-06-20_2300        hourly_mon2fri_11_15_19.2023-06-20_1100  Sun_at_23.2023-04-09_2300                Sun_at_23.2023-06-11_2300\ndaily_at_23_noSun.2023-06-10_2300  daily_at_23_noSun.2023-06-21_2300        hourly_mon2fri_11_15_19.2023-06-20_1500  Sun_at_23.2023-04-16_2300                Sun_at_23.2023-06-18_2300\ndaily_at_23_noSun.2023-06-12_2300  hourly_mon2fri_11_15_19.2023-06-15_1900  hourly_mon2fri_11_15_19.2023-06-20_1900  Sun_at_23.2023-04-23_2300\ndaily_at_23_noSun.2023-06-13_2300  hourly_mon2fri_11_15_19.2023-06-16_1100  hourly_mon2fri_11_15_19.2023-06-21_1100  Sun_at_23.2023-04-30_2300\ndaily_at_23_noSun.2023-06-14_2300  hourly_mon2fri_11_15_19.2023-06-16_1500  hourly_mon2fri_11_15_19.2023-06-21_1500  Sun_at_23.2023-05-07_2300\n</code></pre> <p>We can see a daily snapshot at 23.00h during the last 15 days (<code>daily_at_23_noSun.2023-06-XX_2300</code>). Then we have 3 snapshots per day (at 11h,15h and 19h) during the last 5 working-days (<code>hourly_mon2fri_11_15_19.2023-06-XX_XX00</code>) and then we have one snapshot weekly (sunday at 23h) during the last 12 weeks (<code>Sun_at_23.2023-0X-XX_2300</code>).</p> <p>Inside every snapshot, we can see the same file structure of <code>projects</code>:</p> <pre><code>mgrau@login01:/data/bbg/projects$ ls /data/bbg/projects/.snapshot/daily_at_23_noSun.2023-06-05_2300\nall_aecc                      clustering_3d          diskusage20200511.txt   healthy_chemo              nanopore              regulatory_regions        small_collaborations_ines\nall_aecc_pediatric            cndrivers              diskusage20200619.txt   hotmaps_signatures         neoantigen            repair_states             stjude\nalphafold_features            colorectal_apoe        diskusage20200725.txt   immune_biomarkers          new_oncodrivemut      replication_timing        st_jude_life\nbgframework                   courses                diskusage20200926.txt   immune_pheno_hartwig       noncoding_regions     reverse_calling           stockholm_ai\nbladder_ts                    cptac_analysis         diskusage20201011.txt   intogen                    nonsense_cptac        rhabdoid_tumors           structural_variants\nblca_eduardporta              damage_maps            diskusage_20211119.txt  intogen_2017               olivia                sample_specific_features  test_folder_delete\nboostdm                       dde                    diskusage.txt           intogen_plus               oncodrive             sample_specific_profiles  tf_mutations\nboostdm_ch                    degrons                driver_potential        Liver_Mouse                oncodrive3d           samuels_hmf               translation_fidelity\nboostdm_germline_sensitivity  diskusage20200104.txt  exemple_test            meso_exomes                oncodriveclustl       sars_cov_2                ubiquitins\nbreakpoints                   diskusage20200115.txt  expression_signatures   methyl_predictors          oriol_aml_intogen     scell_tall                worms\nbuild_table.py                diskusage20200201.txt  genomewide_mmr          miguel_nanopore            pagerank_combination  service                   zfp36l1\ncgi                           diskusage20200215.txt  genomewide_MMR          mutfootprints              pancreas_meritxell    sherlock\ncgi_clinics                   diskusage20200319.txt  genomic_regions         mutfootprints_code_review  pepe_clustering       signature_sensitivity\nchemogenomics                 diskusage20200330.txt  hairpins                mutograph                  periodicity           signet\nchemotrans                    diskusage20200421.txt  hartwig                 mut_region_profile         pileup_mappability    simuclones\nclonalhemato_ukb              diskusage20200427.txt  hartwig_signatures_id   mut_risk                   prominent             sjd_pediatric_tumors\n</code></pre> <p>We can then copy back the file deleted to the original location.</p>"},{"location":"Cluster_basics/Backups/#reference","title":"Reference","text":"<ul> <li>Miguel Grau</li> <li>Jordi Deu-Pons</li> </ul>"},{"location":"Cluster_basics/Dynamic_paths/","title":"Dynamic path configuration","text":"<p>To ensure compatibility when running scripts or notebooks across different clusters (IRB or BBG), you can use the following Python line to dynamically set the working directory based on the machine hostname:</p> <pre><code>import socket\n\nWORKSPACE = \"/data/bbg\" if socket.gethostname().startswith(\"irb\") else \"/workspace\"\n</code></pre> <p>Warning</p> <p>This line assumes the script is being executed on a working node, where the hostname reflects the actual compute environment. It will not work if you are on the login node, which you shouldn't be using for computation in any case.</p>"},{"location":"Cluster_basics/Dynamic_paths/#how-to-use","title":"How to Use","text":""},{"location":"Cluster_basics/Dynamic_paths/#option-1-directly-in-a-notebook-or-script","title":"Option 1: directly in a notebook or script","text":"<p>Add the above line at the top of your Jupyter notebook or Python script:</p> <pre><code>import socket\n\nWORKSPACE = \"/data/bbg\" if socket.gethostname().startswith(\"irb\") else \"/workspace\"\n\nexample_path = f\"{WORKSPACE}/projects/my_project/\"\nprint(f\"Path: {example_path}\")\n</code></pre>"},{"location":"Cluster_basics/Dynamic_paths/#option-2-using-a-file-including-global-variables","title":"Option 2: Using a file including global variables","text":"<p>For better reusability, define this variable in a separate Python file (e.g., <code>global_variables.py</code>):</p> <pre><code>import socket\n\nWORKSPACE = \"/data/bbg\" if socket.gethostname().startswith(\"irb\") else \"/workspace\"\n</code></pre> <p>Then, import it in your notebook or script:</p> <pre><code>from global_variables import WORKSPACE\n\nexample_path = f\"{WORKSPACE}/projects/my_project/\"\nprint(f\"Path: {example_path}\")\n</code></pre> <p>This way, you only need to define the path logic once and can reuse it throughout your codebase.</p> <p>Info</p> <p>Keeping shared variables\u2014like paths, pipeline run names, or directory names\u2014in a single Python file that can be imported from other scripts or notebooks is a good practice. It helps avoid inconsistencies and reduces the risk of forgetting to update paths or values across multiple files.</p>"},{"location":"Cluster_basics/Dynamic_paths/#reference","title":"Reference","text":"<ul> <li>Stefano Pellegrini</li> </ul>"},{"location":"Cluster_basics/Headers/","title":"Headers","text":""},{"location":"Cluster_basics/Headers/#description","title":"Description","text":"<p>When you are in the cluster, in order to visualize the column names from a file with a table, you can use the command:</p> <pre><code>headers name_file.tsv\n</code></pre> <p>This will return you the name of the columns with the number of the column, for example:</p> <pre><code>1 tumor_type\n2 gene\n3 chr\n4 pos\n5 ref\n6 alt\n</code></pre>"},{"location":"Cluster_basics/Headers/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Paula Gomis</li> </ul>"},{"location":"Cluster_basics/IRB_cluster/","title":"IRB cluster","text":"<p>This link redirects you to Miguel's Agora about the new IRB cluster. Index of the contents:</p> <ul> <li>Cluster overview (structure, partitions, QoS, spot)</li> <li>Storage: NetApp, s3 and scratch</li> <li>SLURM basics</li> <li>How to use environments: Conda, easyBuild, spack, singularity\u2026</li> <li>Qmap</li> <li>Jupyter notebook</li> <li>Backups</li> <li>What changes when using nextflow?</li> <li>First steps</li> </ul>"},{"location":"Cluster_basics/IRB_cluster/#references","title":"References","text":"<ul> <li>Miguel Grau</li> <li>Davide Scarpetta</li> </ul>"},{"location":"Cluster_basics/Interactive/","title":"Interactive","text":"<p>The <code>interactive</code> command gives to the user an interactive shell in the cluster with slurm allocation. In other words, it allocates the user to a specific node of the cluster so that the jobs can be executed there without disturbing the rest of the users.</p>","tags":["IRBCluster","BBGCluster","HPC"],"boost":2},{"location":"Cluster_basics/Interactive/#usage","title":"Usage","text":"<p>There is a difference between running an interactive session within the BBGCluster or the IRB Cluster. Please select the tab accordingly to where you are working.</p> BBG ClusterIRB Cluster <p>Once you enter the bbgcluster, you will see in the terminal <code>&lt;username&gt;@login01</code>. It is here where,  if you want to be allocated to your own node, you can just run the command:</p> <pre><code>$ interactive\n</code></pre> <p>If the <code>login01</code> has changed to <code>bbgn###</code> where <code>###</code> is the number identifying the current node.</p> <p>Apart from the basic use, there are optional arguments/flags for extra features:</p> <ul> <li><code>-c</code>: Number of CPU cores (default: 1)</li> <li><code>-m</code>: Total amount of memory (GB) (default: 8 [GB])</li> <li><code>-w</code>: Target node</li> <li><code>-J</code>: Job name</li> <li><code>-x</code>: Binary that you want to run interactively</li> <li><code>-p</code>: specify queue (see queues)</li> </ul> <p>In order to run <code>interactive</code> command in the IRB cluster you need to tell your terminal where to find it first.</p> <p>Installation</p> <p>The <code>interactive</code> script is located here: <code>/data/bbg/software/bin/interactive</code>. In order to have it always available from wherever directory you want to run it, you need to add the following to your <code>.bashrc</code></p> <pre><code>    export PATH=/home/$USER/.local/bin/:/data/bbg/software/bin/:$PATH\n</code></pre> <p>Once you enter the irbcluster, you will see in the terminal <code>&lt;username&gt;@irblogin01</code>. Here is where you'd need to allocate desired resources to start working. The command to run is the following:</p> <pre><code>$ interactive\n</code></pre> <p>If the <code>irblogin01</code> has changed to <code>irbcn##</code> where <code>###</code> is the number identifying the current node.</p> <p>Apart from the basic use, there are optional arguments/flags for extra features:</p> <ul> <li><code>-c</code>: Number of CPU cores (default: 1)</li> <li><code>-m</code>: Total amount of memory (GB) (default: 8 [GB])</li> <li><code>-w</code>: Target node</li> <li><code>-J</code>: Job name</li> <li><code>-x</code>: Binary that you want to run interactively</li> <li><code>-p</code>: specify queue (see queues) (default: <code>bbg_cpu_zen4</code>) </li> <li><code>-q</code>: specify the QoS (see slides)</li> </ul>","tags":["IRBCluster","BBGCluster","HPC"],"boost":2},{"location":"Cluster_basics/Interactive/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>","tags":["IRBCluster","BBGCluster","HPC"],"boost":2},{"location":"Cluster_basics/NewFolders/","title":"New folder in workspace","text":""},{"location":"Cluster_basics/NewFolders/#description","title":"Description","text":"<p>Appart from your own personal folder, there are four main shared folders within the <code>/workspace</code> folder in the cluster. These are:</p> <ul> <li>Projects: analysis/results files. Backup and snapshots. High safe.</li> <li>Datasets: data files downloaded from public/private repositories. Re-downloading is possible/straightforward. Backup. Medium safe.</li> <li>No backup: intermediate files generated during pipelines execution or big amount of data re-downloadable. Low safe.</li> <li>Datasafe: datasets generated by us or from collaborators, not in public repositories. Snapshots. High safe.</li> </ul> <p>If you want to create a new folder inside one of the previously mentioned, perform the following instructions:</p> <ol> <li>Go to the bbgdashboard</li> <li>Log in (or check that you are already logged in)</li> <li> <p>Go to the <code>Cluster</code> tab.</p> <p></p> </li> <li> <p>Click on one of the four options where you want to create your folder.</p> <p></p> </li> <li> <p>Write the title and a description of the new folder and click \"OK\".</p> <p></p> </li> </ol>"},{"location":"Cluster_basics/NewFolders/#possible-errors","title":"Possible errors","text":"<p>If after clicking the \"OK\" button an ERROR message is displayed:</p> <ul> <li>Make sure that you are logged in in the dashboard itself.</li> <li>Try doing everything from an incognito window.</li> </ul>"},{"location":"Cluster_basics/NewFolders/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Miguel Grau</li> <li>Jordi Deu-Pons</li> </ul>"},{"location":"Cluster_basics/Notebooks_in_cluster/","title":"Running notebooks in the cluster","text":""},{"location":"Cluster_basics/Notebooks_in_cluster/#description","title":"Description","text":"<p>Running a jupyter notebook in the cluster allows you to work with a notebook which will be running even if you disconnect from the cluster.</p> <p>This is especially useful for time-consuming/memory-consuming processes or notebooks with a high number of variables/packages needed, so that you have more computational power than your local computer, you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress and you can come back to a notebook without the need of loading all the variables/packages again.</p> <p>To run a notebook in the cluster, a screen and an interactive will be used.</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#create-a-notebook","title":"Create a notebook","text":"<p>You will need to follow the next steps:</p> <ol> <li> <p>Connect to the cluster:</p> <pre><code># bbgcluster\nssh -p 22022 &lt;username&gt;@bbgcluster\n\n#irb cluster\nssh -p 22022 &lt;username&gt;@irblogin02.irbbarcelona.pcb.ub.es\n</code></pre> </li> <li> <p>Open a screen:</p> <pre><code>&lt;username&gt;@login01:~$ screen -S &lt;screen_name&gt;\n</code></pre> </li> <li> <p>Run an interactive job and remember the node you are assigned to (e.g. bbgn005)</p> <pre><code>[screen_name] &lt;username&gt;@login01:~$ interactive\n</code></pre> <p>Info</p> <p>If your notebook needs more than 8G and 2 cores, you can specify it here -- see interactive section.</p> </li> <li> <p>Activate conda base or the conda environment that you need in your notebook:</p> <pre><code>[screen_name] &lt;username&gt;@bbgn005:~$ conda activate &lt;your environment&gt;\n</code></pre> </li> <li> <p>Go to the folder that you wish to run the notebook:</p> <pre><code>(base)[screen_name] &lt;username&gt;@bbgn005:~$ cd /data/bbg/folder\n</code></pre> </li> <li> <p>Run the jupyter notebook:</p> NotebookLab <p>Copy the following command:</p> <p><code>bash &lt;!--markdownlint-disable MD046--&gt; unset XDG_RUNTIME_DIR &amp;&amp; jupyter notebook --ip=0.0.0.0</code></p> <p>Copy the following command:</p> <pre><code>unset XDG_RUNTIME_DIR &amp;&amp; jupyter lab --ip=0.0.0.0\n</code></pre> </li> <li> <p>Keep the URL with the token and the port (e.g.8888) in which the interactive is running:</p> <pre><code>[I 10:37:20.371 NotebookApp] The Jupyter Notebook is running at: http://127.0.0.1:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c\n</code></pre> </li> </ol> <p>Now, your notebook is running and you can dettach from the screen by pressing <code>Ctrl + A + D</code>. You can now close the terminal and the notebook will continue running in the cluster.</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#open-a-notebook","title":"Open a notebook","text":"<p>In order to open an already existing notebook, you'll need to know the port (e.g 8888) and the node of the cluster (e.g bbgn005) where you created it in the previous step.</p> <pre><code># bbgcluster example\nssh -L &lt;port&gt;:&lt;node&gt;:&lt;port&gt; -p 22022 &lt;username&gt;@bbgcluster\n# For example: ssh -L 8888:bbgn005:8888 -p 22022 clopeze@bbgcluster\n\n# IRB cluster example. No port required\nssh -L &lt;port&gt;:&lt;node&gt;:&lt;port&gt; &lt;username&gt;@irblogin02.irbbarcelona.pcb.ub.es\n# For example: ssh -L 8888:irbccn39:8888 mgrau@irblogin02.irbbarcelona.pcb.ub.es\n</code></pre> <p>Note</p> <p>If you don't want to remember these commands, you can create an alias. You can do this by adding the following lines to your <code>~/.bashrc</code> file or <code>~/.bash_aliases</code> file:</p> <pre><code>notebook(){\n    # Ask for cluster user\n    echo \"Enter cluster user: \"\n    read user\n    # Ask for node\n    echo \"Enter node name: \"\n    read node\n    # Ask for port\n    echo \"Enter port: \"\n    read port\n    # Connect to ssh cluster\n    ssh -L $port:$node:$port -p 22022 ${user}@bbgcluster\n}\n</code></pre> <p>Open the URL you obtain when creating the notebook in the cluster (step 7).</p> <pre><code># For example: http://127.0.0.1:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c\n</code></pre>"},{"location":"Cluster_basics/Notebooks_in_cluster/#close-a-notebook","title":"Close a notebook","text":"<p>When you don't need the notebook to continue running in the cluster, reconnect to the screen:</p> <pre><code>screen -r &lt;screen_name&gt;\n</code></pre> <p>And kill jupyter (Ctrl + C) and exit the screen (write <code>exit</code> in the terminal and press enter)</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#errors-and-solutions","title":"Errors and solutions","text":""},{"location":"Cluster_basics/Notebooks_in_cluster/#i-forgot-the-url-of-the-notebook","title":"I forgot the URL of the notebook","text":"<ol> <li> <p>Enter to the cluster and check your screens:</p> <pre><code>screen -ls\n</code></pre> </li> <li> <p>Enter the screen where you have your notebook:</p> <pre><code>screen -r &lt;screen name&gt;\n</code></pre> </li> <li> <p>Scroll up until you find the URL, which should look like:</p> <pre><code>http://127.0.0.1:8888/?token=730ea7a95c02207c9fb7cbd434c2de81e03168845d42c23c\n</code></pre> </li> </ol>"},{"location":"Cluster_basics/Notebooks_in_cluster/#my-notebook-doesnt-open","title":"My notebook doesn't open","text":"<p>One possibility is that the running notebook in the cluster has crashed. You can check this by going to the cluster, entering the screen where you have the notebook and check if it is still running.</p> <p>If not, you should create a notebook following the steps at the beginning of this page (Create a new notebook).</p>"},{"location":"Cluster_basics/Notebooks_in_cluster/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez-Elorduy</li> <li>Paula Gomis</li> <li>Federica Brando</li> </ul>"},{"location":"Cluster_basics/Screen/","title":"Screen","text":""},{"location":"Cluster_basics/Screen/#description","title":"Description","text":"<p>The <code>screen</code> command opens a session which will be running even if you disconnect from the cluster.</p> <p>This is especially useful for time-consuming processes, so that you can leave them running in the background without the fear of accidentally disconnecting and losing all the progress.</p> <p>You can also open several screens for different processes, which you can detach and attach to them as you like.</p>"},{"location":"Cluster_basics/Screen/#basic-commands","title":"Basic commands","text":""},{"location":"Cluster_basics/Screen/#new-screen","title":"New screen","text":"<p>Creates a new screen with name \"custom_name\".</p> <pre><code>screen -S &lt;custom_name&gt;\n</code></pre> <p>Warning</p> <p>When opening a new screen, this should be done from the <code>login01</code> node, since this guarantees that the screen will be constantly running and not shut down (which could happen if the screen is opened in one of the other nodes).</p>"},{"location":"Cluster_basics/Screen/#list-screens","title":"List screens","text":"<p>List all the created screens.</p> <pre><code>screen -ls\n</code></pre>"},{"location":"Cluster_basics/Screen/#detach","title":"Detach","text":"<p>Detaches from a screen</p> <pre><code>Ctrl + A -&gt; D\n</code></pre>"},{"location":"Cluster_basics/Screen/#re-attach","title":"Re-attach","text":"<p>Re-attaches to a detached screen.</p> <pre><code>screen -r [#]\n</code></pre> <p>Naming a screen session which is already open:</p> <ol> <li>Ctrl +A </li> <li>type <code>:sessionname mySessionName</code> (the semicolon is needed, replace mySessionName by the name of your preference)</li> </ol> <p>Note</p> <p>If there are multiple screens available, include the number of the screen id (or name) to identify which screen to re-attach.</p>"},{"location":"Cluster_basics/Screen/#exit-and-kill-screen","title":"Exit and kill screen","text":"<pre><code>exit\n</code></pre>"},{"location":"Cluster_basics/Screen/#kill-a-detached-screen","title":"Kill a detached screen","text":"<pre><code>screen -X -S [screen number ID or name] quit\n</code></pre>"},{"location":"Cluster_basics/Screen/#kill-all-screens","title":"Kill all screens","text":"<pre><code>pkill screen\n</code></pre>"},{"location":"Cluster_basics/Screen/#force-screen-detach-and-reattach-to-your-current-terminal","title":"Force screen detach and reattach to your current terminal","text":"<p>This command can be useful when a screen session remains attached (e.g., after a dropped SSH connection), preventing access. This situation may occur if the session was not properly detached or became a ghost session.</p> <p>To check your screen status, use the following command:</p> <pre><code>screen -ls\n\nThere are screens on:\n 1234567.myscreen (Attached)\n</code></pre> <p>To force detach and reattach to your current terminal :</p> <pre><code>screen -D -r 1234567.myscreen\n</code></pre>"},{"location":"Cluster_basics/Screen/#documentation","title":"Documentation","text":"<p>For a more extensive list of commands, check the screen cheatsheet.</p> <p>You can also check the full documentation.</p>"},{"location":"Cluster_basics/Screen/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Rocio Chamorro</li> </ul>"},{"location":"Cluster_basics/Structure/","title":"Structure","text":""},{"location":"Cluster_basics/Structure/#description","title":"Description","text":"<p>The workspace is organized in several folders, each of them with different purposes and different security backups.</p> <ul> <li>Projects : Files from analysis or results obtained in the projects. It has backups and snapshots so it is highly safe.</li> <li>Datasets : Data files that have been downloaded from public or private repositories and re-downloading the is possible or straightforward. It only has backups, so it is medium safe.</li> <li>nobackup/nobackup2 : Intermediate files that have been generated during pipelines execution or big amounts of data that is re-downloadable. It is lowly safe.</li> <li>Datasafe : Datasets that have been generated by us or from collaborators, and it is not in public repositories. It has Snapshots and it is highly safe.</li> </ul>"},{"location":"Cluster_basics/Structure/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> </ul>"},{"location":"Cluster_basics/queues/","title":"Queues","text":"<p>When submitting jobs to the bbgcluster, with interactive or e.g., sbatch, you have the option to specify the queue/partition where sending the job. By default (if not specified), it goes to the <code>normal</code>partition.</p>"},{"location":"Cluster_basics/queues/#choose-the-correct-queue","title":"Choose the correct queue","text":"<p>Selecting the right queue is crucial for the proper functioning of the cluster:</p> <ul> <li><code>normal</code>: Standard job in terms of time and resources. E.g, notebook, an <code>interactive</code>, etc</li> <li><code>bigmem</code>: Job requiring a lot of memory. 512Gb at max.</li> <li><code>bigrun</code>: Job requiring a lot of nodes during hours/days (a lot of cpus/mem). Normally this case is when using a (nextflow) pipeline and your main job is submitting many sub-jobs to the queue for days. Optimal for intogen/boostDM, sarek, deepUMIcaller,... jobs</li> </ul>"},{"location":"Cluster_basics/queues/#list-queues","title":"List queues","text":"<p>Once you enter the bbgcluster, you will see in the terminal <code>&lt;username&gt;@login01</code>:</p> <pre><code>&lt;username&gt;@login01:~$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnormal*      up   infinite      1   drng bbgn004\nnormal*      up   infinite      6    mix bbgn[012,014,016,019-020,022]\nnormal*      up   infinite      2  alloc bbgn[021,023]\nnormal*      up   infinite     11   idle bbgn[005-011,013,015,017-018]\nbigmem       up   infinite      2    mix bbgn[020,022]\nbigmem       up   infinite      2  alloc bbgn[021,023]\nbigrun       up   infinite      6    mix bbgn[012,014,016,019-020,022]\nbigrun       up   infinite      2  alloc bbgn[021,023]\nbigrun       up   infinite      8   idle bbgn[008-011,013,015,017-018]\n</code></pre> <p>In here, you can see all the three partitions (<code>normal</code>, <code>bigmem</code> and <code>bigrun</code>) and  the status of every working node (bbgnXXX). The possible states are:</p> <ul> <li><code>drng</code>: The node is under maintenance.</li> <li><code>alloc</code>: Nodes with no resources available</li> <li><code>idle</code>: Nodes with all the resources available.</li> <li><code>mix</code>: Nodes with some resources available.</li> </ul>"},{"location":"Cluster_basics/queues/#nextflow-example","title":"Nextflow example","text":"<p>You can edit your nextflow config file and define the partition:</p> <pre><code>process { \nexecutor = 'slurm' \nqueue = 'bigrun' \nerrorStrategy = 'retry' \nmaxRetries = 2 \n} \n\nexecutor { \nqueueSize = 30 \n}\n</code></pre>"},{"location":"Cluster_basics/queues/#reference","title":"Reference","text":"<ul> <li>Miguel Grau</li> <li>Federica Brando</li> </ul>"},{"location":"Cluster_basics/s3/","title":"S3 storage","text":""},{"location":"Cluster_basics/s3/#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is a S3 Storage</li> <li>How to access<ul> <li>Minio-web</li> <li>Terminal</li> </ul> </li> <li>How to use it<ul> <li>Seqera and Nextflow</li> <li>Python</li> </ul> </li> </ul>"},{"location":"Cluster_basics/s3/#what-is-a-s3-storage","title":"What is a S3 Storage?","text":"<p>S3 (Simple Storage Service) is an object storage service. It stores data as objects inside buckets, rather than as files in directories. Each object consists of:</p> <ul> <li>Data (the file itself),</li> <li>Metadata (descriptive tags), and</li> <li>A unique key (the object's identifier within a bucket).</li> </ul> <p>Unlike traditional file systems like our standard partitions (NetApp), S3 is not a native POSIX filesystem. It's not designed for direct file editing or low-latency access. While it can be mounted using tools like <code>rclone mount</code> or <code>s3fs</code>, this is more of a workaround. It may not behave exactly like a regular shared folder. Instead, most modern tools and workflows such as Nextflow, Snakemake, and Python libraries are increasingly adapted to interact with S3 natively. This offers better performance and scalability for data-intensive pipelines.</p> <p>S3 offers several advantages: it's scalable, ideal for storing large volumes of data (including hundreds of GB or more). It also supports fine-grained access control, and integrates well with cloud-native or container-based pipelines. In our case, S3 is used as static storage for raw data and final results, not as a working directory where files are constantly modified.</p> <p>We use Minio, a local S3-compatible server (instead of AWS S3) to manage this storage on-premises.</p>"},{"location":"Cluster_basics/s3/#how-to-access","title":"How to access","text":"<p>Note</p> <p>S3 only available from the new IRB cluster</p> <p>By default, MinIO web access to S3 is available to all BBG members. It allows browsing buckets and downloading small files (e.g., metadata, images).</p> <p>However, to access S3 from the terminal (e.g., to navigate, run Python scripts, or execute Nextflow pipelines outside Seqera), you need to generate S3 credentials.</p> <p>If you only want to use S3 files in your Nextflow pipelines through seqera platform, no further setup is needed. Credentials are already configured in the pipelines; you only need to export them.</p>"},{"location":"Cluster_basics/s3/#minio-web","title":"Minio Web","text":"<p>Using your LDAP credentials (the same user/password used for the cluster), log in at:</p> <p>irbminio.irbbarcelona.pcb.ub.es:9001</p> <p>Depending your project, you will see different buckets: </p>"},{"location":"Cluster_basics/s3/#terminal","title":"Terminal","text":"<p>To use it via terminal, once in the IRB cluster:</p> <pre><code># Load conda environment\n$ ml load anaconda3/2023.09-0-yjzjr4h\n$ module load anaconda3\n$ conda activate s3-minio\n\n# Generate credentials\n# -r -&gt; creates the ~/.config/rclone/rclone.conf file\n# -d -&gt; duration in days for credentials validity\n$ python3 /apps/scripts/irb-storage-public-scripts/minio/minio-sts-credentials-request.py -u mgrau -r -d 365\n\n# test\n$ rclone lsf irb-minio://bbg\n</code></pre> <p>This script (minio-sts-credentials-request.py) generates the config file used by rclone:</p> <pre><code>$ cat ~/.config/rclone/rclone.conf\n\n[irb-minio]\ntype = s3\nprovider = Minio\nendpoint = http://irbminio.irbbarcelona.pcb.ub.es:9000\nacl = bucket-owner-full-control\nenv_auth = false\naccess_key_id = ***\nsecret_access_key = ***\nsession_token = ***\n</code></pre> <p>If you want to use another S3 client, you need to create the credentials file. For example, for aws-cli (replace with  your keys/token from the rclone.config):</p> <pre><code>$ cat ~/.aws/credentials\n\n[default]\naws_access_key_id= **\naws_secret_access_key= **\naws_session_token= **\nendpoint_url = http://irbminio.irbbarcelona.pcb.ub.es:9000\n</code></pre> <p>As mentioned earlier, you can mount an S3 bucket as a POSIX-like partition (similar to what McGyver does):</p> <p>Warning</p> <p>This allows you to browse S3 buckets as if they were part of the local file system, although it's typically read-only and it is not recommended for high-performance or heavy I/O use cases.</p> <pre><code>$ ls /home/mgrau/s3/bbg-scratch/\n# Mount\n$ rclone mount irb-minio:bbg-scratch /home/mgrau/s3/bbg-scratch --vfs-cache-mode off --read-only &amp;\n[1] 636085\n$ ls /home/mgrau/s3/bbg-scratch/\nwork\n# Unmount\n$ fusermount -u /home/mgrau/s3/bbg-scratch\n[1]+  Done                    rclone mount irb-minio:bbg-scratch /home/mgrau/s3/bbg-scratch --vfs-cache-mode off --read-only\n$ ls /home/mgrau/s3/bbg-scratch/\n$\n</code></pre>"},{"location":"Cluster_basics/s3/#how-to-use-it","title":"How to use it","text":""},{"location":"Cluster_basics/s3/#seqera-and-nextflow","title":"Seqera and Nextflow","text":"<p>As mentioned, you don\u2019t need to include any credentials explicitly in Seqera to run a job. When accessing S3 data from a pipeline, remember to select the secrets and include the info in the config:</p> <p></p> <p>If using nextflow in a terminal, you can add the credentials to the nextflow.config</p>"},{"location":"Cluster_basics/s3/#python","title":"Python","text":"<p>To access an S3 file from a Python script, there are different libraries: boto3, s3fs and dask. All three will automatically use the credentials in <code>~/.aws/credentials</code> if it exists.</p> <p>Note</p> <p>AI recommends... Dask! Compared to s3fs and boto3, is much faster and optimized for parallel and distributed computing, enabling efficient processing of files hundreds of gigabytes in size. While boto3 is a low-level client and s3fs provides filesystem-like access, Dask builds on them to offer high-level, scalable data workflows that can handle massive datasets seamlessly.</p>"},{"location":"Cluster_basics/s3/#boto3","title":"boto3","text":"<pre><code>import boto3\nimport pandas as pd\nimport botocore\n\n\n# Initialize the S3 client\ns3 = boto3.client('s3')\n\n# Define your bucket and file key\nbucket_name = 'bbg'\nfile_key = 'data/example/file.vcf'\n\n# Read the file object directly, without downloading\nresponse = s3.get_object(Bucket=bucket_name, Key=file_key)\ncontent = response['Body'].read().decode('utf-8')\n\n# Extract the data starting from the VCF header (#CHROM ...)\nlines = content.strip().split('\\n')\nvcf_data = [line for line in lines if not line.startswith('##')]  # Skip metadata\n\n# Load the VCF data into a DataFrame\ndf = pd.read_csv(StringIO('\\n'.join(vcf_data)), sep='\\t', comment='#')\n\n# Display the first few rows\nprint(df.head())\n</code></pre>"},{"location":"Cluster_basics/s3/#s3fs","title":"s3fs","text":"<pre><code>import s3fs\nimport pandas as pd\n\n# Initialize the S3 file system\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'http://irbminio:9000'})\n\n# Read the VCF file with proper handling\nwith fs.open(\"bbg/data/example/file.vcf\", 'r') as f:\n    # Extract the data starting from the VCF header (#CHROM ...)\n    vcf_data = [line for line in f if not line.startswith('##')]\n\n# Convert the data to a DataFrame\nfrom io import StringIO\ndf = pd.read_csv(StringIO(''.join(vcf_data)), sep='\\t', comment='#')\n\n# Display the DataFrame\nprint(df.head())\n</code></pre>"},{"location":"Cluster_basics/s3/#dask","title":"dask","text":"<pre><code>import dask.dataframe as dd\nimport s3fs\n\n# Read the file using Dask (with appropriate filters)\ndf = dd.read_csv(\"s3://bbg/data/example/file.vcf\",\n                 sep='\\t', \n                 comment='#',  # Ignore metadata lines starting with `##`\n                 blocksize='16MB',  # Adjust block size as needed\n                 dtype='str')  # Ensures flexible data type handling\n\n# Display the first few rows\nprint(df.head())\n</code></pre>"},{"location":"Cluster_basics/terminal/","title":"Terminal","text":"<p>In this section we are going to show the basic commands to navigate and manage files with the terminal.</p>"},{"location":"Cluster_basics/terminal/#creating-folders-and-files","title":"Creating folders and files","text":""},{"location":"Cluster_basics/terminal/#directories-folders","title":"Directories (folders)","text":"Command Description <code>ls</code> Show the content of the current directory <code>cd anotherdir</code> Change directory to <code>anotherdir</code> <code>cd ..</code> Change to previous directory <code>cd ~</code> Change to home folder (<code>/home/&lt;user&gt;/</code>) <code>cd /</code> Change to root folder (<code>/</code>) <code>pwd</code> Show path of current directory <code>mkdir newdirectory</code> Create new directory called <code>newdirectory</code>"},{"location":"Cluster_basics/terminal/#files","title":"Files","text":"Command Description <code>touch newfile.txt</code> Create new file called <code>newfile.txt</code> <code>echo \"Hello BBGLab!\" &gt; newfile.txt</code> Save output of <code>echo</code> command into a file called <code>newfile.txt</code> <code>echo \"In the morning\" &gt;&gt; file.txt</code> Append to the end of the file <code>file.txt</code> the line output of <code>echo</code>"},{"location":"Cluster_basics/terminal/#moving-and-manipulating-files","title":"Moving and manipulating files","text":"Command Description <code>mv file.txt destination</code> Move file <code>file.txt</code> to directory <code>destination</code> <code>mv dir1/* destination</code> Move all contents of <code>dir1</code> to <code>destination</code> <code>mv file1.txt file2.txt</code> Change name of <code>file1.txt</code> to <code>file2.txt</code> <code>cp file.txt destination</code> Copy <code>file.txt</code> into directory <code>destination</code> <code>cp file1.txt file2.txt</code> Copy <code>file1.txt</code> into <code>file2.txt</code> <code>diff file1.txt file2.txt</code> Compare the contents of two files and display the differences <code>rm file.txt</code> Remove <code>file.txt</code> <code>rm -rf dir1</code> Remove directory <code>dir1</code> and ALL THE FILES INSIDE"},{"location":"Cluster_basics/terminal/#compressed-files","title":"Compressed files","text":"Command Description <code>gzip file.txt</code> Compress <code>file.txt</code> <code>gunzip file.txt</code> Expand <code>file.txt</code> <code>zcat file.txt.gz</code> Read a gzipped file without uncompressing"},{"location":"Cluster_basics/terminal/#other-basic-commands","title":"Other basic commands","text":"Command Description <code>echo \"message\"</code> Print <code>message</code> in the terminal <code>cat file.txt</code> Show contents of <code>file.txt</code> <code>less file.txt</code> Show contents of <code>file.txt</code> one page at a time <code>head file.txt</code> Show first 10 lines of <code>file.txt</code> <code>grep \"sentence\" file.txt</code> Search the word \"sentence\" in <code>file.txt</code> <code>find . -name \"file.txt\"</code> Find in current directory (<code>.</code>) the file <code>file.txt</code> <code>man echo</code> Manual (documentation) of command <code>echo</code> <code>which echo</code> Show the full path of command <code>echo</code>"},{"location":"Cluster_basics/terminal/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Federica Brando</li> <li>Miguel Grau</li> <li>Jordi Deu-Pons</li> <li>Laura Torrens</li> </ul>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/","title":"Qmap Submit","text":""},{"location":"Cluster_basics/Submitting_jobs/Qmap/#description","title":"Description","text":"<p>How to submit jobs to the Cluster using Qmap.</p> <p>Qmap documentation: https://qmap.readthedocs.io</p>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#howto","title":"Howto","text":""},{"location":"Cluster_basics/Submitting_jobs/Qmap/#1-prepare-qmap-file-example","title":"1. Prepare .qmap file (example)","text":"<pre><code>[params]\nmemory=50G\n\n[pre]\n . \"/home/$USER/miniconda3/etc/profile.d/conda.sh\"\nconda activate sciclone-env\n\n[jobs]\nRscript run.R ../data/vafs.dat bmm 3 ./results.beta.3\nRscript run.R ../data/vafs.dat gaussian.bmm 3 ./results.gaussian.3\nRscript run.R ../data/vafs.dat binomial.bmm 3 ./results.binomial.3\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#2-run-qmap-submit-from-the-login-node","title":"2. Run \"qmap submit\" from the login node","text":"<pre><code>qmap submit filename.qmap\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#aditional-info","title":"Aditional info","text":""},{"location":"Cluster_basics/Submitting_jobs/Qmap/#extra-parameters","title":"Extra parameters","text":"<p>If you need to provide any extra parameter directly to SLURM, you can do it defining a new profile.config file. E.g.</p> <pre><code>executor = slurm\n\n[params]\nextra = -p bigmem\n</code></pre> <p>By default, the queue used is <code>bigrun</code>.</p>"},{"location":"Cluster_basics/Submitting_jobs/Qmap/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> <li>Miguel Grau</li> </ul>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/","title":"SLURM","text":"<p>SLURM is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.</p>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#description","title":"Description","text":"<p>With SLURM commands you can SUBMIT and MANAGE jobs in the cluster.</p>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#submit","title":"SUBMIT","text":"<p>There are 3 major command to submit jobs:</p> <ul> <li><code>sbatch</code> : Submits a batch script for later execution</li> <li><code>srun</code> : Obtain a job allocation and execute an application</li> <li><code>salloc</code> : Obtain a job allocation</li> </ul> <p>Use <code>interactive</code>!</p> <p>Instead of using the above mentioned commands, you can use <code>interactive</code>, a in-house command that allows to allocate resources in the cluster. More reference here</p> <pre><code>$ interactive -h \nUsage: interactive [-c] [-m] [-w] [-J] [-x]\n\nOptional arguments:\n    -c: number of CPU cores (default: 1)\n    -m: total amount of memory (GB) (default: 8 [GB])\n    -w: target node\n    -J: job name\n    -x: binary that you want to run interactively\n    -p: specify queue\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#1-salloc","title":"1. <code>salloc</code>","text":"<p>Allows you to allocate the resources that you need on the cluster.</p> <pre><code>salloc [OPTIONS]\n</code></pre> <p>The options are plenty and can be inspected by typing <code>salloc --help</code> command.</p> Example <p><pre><code>salloc -w bbgn004 -J my_cool_job --mem-per-cpu=4GB\n</code></pre> It's asking to allocate 4GB of memory on node bbgn004 for a job named \"my_cool_job\"</p>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#2-srun","title":"2. <code>srun</code>","text":"<p>Submits an application for immediate execution. The command is interactive, it will output on the terminal that runs it and it is blocking, this means that you cannot use the terminal where you <code>srun</code> command.</p> <p>&lt;my_applicayion.sh&gt;</p> <pre><code>#!/bin/bash\n\necho \"$(date +'%r') This is a dummy script\"\nsleep 5s\necho \"$(date +'%r') Bye\"\n</code></pre> <p>Then you can run the application with the following:</p> <pre><code>$ srun my_dummy_script.sh \nsrun: job 8725263 queued and waiting for resources\nsrun: job 8725263 has been allocated resources\n06:33:32 PM This is a dummy script\n06:33:37 PM Bye\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#3-sbatch","title":"3. <code>sbatch</code>","text":"<p>Submits a batch script for later execution. You need a batch script and a set of parameters. There are a fair amount of parameters that can be inspected with the following command:</p> <p><code>sbatch -h</code> or <code>sbatch -u</code></p> <p>An example of batch script is the following:</p> <p>&lt;my_job_script&gt;</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:30:00\n#SBATCH -c 8\n#SBATCH --mem 16gb\n#SBATCH -J my_job_name\n\n# Load the module environment suitable for the job\nconda activate my_env\n\n# And finally run the job\u200b\nsrun ./test1.sh \nsrun ./test2.sh\n</code></pre> <p>Then you can submit the job as follows:</p> <pre><code>sbatch &lt;my_job_script&gt;\n</code></pre> <p>This will send to the cluster a job with 8 cores and 16GB of RAM named my_job_name and with a time limit of 30mins.</p> <p>Use <code>qmap</code>!</p> <p>Do you need to run parallel jobs in the cluster? You can use an in-house package called <code>qmap</code>, that allows you to let SLURM decide where and which node to use to run the commands with basic template file. More info here</p> <p>Another method to submit more jobs to the cluster is to use SLURM array jobs. The setup of these jobs is virtually similar to a regular SLURM job, but with the difference that a `SLURM_JOB_</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=example_jobname\n#SBATCH --cpus-per-task=1\n#SBATCH --array=1-1425%30\n#SBATCH --time=01:00:00\n#SBATCH --mem=8gb\n#SBATCH --output output/outfile-%j.out\n\nsh example_job.sh ${SLURM_ARRAY_TASK_ID}\n</code></pre>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#manage","title":"MANAGE","text":""},{"location":"Cluster_basics/Submitting_jobs/SLURM/#1-squeue","title":"1. <code>squeue</code>","text":"<p>In order to see the queue status of the cluster and what is going on in the nodes, you can access the list  with the general command:</p> <pre><code>squeue\n</code></pre> <p>You can also specify some parameters:</p> <ul> <li><code>--user $USER</code> will show only your JOBS</li> <li><code>--job=$JOB_ID</code> will show only the JOBS listed with their JOB_Id</li> <li><code>--states=[RUNNING]</code> will show only RUNNING jobs.</li> </ul> <p>Full list of parameters can be seen with <code>squeue --help</code> command.</p>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#2-scancel","title":"2. <code>scancel</code>","text":"<pre><code>scancel [job_id]\n</code></pre> <p>You can specify some parameters:</p> <ul> <li><code>-A</code> will act only on jobs charging this account</li> <li><code>-state=PENDING</code> will act only on PENDING jobs</li> <li><code>-u</code> will act only on jobs of this user</li> </ul> <p>Full list of parameters can be seen with <code>scancel --help</code> command.</p> <p>What about canceling more than one job?</p> <p>SPOILER ALERT! You can! You can either list all the jobs one after the other.</p> <pre><code>scancel job_id1 job_id2 job_id3\n</code></pre> <p>If your jobs are sequencially you can use:</p> <pre><code>scancel {job_id1...job_id3}\n</code></pre> <p>In this case also job_id2 will be deleted</p>"},{"location":"Cluster_basics/Submitting_jobs/SLURM/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Datasets/Archive_data/","title":"How to Archive Data - BBGLab","text":"<p>Any folder in the cluster that we think should be archived we have to communicate it to Miguel.</p>"},{"location":"Datasets/Archive_data/#description","title":"Description","text":"<p>We have this google form to annotate any folder/dataset we want to archive. This way Miguel has a follow up table with all the listed data pending to archive.</p>"},{"location":"Datasets/Archive_data/#reference","title":"Reference","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9 Miguel Grau</p>"},{"location":"Datasets/Datasets_BBGLAB/","title":"Datasets - BBGLab","text":"<p>In the BBG Lab we have a google spreadsheet that contains all the information about the datasets we use.</p>"},{"location":"Datasets/Datasets_BBGLAB/#description","title":"Description","text":"<p>This spreadheet contains 4 sheets:  </p> <ul> <li>External: This table contains information about all the external datasets (related to sample sequencing data) used in the BBG Lab. It includes downloaded datasets (and the path to the cluster where you can find it) as well as datasets of which we have access through a cloud or external connection (e.g. Genomics England, UKbiobank).</li> <li>Internal: This table contains information about all the internal datasets sequenced at the BBG Lab (e.g. samples from patients from Sant Joan de D\u00e9u).  </li> <li>Form External: This table contains new external datasets added by any BBG user. You can add new external datasets using this form. New datasets will be added as new rows.  </li> <li>Form Internal: This table contains new internal datasets added by any BBG user. You can add new internal datasets using this form. New datasets will be added as new rows.</li> </ul> <p>The idea is to have a table with the minimum information necessary to identify the dataset, so the table is realistically maintainable. Each user can search for more detailed information just by going to the path where the data is located in the cluster, by looking at the original online repository or simply by asking the BBG user of the specific dataset.  </p> <p>This spreadsheet can only be modified by M\u00f2nica, Martina and Paula. If you know about a dataset that is not included in the spreadsheet, you can use the forms to add them. M\u00f2nica, Martina and Paula will review the new datasets and add them in the main tables.  </p> <p>To add new datasets, it is only mandatory to write the name of the dataset (as descriptive as possible) and the user e-mail. M\u00f2nica, Martina and Paula will try to find the rest of the information.</p>"},{"location":"Datasets/Datasets_BBGLAB/#reference","title":"Reference","text":"<ul> <li>M\u00f2nica S\u00e1nchez Gux\u00e9  </li> <li>Martina Gasull  </li> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/BBG_standards/Mappable_genome/","title":"Mappable Genome","text":""},{"location":"Datasets/BBG_standards/Mappable_genome/#description","title":"Description","text":"<p>The mappable genome is a fraction of the whole genome that excludes low-mappability regions. This mappable genome is usually applied in projects that need a highly reliable list of mutations.</p> <p>It was created by Loris Mularoni and later refined by Claudia Arnedo (both previous members of the BBG Lab).</p> <p>There are 3 main steps for generating the mappable genome:</p> <ol> <li>A first step is performed by Loris with a tool GEM Mapper from this paper from 2012. The file obtained can be found here: <code>/data/bbg/projects/pileup_mappability/hg38/hg38_100bp.coverage.regions.gz</code></li> <li>A second step is to filter blacklisted regions (aka problematic regions) from ENCODE project. This file is downloaded from ENCODE project web or UCSC database. Here  is explained what is this file and what it includes. From here we can download the most updated file.</li> <li>A third step is to filter common polymorphisms by using gnomAD. This step may be omitted depending on our project'sobjectives. Claudia's hotspots project needed a very clean mappable genome, so she had to avoid any possible germline contamination.</li> </ol> <p>The input file generated in step 1, the downloaded ENCODE blacklisted regions file (not the latest) and the gnomad 3.0 file are in Claudia's hotspots repo here: <code>/data/bbg/projects/hartwig/hotspots/hotspotfinder/2022_06/mappable_genome/data/inputs/</code>. In this folder there is a file <code>paths.txt</code> with the paths to the original files.</p> <p>These are the main cluster paths relevant for the making of the mappable genome:</p> <ul> <li><code>/data/bbg/projects/pileup_mappability/</code>: mappable genome with GEM mapper.</li> <li><code>/data/bbg/datasets/mappability_blacklists/</code>: blacklisted regions from ENCODE</li> <li><code>/data/bbg/datasets/genomes/</code>: reference genomes</li> <li><code>/data/bbg/projects/genomic_regions/</code>: genomic regions annotations</li> </ul> <p>M\u00f2nica has created a mappable genome with an updated blacklisted regions file and saved it here:\\ <code>/data/bbg/projects/pileup_mappability/hg38/hg38_100bp.coverage.regions.filtered_blacklisted.gz</code></p> <p>The updated blacklisted regions file is downloaded here:\\ <code>/data/bbg/datasets/mappability_blacklists/hg38/20240807/</code></p> <p>This plot is the comparison of the trinucleotide counts of the mappable genome (with updated ENCODE blacklisted regions and without filtering the SNPs from gnomAD) with the counts of the genome used in COSMIC (the one used to calculate the signatures). There are no major differences. </p>"},{"location":"Datasets/BBG_standards/Mappable_genome/#references","title":"References","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9 Olivia Dove</p>"},{"location":"Datasets/BBG_standards/Variant_consequences/","title":"Variant consequences","text":"<p>DISCLAIMER:  the decisions reflected in the instructions below might not be the best solution for all use cases, but are done with the best of my knowledge and with the best intention to make something useful and applicable across projects in the lab.</p>"},{"location":"Datasets/BBG_standards/Variant_consequences/#problem","title":"Problem","text":"<p>The problem is here, when having multiple annotations for a given variant, how to summarize that.</p> <pre><code>chr10:103590136_A&gt;T     chr10:103590136 T       ENSG00000107954 ENST00000369780 Transcript      missense_variant,splice_region_variant  2172    1489    497\nchr10:113850156_C&gt;G     chr10:113850156 G       ENSG00000288933 ENST00000692647 Transcript      intron_variant,non_coding_transcript_variant    -       -\nchr10:132808904_T&gt;C     chr10:132808904 C       ENSG00000171811 ENST00000368586 Transcript      splice_region_variant,synonymous_variant        7751    7665\n</code></pre> <p>We could also define a way of grouping consequences into: Nonsense, Missense, Splice affecting, Synonymous. Since some times we might not be interested in having very detailed consequence types but more broader categories.</p>"},{"location":"Datasets/BBG_standards/Variant_consequences/#description","title":"Description","text":"<p>This document contains information on how to handle this situation to get a single consequence per variant if this is the goal of the users in some project.</p> <p>The first section covers how to rank the variants according to the deleteriousness, and the second section covers how to group variants into broader consequence types.</p> <p>So far we have been using the following ranking of Sequence Ontology terms associated with consequence types of variants: https://www.ensembl.org/info/genome/variation/prediction/predicted_data.html For the grouping of variants we have been using ad-hoc collections on a case by case basis -- see e.g. boostDM. I agree that it would be good to establish once and for all a consensus grouping of SO consequence types for general variant analyses.</p>"},{"location":"Datasets/BBG_standards/Variant_consequences/#reducing-to-a-single-consequence-per-variant","title":"Reducing to a single consequence per variant","text":""},{"location":"Datasets/BBG_standards/Variant_consequences/#step-1-choose-one-consequence-per-transcript","title":"Step 1: Choose one consequence per transcript","text":"<p>In some cases you might get more than one consequence type, for the same variant in the same transcript.</p> <p>In these cases you want to preserve only the most deleterious one, but you might want to be careful with variants in non-coding transcripts.</p> <p>By applying <code>most_deleterious_within_variant()</code> function to the string of variant consequence you will get a single consequence per transcript.</p> <p>See the order of the consequences in the <code>CONSEQUENCES_LIST_WITHIN</code> list to understand which is the priority of consequence types when grouping.</p> <pre><code>CONSEQUENCES_LIST_WITHIN = [\n    'NMD_transcript_variant',\n    'non_coding_transcript_exon_variant',\n    'non_coding_transcript_variant',\n    'mature_miRNA_variant',\n\n    'transcript_ablation',\n    'splice_acceptor_variant',\n    'splice_donor_variant',\n    'stop_gained',\n    'frameshift_variant',\n    'stop_lost',\n    'start_lost',\n    'transcript_amplification',\n    'inframe_insertion',\n    'inframe_deletion',\n    'missense_variant',\n    'protein_altering_variant',\n    'splice_region_variant',\n    'splice_donor_5th_base_variant',\n    'splice_donor_region_variant',\n    'splice_polypyrimidine_tract_variant',\n    'incomplete_terminal_codon_variant',\n    'start_retained_variant',\n    'stop_retained_variant',\n    'synonymous_variant',\n    'coding_sequence_variant',\n    '5_prime_UTR_variant',\n    '3_prime_UTR_variant',\n    'intron_variant',\n    'upstream_gene_variant',\n    'downstream_gene_variant',\n    'TFBS_ablation',\n    'TFBS_amplification',\n    'TF_binding_site_variant',\n    'regulatory_region_ablation',\n    'regulatory_region_amplification',\n    'feature_elongation',\n    'regulatory_region_variant',\n    'feature_truncation',\n    'intergenic_variant',\n    '-'\n]\n\nconsequence_rank_dict_within = {consequence : rank for rank, consequence in enumerate(CONSEQUENCES_LIST_WITHIN)}\nrank_consequence_dict_within = {rank : consequence for rank, consequence in enumerate(CONSEQUENCES_LIST_WITHIN)}\n\n\ndef most_deleterious_within_variant(impact_vep_string):\n    \"\"\"\n    to be used when summarizing the different consquences assigned to a same variable in the same transcript\n    here we change for example the relevance of NMD_transcript_variant, since we do not want it to make it very damaging\n    \"\"\"\n    # TODO: revise if we need to have a try and except or it is better to make sure that the consequence\n    # dictionary and ranks correspond to the correct ensembl version?\n    try :\n        all_consequences = impact_vep_string.split(\",\")\n        all_consequences_ranks = map(lambda x: consequence_rank_dict_within[x], all_consequences)\n        return rank_consequence_dict_within[min(all_consequences_ranks)]\n    except:\n        return '-'\n</code></pre>"},{"location":"Datasets/BBG_standards/Variant_consequences/#step-2-from-a-single-consequence-per-variant-per-transcript-to-a-single-consequence-per-variant","title":"Step 2: From a single consequence per variant per transcript, to a single consequence per variant","text":"<p>Consequence list taken from: https://www.ensembl.org/info/genome/variation/prediction/predicted_data.html </p> <p>List of consequences updated on the 2023-06-22. Ensembl 109</p> <pre><code>CONSEQUENCES_LIST = [\n    'transcript_ablation',\n    'splice_acceptor_variant',\n    'splice_donor_variant',\n    'stop_gained',\n    'frameshift_variant',\n    'stop_lost',\n    'start_lost',\n    'transcript_amplification',\n    'inframe_insertion',\n    'inframe_deletion',\n    'missense_variant',\n    'protein_altering_variant',\n    'splice_region_variant',\n    'splice_donor_5th_base_variant',\n    'splice_donor_region_variant',\n    'splice_polypyrimidine_tract_variant',\n    'incomplete_terminal_codon_variant',\n    'start_retained_variant',\n    'stop_retained_variant',\n    'synonymous_variant',\n    'coding_sequence_variant',\n    'mature_miRNA_variant',\n    '5_prime_UTR_variant',\n    '3_prime_UTR_variant',\n    'non_coding_transcript_exon_variant',\n    'intron_variant',\n    'NMD_transcript_variant',\n    'non_coding_transcript_variant',\n    'upstream_gene_variant',\n    'downstream_gene_variant',\n    'TFBS_ablation',\n    'TFBS_amplification',\n    'TF_binding_site_variant',\n    'regulatory_region_ablation',\n    'regulatory_region_amplification',\n    'feature_elongation',\n    'regulatory_region_variant',\n    'feature_truncation',\n    'intergenic_variant'\n]\n\nconsequence_rank_dict = { consequence : rank for rank, consequence in enumerate(CONSEQUENCES_LIST) }\nrank_consequence_dict = { rank : consequence for rank, consequence in enumerate(CONSEQUENCES_LIST) }\nconsequence_rank_dict\n\n\nlist_of_annotations # this is the column of annotation\nlist_of_single_annotations = []\nfor x in list_of_annotations:\n    all_consequences = x.split(\",\")\n    all_consequences_ranks = map(lambda x: consequence_rank_dict[x], all_consequences)\n    list_of_single_annotations.append(rank_consequence_dict[min(all_consequences_ranks)])\n</code></pre> <p><code>list_of_single_annotations</code> will be a list with a single annotation per row that can be added as a new column in the dataframe that you will have.</p>"},{"location":"Datasets/BBG_standards/Variant_consequences/#broader-categories-of-variants","title":"Broader categories of variants","text":"<p>Going from the previous list into a more broader terms.</p> <pre><code>GROUPING_DICT = {\n    'transcript_ablation': 'nonsense',\n    'splice_acceptor_variant': 'nonsense',\n    'splice_donor_variant': 'nonsense',\n    'stop_gained': 'nonsense',\n    'frameshift_variant': 'nonsense',\n    'stop_lost': 'nonsense',\n    'start_lost': 'nonsense',\n\n    'missense_variant': 'missense',\n    'inframe_insertion': 'missense',\n    'inframe_deletion': 'missense',\n\n    'splice_donor_variant': 'essential_splice',\n    'splice_acceptor_variant': 'essential_splice',\n    'splice_donor_5th_base_variant': 'essential_splice',\n\n\n    'splice_region_variant': 'splice_region_variant',\n    'splice_donor_region_variant': 'splice_donor_region_variant',\n    'splice_polypyrimidine_tract_variant': 'splice_polypyrimidine_tract_variant',\n\n    'synonymous_variant': 'synonymous',\n    'incomplete_terminal_codon_variant': 'synonymous',\n    'start_retained_variant': 'synonymous',\n    'stop_retained_variant': 'synonymous',\n\n    'protein_altering_variant' : 'protein_altering_variant', ##\n    'transcript_amplification' : 'transcript_amplification', ##\n    'coding_sequence_variant': 'coding_sequence_variant', ##\n\n\n    '5_prime_UTR_variant': 'non_coding_exon_region',\n    '3_prime_UTR_variant': 'non_coding_exon_region',\n    'non_coding_transcript_exon_variant': 'non_coding_exon_region',\n\n    'NMD_transcript_variant': 'non_coding_exon_region',\n\n    'intron_variant': 'intron_variant',\n\n    'non_coding_transcript_variant' : 'non_coding_transcript_variant',\n    'mature_miRNA_variant': 'non_coding_transcript_variant', # TODO fix this\n\n    'upstream_gene_variant': 'non_genic_variant',\n    'downstream_gene_variant': 'non_genic_variant',\n    'TFBS_ablation': 'non_genic_variant',\n    'TFBS_amplification': 'non_genic_variant',\n    'TF_binding_site_variant': 'non_genic_variant',\n    'regulatory_region_ablation': 'non_genic_variant',\n    'regulatory_region_amplification': 'non_genic_variant',\n    'feature_elongation': 'non_genic_variant',\n    'regulatory_region_variant': 'non_genic_variant',\n    'feature_truncation': 'non_genic_variant',\n    'intergenic_variant': 'non_genic_variant',\n    '-'  : '-'\n\n}\n\n\nPROTEIN_AFFECTING_DICT = {\n\n    'nonsense' : 'protein_affecting',\n    'missense' : 'protein_affecting',\n    'essential_splice' : 'protein_affecting',\n    'splice_region' : 'ambiguous',\n    'synonymous' : 'non_protein_affecting',\n\n    'protein_altering_variant' : 'protein_affecting',\n    'transcript_amplification' : 'protein_affecting',\n    'coding_sequence_variant' : 'ambiguous',\n\n    'splice_region_variant': 'ambiguous',\n    'splice_donor_region_variant': 'ambiguous',\n    'splice_polypyrimidine_tract_variant': 'ambiguous',\n\n    'non_coding_exon_region' : 'non_protein_affecting',\n    'intron_variant' : 'non_protein_affecting',\n    'non_coding_transcript_variant' : 'non_protein_affecting',\n    'non_genic_variant' : 'non_protein_affecting',\n    '-'  : '-',\n}\n\n\nconsequence_rank_dict = { consequence : rank for rank, consequence in enumerate(CONSEQUENCES_LIST) }\nrank_consequence_dict = { rank : consequence for rank, consequence in enumerate(CONSEQUENCES_LIST) }\nconsequence_rank_dict\n\n\nlist_of_annotations # this is the column of annotation\nlist_of_single_annotations = []\nlist_of_broad_annotations = []\nlist_of_protein_affecting_annotations = []\nfor x in list_of_annotations:\n    all_consequences = x.split(\",\")\n    all_consequences_ranks = map(lambda x: consequence_rank_dict[x], all_consequences)\n    single_consequence = rank_consequence_dict[min(all_consequences_ranks)]\n    list_of_single_annotations.append(single_consequence)\n    list_of_broad_annotations.append(GROUPING_DICT[single_consequence])\n    list_of_protein_affecting_annotations.append(PROTEIN_AFFECTING_DICT[single_consequence])\n</code></pre> <ul> <li><code>list_of_single_annotations</code> will be a list with a single annotation per row that can be added as a new column in the dataframe that you will have.</li> <li><code>list_of_broad_annotations</code> will be a list with a single broad annotation per row that can be added as a new column in the dataframe that you will have.</li> </ul>"},{"location":"Datasets/BBG_standards/Variant_consequences/#reference","title":"Reference","text":"<p>For the list of variant consequences: http://www.ensembl.org/info/genome/variation/prediction/predicted_data.html</p> <p>For the dictionary, we should agree on the definitions, but this was done by me on the 2023-06-22.</p> <ul> <li>Ferriol Calvet</li> </ul>"},{"location":"Datasets/General_datasets/BeatAML/","title":"BeatAML","text":""},{"location":"Datasets/General_datasets/BeatAML/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/BeatAML/#reference","title":"Reference","text":""},{"location":"Datasets/General_datasets/CGCI/","title":"CGCI","text":""},{"location":"Datasets/General_datasets/CGCI/#description","title":"Description","text":"<p>These are WGS datasets from The Cancer Genome Characterization Initiative (CGCI) from NIH. Webpage: https://www.cancer.gov/ccg/research/genome-sequencing/cgci/</p> <p>These datasets are:</p> <ul> <li>Burkitt Lymphoma Genome Sequencing Project (BLGSP)</li> <li>HIV+ Tumor Molecular Characterization Project: Cervical Cancer</li> </ul> <p>They are downloaded from Genomics Data Commons (GDC) and stored at:</p> <pre><code>/data/bbg/datasets/intogen/input/data/genomes/gdc/cgci/\n</code></pre> <p>They are included in intOGen 2023 and 2024: <code>CGCI_WGS_BL_2020</code> <code>CGCI_WGS_CESC_2020</code></p> <p>We have granted access to these datasets (phs000235, data agreement code: 26799 )</p>"},{"location":"Datasets/General_datasets/CGCI/#reference","title":"Reference","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9 Paula Gomis</p>"},{"location":"Datasets/General_datasets/CPTAC/","title":"CPTAC","text":""},{"location":"Datasets/General_datasets/CPTAC/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/CPTAC/#data-access","title":"Data access","text":"<p>Website</p>"},{"location":"Datasets/General_datasets/CPTAC/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":""},{"location":"Datasets/General_datasets/CPTAC/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/GENIE/","title":"GENIE","text":""},{"location":"Datasets/General_datasets/GENIE/#description","title":"Description","text":""},{"location":"Datasets/General_datasets/GENIE/#reference","title":"Reference","text":""},{"location":"Datasets/General_datasets/Hartwig/","title":"Hartwig","text":""},{"location":"Datasets/General_datasets/Hartwig/#description","title":"Description","text":"<p>The Hartwig Medical Foundation (HMF) is aiming to make whole genome sequencing WGS) the future standard of care in the Netherlands ( https://www.hartwigmedicalfoundation.nl/en/). The HMF maintains and develops a growing database with &gt;5,000 metastatic cancer patients with rich clincial and genomic data (https://www.hartwigmedicalfoundation.nl/en/data/database/) - the largest such database in the entire world. The only way to truly to understand the HMF data is to look through the very detailed provided github repos. Most of the bioinformatics pipeline in HMF is made with their own in-house tools.</p>"},{"location":"Datasets/General_datasets/Hartwig/#data-access","title":"Data access","text":"<p>To use data from Hartwig Medical Foundation Database you need special permission. Contact Martina or Paula if you need to use this data.</p> <p>Once you obtains permissions, you can find the data from Hartwig Medical Foundation Database on bbgcluster here:</p> <pre><code>/data/bbg/datasets/hartwig\n</code></pre>"},{"location":"Datasets/General_datasets/Hartwig/#processed-data","title":"Processed data","text":"<p>In our lab several projects have used HMF data. From these projects the HMF data was pre-processed in away that could be re-used for other projects.  </p> <p>For one such project (immunobiomarkers) a pipeline to extract and format HMF biomarkers was created (https://bitbucket.org/bbglab/hartwig_biomarkers/src/master/). This pipeline creates pre-processed data files of different data types. The HMF processed clinical data is output on our cluster here:</p> <pre><code>/data/bbg/datasets/hartwig/20220809/biomarkers/tmp/clinical_ready.csv\n</code></pre> <p>The full output from the biomarkers pipline collects more than 60,000 curated columns of biomarkers into a table ready for analysis (located in folder below):</p> <pre><code>/data/bbg/datasets/hartwig/20220809/biomarkers/clean/biomarkers_AdjTPM.csv\n</code></pre>"},{"location":"Datasets/General_datasets/Hartwig/#exhaustive-study","title":"Exhaustive Study","text":"<p>For the immunobiomarkers project an exhaustive study of biomarkers was run. The exhaustive analysis is found within this repo - https://bitbucket.org/bbglab/immune_biomarkers/src/main/ - and the code could be re-purposed in the future to study other classes of treatmetns (e.g. Chemo, Targeted, Hormonal therapies).</p>"},{"location":"Datasets/General_datasets/Hartwig/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Researchers making use of data provided by Hartwig Medical Foundation must acknowledge this in every publication, by using at least the text below:</p> <p>This publication and the underlying research are partly facilitated by Hartwig Medical Foundation and the Center for Personalized Cancer Treatment (CPCT) which have generated, analysed and made available data for this research.</p> <p>Read this document  to learn more about how to cite Hartwig Medical Foundation Database.</p>"},{"location":"Datasets/General_datasets/Hartwig/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> <li>Joseph Usset</li> </ul>"},{"location":"Datasets/General_datasets/ICGC/","title":"ICGC","text":""},{"location":"Datasets/General_datasets/ICGC/#description","title":"Description","text":"<p>The International Cancer Genome Consortium (ICGC) is a global initiative to build a comprehensive catalog of mutational abnormalities in the major tumor types. ICGC\u2019s Data Portal is a user-friendly platform for efficient visualization, analysis and interpretation of large, diverse cancer datasets.</p> <p>The portal currently contains data from 86 worldwide cancer projects, collectively representing about 77 million somatic mutations and molecular data from over 20,000 contributors.</p> <p>ICGC Data portal also inculde cases from PCWAG and TCGA cohorts.</p> <p>decomissioning</p> <p>The ICGC Data Portal officially closed in June, 2024</p> <p>While the interactive web portal has been decommissioned, the most recent release and PCAWG data remain available for authorized users. For information on accessing ICGC 25K data, see the documentation for ICGC 25K Data Access.</p> <p>If you have any questions please contact the ICGC ARGO Helpdesk</p>"},{"location":"Datasets/General_datasets/ICGC/#new-features","title":"New features","text":"<ul> <li>Data Release 28 was the last data release of the ICGC-25K Data Portal. The ICGC Data Portal is no longer accepting data submissions.</li> <li>We are excited to announce the launch of the ICGC ARGO Data Platform, a major milestone following the ICGC 25K Data Portal. ARGO represents an international effort to advance cancer genomics through high-quality clinical and molecular data for international researchers.</li> </ul>"},{"location":"Datasets/General_datasets/ICGC/#data-access","title":"Data access","text":"<p>You can find the last data-version (2021) from ICGC in the folder:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/datasets/icgc/20211021/\n</code></pre>"},{"location":"Datasets/General_datasets/ICGC/#download-data-files","title":"Download data files","text":"<p>ICGC data files (i.e.  bam files) are stored in different repositories depending on the ICGC project they belong. Such repositories have their own architecture, data access controls, data portals and download clients.</p> <ul> <li>Cancer Genome Collaboratory</li> <li>GDC - Chicago</li> <li>PDC - Chicago</li> <li>EGA - Hinxton</li> <li>Azure - Toronto</li> <li>AWS - Virginia</li> </ul> <p>To download controlled data, a user must apply for access at the corresponding data access control body.</p> <ul> <li>US based projects are authorized by dbGaP.</li> <li>non-US projects are authorized by the ICGC Data Access Compliance Office (DACO).</li> </ul> <p>More information in ICGC DDC Docs</p>"},{"location":"Datasets/General_datasets/ICGC/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>To cite the ICGC 25K Data Portal, please cite this publication:</p> <ul> <li>Zhang J, Bajari R, Andric D, et al. The International Cancer Genome Consortium Data Portal. Nat Biotechnol. 2019;37(4):367\u2010369. doi:10.1038/s41587-019-0055-9</li> </ul>"},{"location":"Datasets/General_datasets/ICGC/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> <li>Joan Enric</li> </ul>"},{"location":"Datasets/General_datasets/Oncotree/","title":"Oncotree Repository","text":""},{"location":"Datasets/General_datasets/Oncotree/#description","title":"Description","text":"<p>Different versions of the Oncotree have been used and modified according to specific projects, resulting in inconsistencies when these projects or tools needed to interact with each other. To address this issue, the BBGLab has created a repository to unify the Oncotree versions.</p> <p>This repository contains all the different Oncotrees used by the BBGLab, with the latest version as the main branch.</p>"},{"location":"Datasets/General_datasets/Oncotree/#repository-structure","title":"Repository Structure","text":"<p>Oncotree repository: https://github.com/bbglab/oncotree</p> <p>Structure:</p> <ul> <li><code>mappings/</code>: Contains the different mappings (JSON files) between various Oncotree versions. For example, you will find the mapping between the Oncotree used by the CGI2021 version and the Intogen2023 version.</li> <li><code>scripts/</code>: Contains the different scripts used to generate the Oncotree, as well as the various mappings.</li> <li><code>oncotree.tsv</code>: Latest version of the Oncotree.</li> <li><code>tree_cancer_types.json</code>: Latest version of the Oncotree in JSON format.</li> <li><code>definitions.json</code>: Contains the dictionary that relates tumor type IDs to their corresponding full names.</li> </ul>"},{"location":"Datasets/General_datasets/Oncotree/#different-oncotree-versions","title":"Different Oncotree Versions","text":"<p>Ideally, you should use the latest version of the Oncotree stored in the <code>oncotree.tsv</code> file. However, you can still access different versions of the Oncotree by changing the repository tag.</p> <p>These tags are named after the tools that used the specific Oncotree versions. For example, to access the Oncotree version used by the CGI2021 tool, change the tag to <code>cgi2021</code>.</p>"},{"location":"Datasets/General_datasets/Oncotree/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"Datasets/General_datasets/PCAWG/","title":"PCAWG","text":""},{"location":"Datasets/General_datasets/PCAWG/#description","title":"Description","text":"<p>The Pan-Cancer Analysis of Whole Genomes (PCAWG) study is an international collaboration to identify common patterns of mutation in more than 2,600 cancer whole genomes from the International Cancer Genome Consortium.</p> <p>Building upon previous work which examined cancer coding regions (Cancer Genome Atlas Research Network, The Cancer Genome Atlas Pan-Cancer analysis project), this project explored the nature and consequences of somatic and germline variations in both coding and non-coding regions, with specific emphasis on cis-regulatory sites, non-coding RNAs, and large-scale structural alterations.</p>"},{"location":"Datasets/General_datasets/PCAWG/#data-access","title":"Data access","text":"<p>You can find the data from PCAWG in the folder:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/pcawg_20160110/filtered\n</code></pre> <p>You can find clinical data from samples from PCAWG in the file:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/pcawg_20160110/original_data/pcawg_donor_clinical_August2016_v9.csv\n</code></pre> <p>You can find clinical data from samples from Hartwig in the file:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/pcawg_20160110/original_data/pcawg_specimen_histology_August2016_v9.csv\n</code></pre> <p>Additional data about ... from PCAWG is available in the folder (you will need special permission to use this data):</p> <pre><code>/data/bbg/datasets/pcawg\n</code></pre> <p>Website</p> <p>https://dcc.icgc.org/pcawg</p>"},{"location":"Datasets/General_datasets/PCAWG/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>When using this dataset, please cite reference: The ICGC/TCGA Pan-Cancer Analysis of Whole Genomes Network. Pan-cancer analysis of whole genomes. Nature (2020). </p> <p>Click here to learn more about how to cite PCAWG.</p>"},{"location":"Datasets/General_datasets/PCAWG/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/PedcBioPortal/","title":"PedcBioPortal","text":""},{"location":"Datasets/General_datasets/PedcBioPortal/#description","title":"Description","text":"<p>The PedcBioPortal for Childhood Cancer Genomics is an instance of cBioPortal supporting the curation and pan-cancer integration of public, pediatric cancer genomics data sets as well as 'open science' initiatives integrated within the Kids First Data Resource Center as well as data from consortia-based efforts including the Children's Brain Tumor Tissue Consortium (CBTTC), the Pediatric NeuroOncology Consortium (PNOC), the St. Baldrick Pediatric Stand Up 2 Cancer Dream Team, and the Pediatric Preclinical Testing Consortium (PPTC).</p>"},{"location":"Datasets/General_datasets/PedcBioPortal/#data-access","title":"Data access","text":"<p>It is necessary to log in with a google account.  </p> <p>Website</p> <p>https://pedcbioportal.kidsfirstdrc.org/datasets/</p>"},{"location":"Datasets/General_datasets/PedcBioPortal/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>(This text is extracted from the FAQs page of cBioPortal)  </p> <p>Please cite the following portal papers:</p> <ul> <li>Cerami et al. The cBio Cancer Genomics Portal: An Open Platform for Exploring Multidimensional Cancer Genomics Data. Cancer Discovery. May 2012 2; 401. PubMed.  </li> <li>Gao et al. Integrative analysis of complex cancer genomics and clinical profiles using the cBioPortal. Sci. Signal. 6, pl1 (2013). PubMed.  </li> <li>de Bruijn et al. Analysis and Visualization of Longitudinal Genomic and Clinical Data from the AACR Project GENIE Biopharma Collaborative in cBioPortal. Cancer Res (2023). PubMed.  </li> </ul> <p>Remember also to cite the source of the data if you are using a publicly available dataset.  </p>"},{"location":"Datasets/General_datasets/PedcBioPortal/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/StJude/","title":"St. Jude","text":""},{"location":"Datasets/General_datasets/StJude/#description","title":"Description","text":"<p>In 2010, St. Jude Children\u2019s Research Hospital and Washington University School of Medicine launched a $65 million, three-year project (St. Jude\u2014Washington University Pediatric Cancer Genome Project) to define the genomic landscape of pediatric cancer, including some of the least understood and most challenging cancers, as an effort to discover the origins of pediatric cancer and seek new treatments. It included whole exome and whole transcriptome sequencing of an additional 1,200 patients, which included more than 20 different cancers.</p> <p>Click here to visualize the list of tumor types available in St. Jude Cloud.</p>"},{"location":"Datasets/General_datasets/StJude/#data-access","title":"Data access","text":"<p>You can find the data from St. Jude Cloud in the folder:</p> <pre><code>/data/bbg/datasets/stjude\n</code></pre> <p>Permission</p> <p>To use data from St. Jude  you need special permission. Contact Martina or Paula if you need to use this data.</p> <p>Website</p> <p>https://www.stjude.cloud/studies/pediatric-cancer-genome-project/ </p>"},{"location":"Datasets/General_datasets/StJude/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Click here to learn how to cite St. Jude data.</p>"},{"location":"Datasets/General_datasets/StJude/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/StJudeLife/","title":"St. Jude LIFE","text":""},{"location":"Datasets/General_datasets/StJudeLife/#description","title":"Description","text":"<p>The objective of the St. Jude LIFE study is to establish a lifetime cohort of childhood cancer survivors to facilitate longitudinal clinical evaluation of health outcomes in aging adults surviving pediatric cancer.</p> <p>The main aims of the project are:</p> <ul> <li>determine prevalence and latency of late effects;</li> <li>identify multifactorial predictors of adverse outcomes;</li> <li>develop risk profiles for adverse health outcomes across the age spectrum;</li> <li>use data to guide health screening and risk-reducing interventions.</li> </ul> <p>Click here to visualize a complete report of the characteristics of the cohort from St. Jude LIFE.</p>"},{"location":"Datasets/General_datasets/StJudeLife/#data-access","title":"Data access","text":"<p>You can find the data from St. Jude LIFE in the folder:</p> <pre><code>/data/bbg/datasets/stjudelife\n</code></pre> <p>Website</p> <p>https://sjlife.stjude.org/</p>"},{"location":"Datasets/General_datasets/StJudeLife/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":""},{"location":"Datasets/General_datasets/StJudeLife/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/TARGET/","title":"TARGET","text":""},{"location":"Datasets/General_datasets/TARGET/#description","title":"Description","text":"<p>The Therapeutically Applicable Research to Generate Effective Treatments (TARGET) program applies a comprehensive genomic approach to determine molecular changes that drive childhood cancers. The goal of the program is to use data to guide the development of effective, less toxic therapies.</p>"},{"location":"Datasets/General_datasets/TARGET/#data-access","title":"Data access","text":"<p>You can find TARGET pediatric cancer data in the folder:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/datasets/target/20190228/original_data/\n/data/bbg/datasets/intogen_datasets/genomes/datasets/gdc/target/20200917/\n</code></pre> <p>Website of the project</p> <p>ocg.cancer.gov/programs/target</p> <p>Website of the data (processed on GDC)</p> <p>portal.gdc.cancer.gov/projects/TARGET/ </p>"},{"location":"Datasets/General_datasets/TARGET/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Researchers making use of TARGET pediatric cancer data must acknowledge this in every publication, by using at least the text below:</p> <p>\"The results published here are in whole or part based upon data generated by the Therapeutically Applicable Research to Generate Effective Treatments (https://ocg.cancer.gov/programs/target) initiative, phs000218. The data used for this analysis are available at https://portal.gdc.cancer.gov/projects. Information about TARGET can be found at http://ocg.cancer.gov/programs/target.\"</p> <p>Click here to learn more about how to cite TARGET pediatric cancer data.</p>"},{"location":"Datasets/General_datasets/TARGET/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>M\u00f2nica S\u00e1nchez Guix\u00e9</li> </ul>"},{"location":"Datasets/General_datasets/TCGA/","title":"TCGA","text":""},{"location":"Datasets/General_datasets/TCGA/#description","title":"Description","text":"<p>The Cancer Genome Atlas (TCGA), a landmark cancer genomics program, molecularly characterized over 20,000 primary cancer and matched normal samples spanning 33 cancer types. This joint effort between NCI and the National Human Genome Research Institute began in 2006, bringing together researchers from diverse disciplines and multiple institutions.</p> <p>TCGA has generated over 2.5 petabytes of genomic, epigenomic, transcriptomic, and proteomic data. The data, which has already led to improvements in our ability to diagnose, treat, and prevent cancer, will remain publicly available for anyone in the research community to use.</p> <p>Click here  to visualize the list of tumor types available in TCGA. For each cancer type, TCGA published an overview of the characterizations performed and an initial analysis of the data.  </p>"},{"location":"Datasets/General_datasets/TCGA/#data-access","title":"Data access","text":"<p>You can find the data from TCGA in the folder:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/tcga_20171006/filtered\n</code></pre> <p>You can find clinical data from samples from TCGA in the folder:</p> <pre><code>/data/bbg/datasets/intogen_datasets/genomes/tcga_20171006/metadata\n</code></pre> <p>Website</p> <p>https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga</p>"},{"location":"Datasets/General_datasets/TCGA/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>Click here  to learn how to cite TCGA.</p>"},{"location":"Datasets/General_datasets/TCGA/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/General_datasets/UK_Biobank/","title":"UK Biobank","text":""},{"location":"Datasets/General_datasets/UK_Biobank/#description","title":"Description","text":"<p>UK Biobank (UKB) is a large-scale biomedical database and research resource, containing in-depth genetic and health information from half a million UK participants. It is a very large and detailed prospective study with over 500,000 participants aged 40\u201369 years when recruited in 2006\u20132010.</p> <p>The study has collected and continues to collect extensive phenotypic and genotypic detail about its participants, including data from questionnaires, physical measures, sample assays, accelerometry, multimodal imaging, genome-wide genotyping and longitudinal follow-up for a wide range of health-related outcomes. It is accessible to approved researchers undertaking vital research into the most common and life-threatening diseases.</p> <p>Website</p> <p>https://www.ukbiobank.ac.uk/</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#genomic-data","title":"Genomic data","text":"<p>UKB includes Whole Exome Sequencing data obtained from blood samples from nearly the whole cohort (about 470,000 individuals). Moreover, it also has Whole Genome Sequencing data obtained also from blood from about 200,000 individuals, and, by the end of 2023, it is expected to be extended to the whole cohort (500,000 individuals). More info here.</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#data-access","title":"Data access","text":"<p>Access to UKB data is restricted to authorized users that have to submit a research proposal to UKB. As BBGLab we can use the data on the study of the genetic basis of clonal hematopoiesis. Only some people in the lab have access to the data, you can ask Martina or Paula for more information.</p> <p>Originally, the data was downloadable, so we have in our cluster a lot of clinical data from the entire cohort and also the WES from 200,000 individuals in the folder:</p> <pre><code>/data/bbg/datasets/ukbiobank_ch/\n</code></pre> <p>More recently, the UK Biobank Research Analysis Platform (UKB RAP) was created, so the data can not be downloaded anymore. The UKB RAP is a platform in the cloud based on DNA Nexus. The WES data from the full cohort and the WGS data are only accessible through this platform.</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>All publications should include the acknowledgement: \u201cThis research has been conducted using data from UK Biobank, a major biomedical database\u201d and where appropriate, include a link to the UK Biobank website: . <p>More information here.</p>"},{"location":"Datasets/General_datasets/UK_Biobank/#reference","title":"Reference","text":"<ul> <li>Santi Demajo</li> <li>Joan Enric Ramis</li> <li>Miguel Grau</li> <li>Paula Gomis</li> </ul>"},{"location":"Datasets/General_datasets/cBioPortal/","title":"cBioPortal","text":""},{"location":"Datasets/General_datasets/cBioPortal/#description","title":"Description","text":"<p>The cBioPortal for Cancer Genomics is an open-access, open-source resource for interactive exploration of multidimensional cancer genomics data sets including molecular profiles and clinical attributes from large-scale cancer genomics projects.</p> <p>The portal supports and stores non-synonymous mutations, DNA copy-number data (putative, discrete values per gene, e.g. \"deeply deleted\" or \"amplified\", as well as log2 or linear copy number data), mRNA and microRNA expression data, protein-level and phosphoprotein level data (RPPA or mass spectrometry based), DNA methylation data, and de-identified clinical data. However, for many studies only somatic mutation data and limited clinical data are available. For TCGA studies, the other data types are also available. Germline mutations are supported by cBioPortal, but are, with a few exceptions, not available in the public instance.</p>"},{"location":"Datasets/General_datasets/cBioPortal/#data-access","title":"Data access","text":"<p>Click here to visualize the list of all the available datasets in cBioPortal and download the data.</p> <p>Website</p> <p>https://www.cbioportal.org/</p>"},{"location":"Datasets/General_datasets/cBioPortal/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>When using data from cBioPortal, please cite the following portal papers:</p> <ul> <li>Cerami et al. The cBio Cancer Genomics Portal: An Open Platform for Exploring Multidimensional Cancer Genomics Data. Cancer Discovery. May 2012 2; 401. PubMed.</li> <li>Gao et al. Integrative analysis of complex cancer genomics and clinical profiles using the cBioPortal. Sci. Signal. 6, pl1 (2013). PubMed.</li> </ul> <p>Remember to also cite the source of the data if you are using a publicly available dataset.</p> <p>Click here to learn more about how to cite cBioPortal.</p>"},{"location":"Datasets/General_datasets/cBioPortal/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Monica Sanchez</li> </ul>"},{"location":"Datasets/Inhouse_datasets/ALL_cohort/","title":"ALL cohort","text":""},{"location":"Datasets/Inhouse_datasets/ALL_cohort/#description","title":"Description","text":"<p>This dataset is from Ines Sent\u00eds preject (a former PhD student from the lab). These are Whole Genome Sequencings from 19 adult T-ALL patients, with primary and relapsed samples, from Hospital del Mar (collaboration with Anna Bigas).</p> <p>This data is archived.  </p> <p>Paper is already publsihed here: Sent\u00eds, I., Gonzalez, S., Genesc\u00e0, E. et al. The evolution of relapse of adult T cell acute lymphoblastic leukemia. Genome Biol 21, 284 (2020). https://doi.org/10.1186/s13059-020-02192-z</p>"},{"location":"Datasets/Inhouse_datasets/ALL_cohort/#reference","title":"Reference","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9 Miguel Grau</p>"},{"location":"Datasets/Inhouse_datasets/Damage_maps/","title":"Damage maps","text":""},{"location":"Datasets/Inhouse_datasets/Damage_maps/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/Damage_maps/#reference","title":"Reference","text":""},{"location":"Datasets/Inhouse_datasets/Nanopore_data/","title":"Nanopore data","text":""},{"location":"Datasets/Inhouse_datasets/Nanopore_data/#description","title":"Description","text":""},{"location":"Datasets/Inhouse_datasets/Nanopore_data/#reference","title":"Reference","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric%20Secondary%20neoplasms/","title":"Pediatric Secondary neoplasms","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric%20Secondary%20neoplasms/#description","title":"Description","text":"<p>This dataset is a collection of WGS of normal-matched tumors from 12 patients with secondary tumors. There are 2 tumor samples per case, except for one case with XPC germline mutation, with 3 tumoral samples (1 for the first tumor -melanoma- and 2 for the second tumor -undifferentiated pleomorfic sarcoma-).</p> <p>This is a collaboration with Jaume Mora and Cinzia Lavarino from Sant Joan de D\u00e9u Hospital.</p> <p>Original FASTQ files can be found at:</p> <pre><code>s3://bbg/datasets/tumor/sjd_seq/[date_of_download]/\n</code></pre> <p><code>platinum</code> results (Hartwig pipeline in google cloud), can be found at:</p> <pre><code>s3://bbg/datasets/tumor/sjd_seq/platinum_results/20210201/\ns3://bbg/datasets/tumor/sjd_seq/platinum_results/20220528/\n/data/bbg/datasafe/sjd_seq/platinum_results/20220809/\n/data/bbg/datasets/sjd_melos/sjd_patient/vcf/run_tfm/20240223_oncoanalyser_results/\n</code></pre> <p>The first third also include the bam files.</p> <p>The <code>20220809</code> run is the one used in the second tumors paper (DOI:10.1158/2159-8290.CD-23-1186). <code>sarek</code>results can be found at:</p> <pre><code>/data/bbg/datasets/tumor/sjd_seq/platinum_results/20220809/[sample_id]/sarek_results/\n/data/bbg/datasets/sjd_melos/sjd_patient/vcf/run_tfm/20240219_sarek_results/\n</code></pre> <p>The <code>20240223_</code> and <code>20240219_</code> runs are from the patient with XPC germline mutation and bam files can be found together with FASTQ.</p>"},{"location":"Datasets/Inhouse_datasets/Pediatric%20Secondary%20neoplasms/#reference","title":"Reference","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9 Bet Figuerola Bou</p>"},{"location":"Datasets/Inhouse_datasets/Pediatric_Rhabdoid_cohort/","title":"Pediatric Rhabdoid cohort","text":""},{"location":"Datasets/Inhouse_datasets/Pediatric_Rhabdoid_cohort/#description","title":"Description","text":"<p>This cohort is a collection of 27 normal-matched WGS from 18 patients from Sant Joan de D\u00e9u Hospital. This is a collaboration with Alexandra Avgustinova.</p> <p>Some patients have 2 tumor samples: pt1, pt3, pt4, pt5, pt6, pt9, pt14, pt18; the rest have 1 tumor sample. Some samples are of low/bad quality: pt5-t2, pt6-t1, pt9-t1, pt14-t2, pt16 Some samples have been re-sequenced (due to low quality): pt12-t1 Some samples are being re-sequenced (due to low quality): pt5-t2, pt6-t1, pt9-t1, pt14-t2  </p> <p>Original FASTQ files can be found at:</p> <pre><code>s3://bbg/datasets/tumor/rhabdoid_tumors/\n</code></pre> <p><code>sarek</code> results including bam files can be found at:</p> <pre><code>/data/bbg/datasets/rhabdoid_tumors/sarek_results/\n</code></pre> <p><code>oncoanalyser</code> results can be found at:</p> <pre><code>/data/bbg/datasets/rhabdoid_tumors/oncoanalyser_results/\n</code></pre>"},{"location":"Datasets/Inhouse_datasets/Pediatric_Rhabdoid_cohort/#reference","title":"Reference","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9</p>"},{"location":"Datasets/Inhouse_datasets/cgi_biomarkers_database/","title":"CGI Biomarkers database","text":""},{"location":"Datasets/Inhouse_datasets/cgi_biomarkers_database/#description","title":"Description","text":"<p>This database is used by Cancer Genome Interpreter (CGI) as a source to assess the relevance of the alterations as biomarkers of drug response.</p> <p>The biomarkers are classified by cancer type, drug response (sensitivity/resistance/toxicity) and the level of clinical evidence supporting this association.</p> <p>The database was created by several clinical and scientific experts in the field of precision oncology and it is currently curated and maintained by the CGI expertise team, through reviewing the guidelines (FDA-Approvals and NCCN Guidelines) on drug approvals.</p> <p>The CGI Biomarkers database was last updated in October 2022.</p>"},{"location":"Datasets/Inhouse_datasets/cgi_biomarkers_database/#data-access","title":"Data access","text":"<p>Click to access the dataset:</p> <p>Website</p> <p>https://www.cancergenomeinterpreter.org/biomarkers/ </p>"},{"location":"Datasets/Inhouse_datasets/cgi_biomarkers_database/#citing-in-publications-and-presentations","title":"Citing in Publications and Presentations","text":"<p>When referring to the datasets any result generated by the CGI framework or the CGI resource itself, please cite:</p> <p>Tamborero, D., Rubio-Perez, C., Deu-Pons, J. et al. Cancer Genome Interpreter annotates the biological and clinical relevance of tumor alterations. Genome Med 10, 25 (2018). https://doi.org/10.1186/s13073-018-0531-8</p>"},{"location":"Datasets/Inhouse_datasets/cgi_biomarkers_database/#more-information","title":"More information","text":"<p>If you want more information check: https://www.cancergenomeinterpreter.org/faq#q12</p>"},{"location":"Datasets/Inhouse_datasets/cgi_biomarkers_database/#reference","title":"Reference","text":"<ul> <li>Magda Guardiola Fern\u00e1ndez</li> </ul>"},{"location":"Datasets/Other_data/Canonical_transcripts/","title":"Canonical transcripts","text":""},{"location":"Datasets/Other_data/Canonical_transcripts/#description","title":"Description","text":""},{"location":"Datasets/Other_data/Canonical_transcripts/#reference","title":"Reference","text":""},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/","title":"Encode Epigenetic data","text":"<p>Cluster location: <code>/data/bbg/datasets/encode/</code></p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#description","title":"Description","text":"<p>Encode genomic data downloaded from https://www.encodeproject.org/</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#contents","title":"Contents","text":"<p>bigWig files of epigenetic tracks across the genome</p> <p>Reference genome: hg38</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#chromatin-files-801-files","title":"Chromatin files (801 files)","text":"<ul> <li>Are large-scale chromatin modifications such as:</li> <li>H3K27ac</li> <li>H3K9me2</li> </ul> <p>Full download link present in chromatin_files.txt Metadata: Chromatin_metadata.tsv</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#tfs-442-files","title":"TFs (442 files)","text":"<p>Although the name does not fully characterize its content this folder contains:</p> <ul> <li>CTCF-sites</li> <li>ATAC sequencing</li> <li>DNase-seq</li> </ul> <p>Metadata: TFs_metadata_2022_12_12.tsv</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#references","title":"References","text":"<p>Information for citing the most recent version of the Encode dataset: https://www.encodeproject.org/help/citing-encode/</p>"},{"location":"Datasets/Other_data/ENCODE_epigenetic_data/#author","title":"Author","text":"<p>Axel Rosendahl Huber - 16-03-2023</p>"},{"location":"Datasets/Other_data/Genomic_regions/","title":"Genomic regions","text":"<p>Genomic regions annotations generated by BBGLab Data includes:</p> <ul> <li><code>3utr</code></li> <li><code>introns</code></li> <li><code>mirna_mat</code></li> <li><code>tfbs</code></li> <li><code>5utr</code></li> <li><code>lncrna_distal_promoters</code></li> <li><code>mirna_pre</code></li> <li><code>utr</code></li> <li><code>cds</code></li> <li><code>lncrna_exons</code></li> <li><code>other_ncrnas</code></li> <li><code>distal_promoters</code></li> <li><code>lncrna_proximal_promoters</code></li> <li><code>proximal_promoters</code></li> <li><code>enhancer</code></li> <li><code>lncrna_splice_sites</code></li> <li><code>splice_sites</code></li> </ul>"},{"location":"Datasets/Other_data/Genomic_regions/#releases","title":"Releases","text":"<ul> <li> <p>Release 2 (30-09-2020): The coordinates are extracted from the <code>gtf3</code> annotation file.</p> <ul> <li>CDS coordinates are generated in two flavours: with and without (default) STOP codon.</li> <li>Gencode v35</li> <li>Ensembl canonical transcripts v101</li> </ul> </li> <li> <p>Release 1 (2019): The coordinates are extracted from the <code>gtf</code> annotation file.</p> <ul> <li>CDS coordinates are generated without STOP codon.</li> <li>Gencode v31</li> <li>Ensembl canonical transcripts v97</li> </ul> </li> </ul>"},{"location":"Datasets/Other_data/Genomic_regions/#description","title":"Description","text":"<p>You can find the data in the folder: <code>/data/bbg/projects/genomic_regions/</code></p> <ul> <li><code>./raw_data</code>: contains databases from which raw data has been downloaded</li> <li><code>./scripts</code>: contains code to parse raw data</li> <li><code>./hg19</code>: contains genomic annotations in hg19 reference genome</li> <li><code>./hg38</code>: contains genomic annotations in hg38 reference genome</li> </ul>"},{"location":"Datasets/Other_data/Genomic_regions/#reference","title":"Reference","text":"<ul> <li>Joan Enric</li> </ul> <p>Created on 2019-07-08 by claudia.arnedo@irbbarcelona.org</p>"},{"location":"Datasets/Other_data/MaveDB_mutation_scores/","title":"MaveDB mutation scores","text":""},{"location":"Datasets/Other_data/MaveDB_mutation_scores/#introduction","title":"Introduction","text":"<p>The MaveDB is a public repository for datasets from Multiplexed Assays of Variant Effect (MAVEs), such as those generated by deep mutational scanning (DMS) or massively parallel reporter assay (MPRA) experiments.</p>"},{"location":"Datasets/Other_data/MaveDB_mutation_scores/#how-to-fetch-mutation-scores-from-the-mavedb-database","title":"How to fetch mutation scores from the MaveDB database","text":"<p>We can retrieve mutation scores for a given set of input mutations using the command line from the ENSEMBL-VEP alongside the MaveDB plugin.</p> <p>Within an interactive session in the cluster, you can use the following command line as a template and modify it according to your specific needs:</p> <pre><code>singularity exec /data/bbg/datasets/vep/homo_sapiens/ensembl-vep_111.0.sif vep --dir /data/bbg/datasets/vep/ \\\n--tab -i input_mutations.tsv --offline --cache -o output_annotated_mutations.tsv \\\n--species homo_sapiens --assembly GRCh38 --fork 8 --canonical \\\n--plugin MaveDB,file=/data/bbg/datasets/vep/homo_sapiens/plugins/MaveDB_variants.tsv.gz\n</code></pre>"},{"location":"Datasets/Other_data/MaveDB_mutation_scores/#remarks","title":"Remarks","text":"<p><code>input_mutations.tsv</code> must be in an accepted format for VEP, a tab separated file with chromosome, start, end and variant like in the following example will work.</p> <pre><code>17  43045712    43045712    T/C\n</code></pre> <p><code>output_annotated_mutations.tsv</code> is the filename of the output.</p> <p><code>--canonical</code> is a VEP command line option, it simply adds a column pointing out whether the transcript is canonical.</p> <p>Bear in mind that some scores have several tracks of data and some postprocessing might be required after the query.</p>"},{"location":"Datasets/Other_data/MaveDB_mutation_scores/#reference","title":"Reference","text":"<p>Ferran Mui\u00f1os, Paula Gomis, Miguel Grau</p> <p>Updated 2024/07/01</p>"},{"location":"Datasets/Other_data/Nmdetective/","title":"Nmdetective","text":""},{"location":"Datasets/Other_data/Nmdetective/#description","title":"Description","text":"<p>These are NMD predictions per every site in the exome. It is taken from this work from Fran Supek: Lindeboom et al 2020 The impact of nonsense-mediated mRNA decay on genetic disease, gene editing and cancer immunotherapy doi: 10.1038/s41588-019-0517-5</p> <p>Data downloaded from doi:10.6084/m9.figshare.7803398</p> <p>Data can be found here:</p> <pre><code>/data/bbg/datasets/nmdetective\n</code></pre>"},{"location":"Datasets/Other_data/Nmdetective/#reference","title":"Reference","text":"<p>M\u00f2nica S\u00e1nchez Guix\u00e9</p>"},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/","title":"Reference Mutational Signatures","text":""},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/#cosmic-catalogue-of-somatic-mutations-in-cancer","title":"COSMIC: Catalogue of Somatic Mutations in Cancer","text":"<ul> <li>URL: cancer.sanger.ac.uk/signatures</li> <li>Extracted from tumors</li> <li>SBS, ID, DBS, CN and SV mutational processes</li> <li>Maintained by the Sanger Institute</li> </ul>"},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/#cosmic-highlights","title":"COSMIC - Highlights","text":"<ul> <li>COSMIC displays for each signature a battery of analyses showing about tissue prevalence, strand asymmetry, enrichment</li> <li>of histone modifications, nucleosome occupancy, CTCF occupancy, etc.</li> </ul>"},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/#where-is-this-data-stored-in-the-cluster","title":"Where is this data stored in the cluster?","text":"<p>A tab-separated table with the COSMIC v3.4 signatures is available at: <code>/data/bbg/datasets/COSMIC_signatures/COSMIC_v3.4_SBS_GRCh38.txt</code>.</p> <p>Key details about the data:</p> <ul> <li>Rows: Represent the 96 possible pyrimidine-centered trinucleotide contexts.</li> <li>Columns: Denote the respective reference signatures. Each column sums to one.</li> <li>Cell values: Indicate the frequencies of mutations matching that context relative to the total mutations observed   (i.e., the proportion of mutations matching that trinucleotide context).</li> </ul> <p>Important considerations:</p> <ul> <li>These frequencies are not equivalent to mutation rates. They were derived from whole genome samples where the   reference trinucleotides are not equally represented.</li> <li>If your study involves a different reference genome or a specific genomic region with a distinct trinucleotide   content (e.g., panel sequencing), you may need to rescale the relative frequencies of the signatures accordingly.</li> </ul>"},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/#signature-groups","title":"Signature groups","text":"<p>As you probably know for several of the signatures in the COSMIC reference catalog there is a known aetiology, and this allows us to define several signature groups.</p> <p>AlexandrovLab has defined these groups of COSMIC signatures. You can use them for your own analysis, but when running any of the tools they developed, it is very easy to filter out any of these groups in case you known that some of those exposures is not present in your samples.</p> <p>See below a copy of their original table that can be found here.</p> Signature subgroup SBS signatures excluded DBS signatures excluded ID signatures excluded MMR_deficiency_signatures 6, 14, 15, 20, 21, 26, 44 7, 10 7 POL_deficiency_signatures 10a, 10b, 10c, 10d, 28 3 - HR_deficiency_signatures 3 13 6 BER_deficiency_signatures 30, 36 - - Chemotherapy_signatures 11, 25, 31, 35, 86, 87, 90, 99 5 - Immunosuppressants_signatures 32 - - Treatment_signatures 11, 25, 31, 32, 35, 86, 87, 90, 99 5 - APOBEC_signatures 2, 13 - - Tobacco_signatures 4, 29, 92 2 3 UV_signatures 7a, 7b, 7c, 7d, 38 1 13 AA_signatures 22a, 22b 20 23 Colibactin_signatures 88 - 18 Artifact_signatures 27, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 95 14 - Lymphoid_signatures 9, 84, 85 - -"},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/#signal","title":"SIGNAL","text":"<ul> <li>URL: signal.mutationalsignatures.com</li> <li>Extracted from both tumors and experimental mutagenesis assays</li> <li>SBS, DBS and SV mutational processes</li> <li>Maintained by the University of Cambridge</li> </ul>"},{"location":"Datasets/Other_data/ReferenceMutationalSignatures/#signal-highlights","title":"SIGNAL - Highlights","text":"<ul> <li>SIGNAL curates organ-specific signatures, consisting on typical profiles that can be realised as weighted combinations of mutational signatures at different organs. For example, here is an example of which organ-specific profiles Signature 1 contributes to depending on the tissue.</li> </ul>"},{"location":"Datasets/Other_data/Reference_genomes/","title":"Reference genomes","text":""},{"location":"Datasets/Other_data/Reference_genomes/#description","title":"Description","text":"<p>The same reference genomes are used across many different projects. Although each user may prefer to have its own copy of the genome sequence, there is a <code>genomes</code> folder in datasets that can serve as the collective storage of reference genomes.</p> <p>You can find the full path to the directory here: <code>/data/bbg/datasets/genomes/</code></p> <p>The contents of this folder are the following:</p> <pre><code>GRCh37\nGRCh38\niGenomes\niGenomes_2022\npurple_resources\nREADME.txt\nSaccharomyces_cerevisiae\nSageGermlinePon.hg38.98x.vcf.gz\nSAGE_resources\n</code></pre> <p>GRCh37 -&gt; contains only the FASTA file of the genome assembly GRCh37</p> <p>GRCh38 -&gt; contains the genomic sequence of GRCh38 human genome assembly with several index files, as well as some annotation files.</p> <p>iGenomes -&gt; contains the illumina Homo sapiens genomes in different versions and from different origins (NCBI, GATK, UCSC)</p> <p>iGenomes_2022 -&gt; contains the illumina Homo sapiens genome for version GRCh38 and from NCBI</p> <p>purple_resources -&gt; contains two files seemingly related to a panel of normals.</p> <p>Saccharomyces_cerevisiae -&gt; contains information of the genome of this species and it was downloaded from UCSC iGenomes.</p> <p>SageGermlinePon... and SAGE resources are resources related to running SAGE.</p>"},{"location":"Datasets/Other_data/Reference_genomes/#recommendations","title":"Recommendations","text":"<p>It would be good to get used to use the reference files that are here avoiding the creation of multiple copies of the same files all along the cluster.</p>"},{"location":"Datasets/Other_data/Reference_genomes/#reference","title":"Reference","text":"<p>Ferriol Calvet</p> <p>2023/03/16</p>"},{"location":"Datasets/Other_data/Repli-Seq/","title":"Repli-Seq data","text":"<p>Cluster location: <code>/data/bbg/nobackup/repliseq</code></p>"},{"location":"Datasets/Other_data/Repli-Seq/#description","title":"Description","text":"<p>Repli-seq data downloaded from http://hgdownload.soe.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwRepliSeq/</p>"},{"location":"Datasets/Other_data/Repli-Seq/#contents","title":"Contents","text":"<p>bigWig files of areas of replication timing across the genome</p> <p>Reference genome: hg19</p>"},{"location":"Datasets/Other_data/Repli-Seq/#references","title":"References","text":"<p>See: http://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;g=wgEncodeUwRepliSeq</p> <p>Hansen RS, Thomas S, Sandstrom R, Canfield TK, Thurman RE, Weaver M, Dorschner MO, Gartler SM, Stamatoyannopoulos JA. Sequencing newly replicated DNA reveals widespread plasticity in human replication timing. Proc Natl Acad Sci U S A. 2010 Jan 5;107(1):139-44. https://doi.org/10.1073/pnas.0912402107</p> <p>Thurman RE, Day N, Noble WS, Stamatoyannopoulos JA. Identification of higher-order functional domains in the human ENCODE regions. Genome Res. 2007 Jun;17(6):917-27. https://doi.org/10.1101/gr.6081407</p>"},{"location":"Datasets/Other_data/Repli-Seq/#author","title":"Author","text":"<p>Axel Rosendahl Huber - 16-03-2023</p>"},{"location":"Datasets/Other_data/Trinucleotide_content/","title":"Nucleotide content of the genome","text":""},{"location":"Datasets/Other_data/Trinucleotide_content/#introduction","title":"Introduction","text":"<p>The nucleotide content of the genome refers to the frequency and distribution of n-nucleotide sequences across the DNA. The most common count is based on trinucleotides. In principle, there are 4\u00b3 = 64 possible trinucleotide combinations, given the four DNA bases (A, C, G, and T). However, because DNA is double-stranded and sequences are often analyzed without strand specificity, many genomic analyses collapse reverse-complement pairs. As a result, the 64 trinucleotides are reduced to 32 unique trinucleotide contexts, with each context representing both a sequence and its reverse complement.</p> <p>It is important to take into account the nucleotide content of the genome we are using when performing analysis that rely on the distribution of mutations across genomic positions, as the n-nucleotides are not equally represented.</p>"},{"location":"Datasets/Other_data/Trinucleotide_content/#where-is-this-data-stored-in-the-cluster","title":"Where is this data stored in the cluster?","text":"<p>This page aims to gather the nucleotide counts files stored in the cluster for several genomes and nucleotide combinations:</p> <ul> <li>Whole genome GRCh38 trinucleotide counts (32 channels)<ul> <li>Created by Alexandrov Lab for SigProfiler</li> <li>Stored at <code>/data/bbg/datasets/genomes/context_counts/sigprofiler/context_counts_GRCh38_96.csv</code> </li> <li>Rows represent trinucleotides, columns chromosomes and cells are filled with the absolute trinucleotide count   for the reference genome</li> </ul> </li> </ul>"},{"location":"Datasets/Other_data/dbNSFP_mutation_scores/","title":"dbNSFP mutation scores","text":""},{"location":"Datasets/Other_data/dbNSFP_mutation_scores/#introduction","title":"Introduction","text":"<p>The dbNSFP database curates a large collection of mutation scores that can reveal very useful for variant effect analysis, including benchmarking of new variant effect tools.</p>"},{"location":"Datasets/Other_data/dbNSFP_mutation_scores/#how-to-fetch-mutation-scores-from-the-dbnsfp-database","title":"How to fetch mutation scores from the dbNSFP database","text":"<p>We can retrieve mutation scores for a given set of input mutations using the command line from the ENSEMBL-VEP alongside the dbNSFP plugin.</p> <p>Within an interactive session in the cluster, you can use the following command line as a template and modify it according to your specific needs:</p> <pre><code>singularity exec /data/bbg/datasets/vep/homo_sapiens/ensembl-vep_111.0.sif vep --dir /data/bbg/datasets/vep/ \\\n--tab -i input_mutations.tsv --offline --cache -o output_annotated_mutations.tsv \\\n--species homo_sapiens --assembly GRCh38 --fork 8 --canonical \\\n--plugin dbNSFP,/data/bbg/datasets/vep/homo_sapiens/plugins/dbNSFP4.5a_grch38.gz,\\\nSIFT_score,SIFT4G_score,Polyphen2_HDIV_score,Polyphen2_HVAR_score,MutationAssessor_score,FATHMM_score,MetaLR_score,MetaRNN_score,CADD_raw,VEST4_score,PROVEAN_score,REVEL_score,ESM1b_score,EVE_score,AlphaMissense_score,phyloP100way_vertebrate,phyloP470way_mammalian,phyloP17way_primate\n</code></pre>"},{"location":"Datasets/Other_data/dbNSFP_mutation_scores/#remarks","title":"Remarks","text":"<p><code>input_mutations.tsv</code> must be in an accepted format for VEP, a tab separated file with chromosome, start, end and variant like in the following example will work.</p> <pre><code>17  43045712    43045712    T/C\n</code></pre> <p><code>output_annotated_mutations.tsv</code> is the filename of the output.</p> <p><code>--canonical</code> is a VEP command line option, it simply adds a column pointing out whether the transcript is canonical.</p> <p>Right after <code>--plugin dbNSFP,/data/bbg/datasets/vep/homo_sapiens/plugins/dbNSFP4.5a_grch38.gz</code> and separated by commas it goes the list of all the accepted scores that you want to be mapped to your query mutations. You can check out the full list of scores here:</p> <p>Bear in mind that some scores have several tracks of data and some postprocessing might be required after the query.</p>"},{"location":"Datasets/Other_data/dbNSFP_mutation_scores/#reference","title":"Reference","text":"<p>Ferran Mui\u00f1os, Paula Gomis, Miguel Grau</p> <p>Updated 2024/07/01</p>"},{"location":"IRB/VPN/","title":"VPN","text":"<p>When working from home or outside the PCB, connecting to the VPN is neeeded in order to have access to resources such as the cluster.</p> <p>Since December 2023, the setup for setting up the VPN has changed. Now, it is required to have a Two Factor Authentication using an Authenticator app.</p>"},{"location":"IRB/VPN/#first-things-first-setting-up","title":"First Things First - Setting Up","text":"<p>After you changed your initial password to your preferred password, as explained in the letter, you are ready to set up your two-factor authentication needed for the VPN.</p> <p>Two Factor Authentication PDF</p> <p>Once your two-factor authentication is set up, you are ready to connect to the VPN.</p>"},{"location":"IRB/VPN/#connecting-on-linux","title":"Connecting on Linux","text":""},{"location":"IRB/VPN/#ubuntu-2004-2204","title":"Ubuntu 20.04 / 22.04","text":"<p>Openconnect-Client Setup - pdf</p>"},{"location":"IRB/VPN/#possible-errors","title":"Possible Errors","text":""},{"location":"IRB/VPN/#vpn-stuck-on-login-successful","title":"VPN stuck on Login successful","text":"<p>With Ubuntu 22, if you keep getting stuck on \"Login Successful\", here is a fix:</p> <p>In your terminal do:</p> <pre><code>sudo add-apt-repository ppa:dwmw2/openconnect\nsudo apt update\nsudo apt-get install openconnect\n</code></pre> <p>This should fix the default way to connect to the VPN.</p>"},{"location":"IRB/VPN/#full-instructions","title":"Full instructions","text":"<p>VPN tutorial - pdf</p>"},{"location":"IRB/VPN/#references","title":"References","text":"<ul> <li>Miguel Grau</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"IRB/eduroam/","title":"Eduroam","text":""},{"location":"IRB/eduroam/#configuration","title":"Configuration","text":"<ul> <li>Security: WPA &amp; WPA2 Enterprise</li> <li>Authentication: Protected EAP (PEAP)</li> <li>Anonymous identity: Empty</li> <li>Check the checkbox \"No CA certificate is required\"</li> <li>PEAP version: Automatic</li> <li>Inner authentication: MSCHAPv2</li> <li>Username: clusteruser@irbbarcelona.org</li> <li>Password: IRB password</li> </ul>"},{"location":"IRB/eduroam/#example","title":"Example","text":""},{"location":"IRB/eduroam/#reference","title":"Reference","text":"<ul> <li>Martina Gasull</li> <li>Carlos L\u00f3pez-Elorduy</li> <li>Miguel Grau</li> </ul>"},{"location":"IRB/fifteenWIFI/","title":"Fifteen wifi","text":""},{"location":"IRB/fifteenWIFI/#configuration","title":"Configuration","text":"<ul> <li>Password: universitas</li> </ul>"},{"location":"IRB/fifteenWIFI/#reference","title":"Reference","text":"<ul> <li>Ferriol Calvet</li> </ul>"},{"location":"Methods/Sequencing/Bedtools/","title":"Bedtools","text":"<p>The Bedtools suite is a collection of tools for a wide-range of genomics analysis tasks.</p>"},{"location":"Methods/Sequencing/Bedtools/#installation","title":"Installation","text":"<p>You can check the installation guide here.</p>"},{"location":"Methods/Sequencing/Bedtools/#usage","title":"Usage","text":"<p>To run the default installed version of Bedtools, simply load the bedtools module:</p> <pre><code>$ bedtools -h\n\nUsage:   bedtools &lt;subcommand&gt; [options]\nFor full usage documentation, run bedtools -h.\n</code></pre>"},{"location":"Methods/Sequencing/Bedtools/#example-job","title":"Example job","text":""},{"location":"Methods/Sequencing/Bedtools/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\n# Report the base-pair overlap between the features in two BED files.\nbedtools intersect -a reads.bed -b genes.bed\n</code></pre>"},{"location":"Methods/Sequencing/Bedtools/#links","title":"Links","text":"<ul> <li>Bedtools GitHub</li> <li>Bedtools documentation</li> </ul>"},{"location":"Methods/Sequencing/Bedtools/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Methods/Sequencing/Bowtie2/","title":"Bowtie2","text":"<p>Bowtie2 is an ultra fast and memory-efficient tool for aligning sequencing reads to long reference sequences.</p>"},{"location":"Methods/Sequencing/Bowtie2/#installation","title":"Installation","text":"<p>You can check the installation documentation here.</p>"},{"location":"Methods/Sequencing/Bowtie2/#conda","title":"Conda","text":"<pre><code>conda install -c bioconda bowtie2\n</code></pre>"},{"location":"Methods/Sequencing/Bowtie2/#package-manager","title":"Package manager","text":"<pre><code>sudo apt update\nsudo apt install bowtie2\n</code></pre>"},{"location":"Methods/Sequencing/Bowtie2/#manually-on-ubuntulinux","title":"Manually on Ubuntu/Linux","text":"<p>Create and go to install directory</p> <pre><code>cd $HOME/tools/bowtie2/\n</code></pre> <p>Download Ubuntu/Linux version</p> <pre><code>wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.4.2/bowtie2-2.4.2-sra-linux-x86_64.zip/download\n</code></pre> <p>Decompress</p> <pre><code>unzip download\n</code></pre> <p>Add location to system PATH</p> <pre><code>export PATH=$HOME/tools/bowtie2/bowtie2-2.4.2-sra-linux-x86_64:$PATH\n</code></pre>"},{"location":"Methods/Sequencing/Bowtie2/#check-installation","title":"Check installation","text":"<pre><code>bowtie2 --help\n</code></pre>"},{"location":"Methods/Sequencing/Bowtie2/#usage","title":"Usage","text":"<pre><code>$ bowtie2 -h\n\nUsage:   bowtie2 [options]* -x &lt;bt2-idx&gt; {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt; |\n         --interleaved &lt;i&gt;} -S [&lt;sam&gt;]\n</code></pre> <p>For full usage documentation, run <code>bowtie2 -h</code>.</p>"},{"location":"Methods/Sequencing/Bowtie2/#example-job","title":"Example job","text":""},{"location":"Methods/Sequencing/Bowtie2/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\n# Prepare example genomes in &lt;inputDir&gt;\n# Output is stored in &lt;outputDir&gt;\nbowtie2-build &lt;inputDir&gt; &lt;outputDir&gt;\nbowtie2-inspect &lt;outputDir&gt;\n</code></pre>"},{"location":"Methods/Sequencing/Bowtie2/#links","title":"Links","text":"<ul> <li>Bowtie2 GitHub</li> <li>Bowtie2 example</li> </ul>"},{"location":"Methods/Sequencing/Bowtie2/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Methods/Sequencing/Cutadapt/","title":"Cutadapt","text":""},{"location":"Methods/Sequencing/Cutadapt/#description","title":"Description","text":"<p>Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.</p>"},{"location":"Methods/Sequencing/Cutadapt/#installation","title":"Installation","text":"<p>You can check the installation documentation here.</p>"},{"location":"Methods/Sequencing/Cutadapt/#package-manager","title":"Package Manager","text":"<pre><code>sudo apt install cutadapt\n</code></pre> <p>Or possibly</p> <pre><code>sudo apt install python3-cutadapt\n</code></pre> <p>Verify the installation:</p> <pre><code>cudatapt --version\n</code></pre>"},{"location":"Methods/Sequencing/Cutadapt/#conda","title":"Conda","text":"<p>If you use <code>conda</code>, you can install <code>cutadapt</code> with:</p> <pre><code>conda install -c bioconda cutadapt\n</code></pre>"},{"location":"Methods/Sequencing/Cutadapt/#basic-usage","title":"Basic usage","text":"<p>For the full list of options:</p> <pre><code>cutadapt -h\n</code></pre>"},{"location":"Methods/Sequencing/Cutadapt/#basic-command-syntax","title":"Basic command syntax","text":"<pre><code>cutadapt -a &lt;adapter-sequence&gt; -o &lt;output-file&gt; &lt;input-file&gt;\n</code></pre> <p>For example, trimming an adapter (<code>AGATCGGAAGAG</code> in this case) from a fastq file will be:</p> <pre><code>cutadapt -a AGATCGGAAGAG -o output_reads.fastq input_reads.fastq\n</code></pre> <p>If you're working with paired-end reads, specify both input and output files:</p> <pre><code>cutadapt -a AGATCGGAAGAG -A AGATCGGAAGAG -o output_R1.fastq -p output_R2.fastq input_R1.fastq input_R2.fastq\n</code></pre> <p>You can use <code>cutadapt</code> to trim low-quality bases from the ends of reads ( in this example quality score &lt; 20):</p> <pre><code>cutadapt -q 20 -o output_reads.fastq input_reads.fastq\n</code></pre> <p>And also to discard reads shorter than a selected value after trimming (in this example 30 nucleotides)</p> <pre><code>cutadapt -m 30 -o output_reads.fastq input_reads.fastq\n</code></pre>"},{"location":"Methods/Sequencing/Cutadapt/#warning","title":"Warning","text":"<p>Check out the full documentation on how to use cutadapt for all different types of adapters. Cutadapt can detect multiple adapter types:</p> Adapter Type Command-line Option Regular 3' adapter <code>-a ADAPTER</code> Regular 5' adapter <code>-g ADAPTER</code> Non-internal 3' adapter <code>-a ADAPTERX</code> Non-internal 5' adapter <code>-g XADAPTER</code> Anchored 3' adapter <code>-a ADAPTER$</code> Anchored 5' adapter <code>-g ^ADAPTER</code> 5' or 3' (both possible) <code>-b ADAPTER</code> Linked adapter <code>-a ADAPTER1...ADAPTER2 -g ADAPTER1...ADAPTER2</code>"},{"location":"Methods/Sequencing/Cutadapt/#reference","title":"Reference","text":"<ul> <li>Full documentation</li> <li>cutadapt GitHub</li> <li>Davide Scarpetta</li> </ul>"},{"location":"Methods/Sequencing/Fastqc/","title":"Fastqc","text":"<p>FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines.</p>"},{"location":"Methods/Sequencing/Fastqc/#installation","title":"Installation","text":""},{"location":"Methods/Sequencing/Fastqc/#apt-get","title":"apt-get","text":"<pre><code>sudo apt-get update\nsudo apt-get install fastqc\n</code></pre>"},{"location":"Methods/Sequencing/Fastqc/#apt","title":"apt","text":"<pre><code>sudo apt update\nsudo apt install fastqc\n</code></pre>"},{"location":"Methods/Sequencing/Fastqc/#usage","title":"Usage","text":"<pre><code>$ fastqc --help\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam]\n           [-c contaminant file] seqfile1 .. seqfileN\n</code></pre>"},{"location":"Methods/Sequencing/Fastqc/#example-job","title":"Example job","text":""},{"location":"Methods/Sequencing/Fastqc/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 1 core and 1GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\nfastqc raw_data.fastq.gz raw_data2.fastq.gz\n</code></pre> <p>Viewing the Fastqc results</p> <p>To view the Fastqc results, you may open the fastqc_report.html file in a web browser or the <code>summary.txt</code> file (located in the zipped output archive) on the command line. For assistance copying files to your local machine, please see the Moving Data page.</p>"},{"location":"Methods/Sequencing/Fastqc/#links","title":"Links","text":"<ul> <li>FastQC website</li> <li>FastQC manual</li> <li>FastQC video tutorial</li> </ul>"},{"location":"Methods/Sequencing/Fastqc/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Methods/Sequencing/Samtools/","title":"SAMtools","text":"<p>SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments.</p>"},{"location":"Methods/Sequencing/Samtools/#installation","title":"Installation","text":""},{"location":"Methods/Sequencing/Samtools/#conda","title":"Conda","text":"<pre><code>conda install -c bioconda samtools\n</code></pre>"},{"location":"Methods/Sequencing/Samtools/#manual","title":"Manual","text":"<p>For the manual installation, you can find the instructions here.</p>"},{"location":"Methods/Sequencing/Samtools/#usage","title":"Usage","text":"<pre><code>samtools view -b -S -o genome_reads_aligned.bam genome_reads_aligned.sam\n</code></pre> <p>Core Usage</p> <p>To ensure that SAMtools uses the correct number of cores, the <code>-@ ${NSLOTS}</code> option should be used on commands that support it.</p>"},{"location":"Methods/Sequencing/Samtools/#example-job","title":"Example job","text":""},{"location":"Methods/Sequencing/Samtools/#serial-job","title":"Serial job","text":"<p>Here is an example job running on 4 cores and 8GB of memory:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 4\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=2G\n\nsamtools view -@ ${NSLOTS} -b -S -o genome_reads_aligned.bam \\\n    genome_reads_aligned.sam\n\nsamtools sort -@ ${NSLOTS} genome_reads_aligned.bam \\\n    &gt; genome_reads_aligned.sorted.bam\n\nsamtools index genome_reads_aligned.sorted.bam\n\nsamtools mpileup -g -f ref_genome_1K.fna genome_reads_aligned.sorted.bam \\\n    &gt; genome_variants.bcf\n</code></pre>"},{"location":"Methods/Sequencing/Samtools/#links","title":"Links","text":"<ul> <li>Samtools GitHub</li> <li>Samtools website</li> <li>Samtools documentation</li> </ul>"},{"location":"Methods/Sequencing/Samtools/#reference","title":"Reference","text":"<ul> <li>Hania Kranas</li> </ul>"},{"location":"Methods/Sequencing/Trimmomatic/","title":"Trimmomatic","text":"<p>Trimmomatic, a  flexible read trimming tool for Illumina NGS data.</p>"},{"location":"Methods/Sequencing/Trimmomatic/#description","title":"Description","text":"<p>Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line.</p> <p>The current trimming steps are:</p> <ul> <li>ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.</li> <li>SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.</li> <li>LEADING: Cut bases off the start of a read, if below a threshold quality</li> <li>TRAILING: Cut bases off the end of a read, if below a threshold quality</li> <li>CROP: Cut the read to a specified length</li> <li>HEADCROP: Cut the specified number of bases from the start of the read</li> <li>MINLEN: Drop the read if it is below a specified length</li> <li>TOPHRED33: Convert quality scores to Phred-33</li> <li>TOPHRED64: Convert quality scores to Phred-64</li> </ul> <p>It works with FASTQ (using phred + 33 or phred + 64 quality scores, depending on the Illumina pipeline used), either uncompressed or gzipp'ed FASTQ. Use of gzip format is determined based on the .gz extension.</p> <p>For single-ended data, one input and one output file are specified, plus the processing steps. For paired-end data, two input files are specified, and 4 output files, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not.</p>"},{"location":"Methods/Sequencing/Trimmomatic/#installation","title":"Installation","text":""},{"location":"Methods/Sequencing/Trimmomatic/#conda","title":"Conda","text":"<p>Use the bioconda channel to install Trimmomatic:</p> <pre><code>conda install -c bioconda trimmomatic\n</code></pre> <p>Check the installation simply running:</p> <pre><code>trimmomatic\n</code></pre> <p>This should output a message or the help output.</p>"},{"location":"Methods/Sequencing/Trimmomatic/#manual-installation","title":"Manual Installation","text":"<p>As <code>Trimmomatic</code> is a Java-based tool, be sure to have Java installed checking the version:</p> <pre><code>java -version\n</code></pre> <p>Install it if needed.</p> <p>The easiest option is to download a binary release zip, and unpack it somewhere convenient. You can download the binary file from the GitHub official <code>Trimmomatic</code> repo or from the website:</p> <pre><code>http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.39.zip\n</code></pre> <p>I recommend to download the <code>version 0.40</code> directly from the GitHub repo. Then you'll have to unzip the downloaded file:</p> <pre><code>unzip Trimmomatic-0.40.zip\n</code></pre> <p>This is optional, but if you want to use <code>Trimmomatic</code> globally, add it to your path:</p> <pre><code>export PATH=/path/to/trimmomatic:$PATH\n</code></pre>"},{"location":"Methods/Sequencing/Trimmomatic/#basic-usage","title":"Basic Usage","text":""},{"location":"Methods/Sequencing/Trimmomatic/#basic-syntax","title":"Basic Syntax","text":"<pre><code>java -jar trimmomatic.jar &lt;mode&gt; &lt;input&gt; &lt;options&gt;\n</code></pre> <ul> <li><code>mode</code>: Specify whether it's single-end (SE) or paired-end (PE) sequencing.</li> <li><code>input</code>: The input FASTQ file(s).</li> <li><code>options</code>: Specify trimming options (e.g., adapters, quality thresholds).</li> </ul>"},{"location":"Methods/Sequencing/Trimmomatic/#paired-end","title":"Paired End","text":"<pre><code>java -jar trimmomatic-0.39.jar PE input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:True LEADING:3 TRAILING:3 MINLEN:36\n</code></pre> <p>This will perform the following:</p> <ul> <li>Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10)</li> <li>Remove leading low quality or N bases (below quality 3) (LEADING:3)</li> <li>Remove trailing low quality or N bases (below quality 3) (TRAILING:3)</li> <li>Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15)</li> <li>Drop reads below the 36 bases long (MINLEN:36)</li> </ul>"},{"location":"Methods/Sequencing/Trimmomatic/#single-end","title":"Single End","text":"<pre><code>java -jar trimmomatic-0.35.jar SE -phred33 input.fq.gz output.fq.gz ILLUMINACLIP:TruSeq3-SE:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n</code></pre> <p>This will perform the same steps as paired-end command, using the single-ended adapter file.</p>"},{"location":"Methods/Sequencing/Trimmomatic/#reference","title":"Reference","text":"<ul> <li>Full documentation</li> <li>trimmomatic GitHub</li> <li>Davide Scarpetta</li> </ul>"},{"location":"Methods/Sequencing/Vep/","title":"Vep","text":""},{"location":"Methods/Sequencing/Vep/#ensembl-vep","title":"Ensembl-VEP","text":"<p>VEP determines the effect of your variants (insertions, deletions and structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions.</p>"},{"location":"Methods/Sequencing/Vep/#how-and-where-to-install-a-new-version","title":"How and where to install a new version?","text":"<p>There is a shared folder in datasets where there are several vep cache versions and docker containers. If you are planning to download one, make sure to store it in this location.</p> <p><code>/data/bbg/datasets/vep</code></p> <p>The easiest way to use a new version of vep is downloading the docker container. Most popular versions are already downloaded in <code>/data/bbg/datasets/vep/homo_sapiens</code> (or <code>/data/bbg/datasets/vep/mus_musculus</code> for mice).</p> <p>An example for downloading a new version:</p> <pre><code>singularity pull docker://ensemblorg/ensembl-vep:release_109.0\n</code></pre> <p>Once the container is ready, we can download the vep-cache required:</p> <pre><code>singularity exec ensembl-vep_109.sif INSTALL.pl -c 109_GRCh38/ -a cf -s homo_sapiens --ASSEMBLY GRCh38\n</code></pre> <p>IMPORTANT. Each database only works with the specific ensembl-vep version used to download the database. In the previous example, the <code>109_GRCh38/</code> will work only with the ensembl-vep_109 version.</p> <p>For more details, you can follow the installation guide.</p>"},{"location":"Methods/Sequencing/Vep/#how-to-use","title":"How to use","text":"<p>Once you are in a working node:</p> <pre><code>mgrau@bbgn009:/data/bbg/datasets/vep/homo_sapiens$ singularity exec ensembl-vep_109.sif vep\n\n#----------------------------------#\n# ENSEMBL VARIANT EFFECT PREDICTOR #\n#----------------------------------#\n\nVersions:\n  ensembl              : 109.10baaec\n  ensembl-funcgen      : 109.cba2db8\n  ensembl-io           : 109.4946a86\n  ensembl-variation    : 109.18a12b6\n  ensembl-vep          : 109.3\n\nHelp: dev@ensembl.org , helpdesk@ensembl.org\nTwitter: @ensembl\n\nhttp://www.ensembl.org/info/docs/tools/vep/script/index.html\n\nUsage:\n./vep [--cache|--offline|--database] [arguments]\n\nBasic options\n=============\n\n--help                 Display this message and quit\n\n-i | --input_file      Input file\n-o | --output_file     Output file\n--force_overwrite      Force overwriting of output file\n--species [species]    Species to use [default: \"human\"]\n\n--everything           Shortcut switch to turn on commonly used options. See web\n                       documentation for details [default: off]\n--fork [num_forks]     Use forking to improve script runtime\n\nFor full option documentation see:\nhttp://www.ensembl.org/info/docs/tools/vep/script/vep_options.html\n</code></pre>"},{"location":"Methods/Sequencing/Vep/#example-job","title":"Example job","text":"<p>A real example command could be:</p> <pre><code>mgrau@bbgn009:/data/bbg/datasets/vep/homo_sapiens$ singularity exec vep109.sif vep --dir /data/bbg/datasets/vep/ -i variants_ref38.vcf.gz --offline --format vcf --vcf --cache -o exampleout.vcf --species homo_sapiens --assembly GRCh38 --fork 8\n</code></pre> <p>To speed up the process, it is recommended to use the downloaded vep-cache files specifying the directory (<code>--dir</code>) and the <code>--offline</code> and <code>--cache</code> options. VEP allows multithreating using the <code>--fork</code> option.</p> <p>For full option documentation see here.</p> <p>Full instructions on how to download and use cached files can be found here.</p>"},{"location":"Methods/Sequencing/Vep/#additional-comments","title":"Additional comments","text":"<p>Be careful when running VEP with the TAB output and then merging again the variants from a VCF file, some indels are reformated in VEP and you cannot pair them with the original mutations.</p> <p>Info</p> <p>You can find another example using VEP data and google cloud -- see Extract minibams from Hartwig data in googleCloud section.</p>"},{"location":"Methods/Sequencing/Vep/#links","title":"Links","text":"<ul> <li>Ensembl-VEP documentation</li> <li>Ensembl-VEP tutorial</li> <li>Ensembl-VEP examples</li> <li>Ensembl-VEP web application</li> </ul>"},{"location":"Methods/Sequencing/Vep/#reference","title":"Reference","text":"<ul> <li>Miguel Grau</li> <li>Federica Brando</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"Methods/Sequencing/pysam/","title":"pysam","text":""},{"location":"Methods/Sequencing/pysam/#description","title":"Description","text":""},{"location":"Methods/Sequencing/pysam/#reference","title":"Reference","text":""},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/","title":"HMFtools","text":""},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#description","title":"Description","text":"<p>HMFtools is a pipeline that can be run on the BBGlab cluster to perform variant calling and analysis. It resembles the exact pipeline that it is used in Google Cloud Platform (see Platinum) but it does not run in parallel and it is not optimized for HPC computing.</p> <p>The scripts, the tools and the reference data are provided by HMF on their GitHub and Google Cloud.</p>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#installation","title":"Installation","text":""},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#create-conda-env-with-requirements","title":"create conda env with requirements","text":"<p>Move to the hmftools pipeline folder:</p> <pre><code>cd /data/bbg/projects/hartwig/hmftools/pipeline/\n</code></pre> <p>create a conda envirnoment with the <code>environment.yml</code> file.</p> <pre><code>conda env create --file environment.yml\n</code></pre> <p>activate the environment</p> <pre><code>conda activate hmftpipe\n</code></pre> <p>Warning</p> <p>You need specific permission to access the <code>/data/bbg/projects/hartwig/</code> following folder. Please contact Martina or Paula if you need to use this data. More info here.</p>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#run","title":"Run","text":"<p>It can be run with a <code>.bam</code> file or a <code>.fastq</code> file.</p> BAMFASTQ.gz <p>Run mode</p> WGS modePanel mode <p>to run the pipeline the command is the following: </p> <pre><code>./scripts/run_pipeline &lt;sample_data&gt; \"tumorID,referenceID\" &lt;gen_version&gt; &lt;run_mode&gt; &lt;threads&gt; &lt;memory&gt;\n</code></pre> <p>to run the pipeline the command is the following: </p> <pre><code>./scripts/run_pipeline &lt;sample_data&gt; \"tumorID\" &lt;gen_versio&gt; &lt;run mode&gt; &lt;threads&gt; &lt;memory&gt;\n</code></pre> <p>This option is going to take longer since we are running the alignment step.</p> <p>Run mode</p> WGS modePanel mode <p>to run the pipeline the command is the following: </p> <pre><code>./scripts/run_pipeline &lt;sample_data&gt; \"tumorID,referenceID\" &lt;gen_version&gt; &lt;run_mode&gt; &lt;threads&gt; &lt;memory&gt;\n</code></pre>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#sample-data-directory-output-directory","title":"<code>sample data</code> directory = output directory","text":"<p>The <code>sample_data</code> directory must have:</p> <ul> <li>a directory named as the sample's tumorId</li> <li>tumor and reference BAM and BAM index files in the sample's directory, named as tumorId and referenceId.bam</li> </ul> <p>i.e. see folder tree below</p> <pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 referenceId.bam\n      \u251c\u2500\u2500 referenceId.bam.bai\n      \u251c\u2500\u2500 tumorId.bam\n      \u2514\u2500\u2500 tumorId.bam.bai\n</code></pre> <p>In the tumorID folder the pipeline will make a directory for each of the steps and store the results in subfolders.</p>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#gen_version-parameter","title":"<code>gen_version</code> parameter","text":"Genome version Genome file V38 <code>./ref_data_dir/V38/ref_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna</code> V37 <code>./ref_data_dir/V37/ref_genome/Homo_sapiens.GRCh37.GATK.illumina.fasta</code>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#threads-and-memory-parameters","title":"<code>threads</code> and <code>memory</code> parameters","text":"<ul> <li><code>threads</code> number of threads used for each component</li> <li><code>memory</code>: GB allocated to each component (default=12GB)</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#example","title":"Example","text":"<p>my directory:</p> <p><pre><code>pipeline/test_data/\n \u2514\u2500\u2500 COLO928T\n      \u251c\u2500\u2500 COLO928R.bam\n      \u251c\u2500\u2500 COLO928R.bam.bai\n      \u251c\u2500\u2500 COLO928T.bam\n      \u2514\u2500\u2500 COLO928T.bam.bai\n</code></pre> Run: </p> <pre><code>./scripts/run_pipeline test_data \"COLO928T,COLO928R\" V37 WGS 8 16\n</code></pre> Example output <pre><code>pipeline/test_data/COLO829T\n\u251c\u2500\u2500 amber\n\u2502   \u251c\u2500\u2500 amber.version\n\u2502   \u251c\u2500\u2500 COLO829R.amber.homozygousregion.tsv\n\u2502   \u251c\u2500\u2500 COLO829R.amber.snp.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829R.amber.snp.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.pcf\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.tsv.gz\n\u2502   \u251c\u2500\u2500 COLO829T.amber.contamination.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.amber.contamination.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.amber.contamination.vcf.gz.tbi\n\u2502   \u2514\u2500\u2500 COLO829T.amber.qc\n\u251c\u2500\u2500 cobalt\n\u2502   \u251c\u2500\u2500 cobalt.version\n\u2502   \u251c\u2500\u2500 COLO829R.cobalt.gc.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829R.cobalt.ratio.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829R.cobalt.ratio.pcf\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.gc.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.ratio.pcf\n\u2502   \u2514\u2500\u2500 COLO829T.cobalt.ratio.tsv.gz\n\u251c\u2500\u2500 COLO829R.bam\n\u251c\u2500\u2500 COLO829R.bam.bai\n\u251c\u2500\u2500 COLO829T.bam\n\u251c\u2500\u2500 COLO829T.bam.bai\n\u251c\u2500\u2500 gridss\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.bam\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.junctions.csv\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam\n\u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.bai\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.fragment_lengths\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.junctions.csv\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.bai\n\u2502   \u2514\u2500\u2500 gridss\n\u2502       \u251c\u2500\u2500 COLO829R.bam.gridss.working\n\u2502       \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.computesamtags.changes.tsv\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.insert_size_histogram.pdf\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.insert_size_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829R.sv_prep.sorted.bam.sv.bam.csi\n\u2502       \u2502   \u2514\u2500\u2500 COLO829R.sv_prep.sorted.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 COLO829T.bam.gridss.working\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.alignment_summary_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.coverage.blacklist.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.downsampled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.excluded_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.subsetCalled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam.bai\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.gridss.working\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.allocated.vcf.idx\n\u2502       \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.computesamtags.changes.tsv\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_histogram.pdf\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam.csi\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.sv_prep.sorted.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 gridss.full.20221116_120546.bbgn004.49271.log\n\u2502       \u251c\u2500\u2500 gridss.timing.20221116_120546.bbgn004.49271.log\n\u2502       \u2514\u2500\u2500 libsswjni.so\n\u251c\u2500\u2500 gripss_germline\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.germline.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.germline.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.germline.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.gripss.germline.vcf.gz.tbi\n\u251c\u2500\u2500 gripss_somatic\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.somatic.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.gripss.somatic.vcf.gz.tbi\n\u251c\u2500\u2500 lilac\n\u2502   \u251c\u2500\u2500 COLO829T.candidates.coverage.csv\n\u2502   \u251c\u2500\u2500 COLO829T.lilac.csv\n\u2502   \u2514\u2500\u2500 COLO829T.lilac.qc.csv\n\u251c\u2500\u2500 linx_germline\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.clusters.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.disruption.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.driver.catalog.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.links.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.germline.svs.tsv\n\u2502   \u2514\u2500\u2500 linx.version\n\u251c\u2500\u2500 linx_somatic\n\u2502   \u251c\u2500\u2500 COLO829T.linx.breakend.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.clusters.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.driver.catalog.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.drivers.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.fusion.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.links.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.svs.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_copy_number.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_fusion.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_gene_exon.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_protein_domain.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_segments.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.linx.vis_sv_data.tsv\n\u2502   \u251c\u2500\u2500 linx.version\n\u2502   \u251c\u2500\u2500 plot_data\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.003.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.003.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.chromosome.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.circos.003.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.cna.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.connector.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.distance.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.exon.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.exon.rank.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.fragile.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.gene.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.gene.name.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.karyotype.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.line_element.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.position.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.scatter.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.scatter.sgl.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr10.debug.segment.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.003.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.003.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.chromosome.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.circos.003.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.cna.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.connector.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.distance.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.exon.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.exon.rank.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.fragile.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.gene.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.gene.name.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.karyotype.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.line_element.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.position.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.scatter.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.scatter.sgl.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.chr18.debug.segment.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.003.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.003.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.chromosome.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.circos.003.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.cna.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.connector.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.distance.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.exon.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.exon.rank.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.fragile.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.gene.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.gene.name.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.karyotype.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.line_element.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.position.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.scatter.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cluster-1.sv1.debug.scatter.sgl.circos\n\u2502   \u2502   \u2514\u2500\u2500 COLO829T.cluster-1.sv1.debug.segment.circos\n\u2502   \u2514\u2500\u2500 plots\n\u2502       \u251c\u2500\u2500 COLO829T.chr10.debug.003.png\n\u2502       \u251c\u2500\u2500 COLO829T.chr18.debug.003.png\n\u2502       \u2514\u2500\u2500 COLO829T.cluster-1.sv1.debug.003.png\n\u251c\u2500\u2500 pave_germline\n\u2502   \u251c\u2500\u2500 COLO829T.sage.germline.pave.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.germline.pave.vcf.gz.tbi\n\u251c\u2500\u2500 pave_somatic\n\u2502   \u251c\u2500\u2500 COLO829T.sage.somatic.pave.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.somatic.pave.vcf.gz.tbi\n\u251c\u2500\u2500 purple\n\u2502   \u251c\u2500\u2500 circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829R.ratio.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.baf.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cnv.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.indel.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.ratio.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.snp.circos\n\u2502   \u2502   \u2514\u2500\u2500 gaps.txt\n\u2502   \u251c\u2500\u2500 COLO829T.driver.catalog.germline.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.driver.catalog.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.gene.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.germline.deletion.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.germline.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.germline.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.range.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.qc\n\u2502   \u251c\u2500\u2500 COLO829T.purple.segment.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.clonality.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.hist.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 plot\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.copynumber.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.map.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.purity.range.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.segment.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.clonality.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.png\n\u2502   \u2502   \u2514\u2500\u2500 COLO829T.somatic.rainfall.png\n\u2502   \u2514\u2500\u2500 purple.version\n\u251c\u2500\u2500 run005_37_wgs.log\n\u251c\u2500\u2500 sage_germline\n\u2502   \u251c\u2500\u2500 COLO829T.sage.germline.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.germline.vcf.gz.tbi\n\u2514\u2500\u2500 sage_somatic\n    \u251c\u2500\u2500 COLO829R.sage.bqr.tsv\n    \u251c\u2500\u2500 COLO829T.sage.bqr.tsv\n    \u251c\u2500\u2500 COLO829T.sage.exon.medians.tsv\n    \u251c\u2500\u2500 COLO829T.sage.gene.coverage.tsv\n    \u251c\u2500\u2500 COLO829T.sage.somatic.vcf.gz\n    \u2514\u2500\u2500 COLO829T.sage.somatic.vcf.gz.tbi\n</code></pre>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#sample-data-directory-output-directory_1","title":"<code>sample data</code> directory = output directory","text":"<p>The <code>sample_data</code> must have:</p> <ul> <li>a directory named as the sample's tumorId</li> <li>tumor BAM and BAM index files in the sample's directory, named as tumorId and referenceId.bam</li> </ul> <p>i.e. see folder tree below</p> <pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 tumorId.bam\n      \u2514\u2500\u2500 tumorId.bam.bai\n</code></pre> <p>In the tumorID folder the pipeline will make a directory for each of the steps and store the results in subfolders.</p>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#gen_version-parameter_1","title":"<code>gen_version</code> parameter","text":"Genome version Genome file V38 <code>./ref_data_dir/V38/ref_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna</code> V37 <code>./ref_data_dir/V37/ref_genome/Homo_sapiens.GRCh37.GATK.illumina.fasta</code>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#threads-and-memory-parameters_1","title":"<code>threads</code> and <code>memory</code> parameters","text":"<ul> <li><code>threads</code> number of threads used for each component</li> <li><code>memory</code>: GB allocated to each component (default=12GB)</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#example_1","title":"Example","text":"<p>my directory:</p> <p><pre><code>pipeline/test_data/\n \u2514\u2500\u2500 COLO928T\n      \u251c\u2500\u2500 COLO928T.bam\n      \u2514\u2500\u2500 COLO928T.bam.bai\n</code></pre> Run: </p> <pre><code>./scripts/run_pipeline test_data COLO928T V37 PANEL 8 16\n</code></pre> Example output <pre><code>pipeline/test_data/COLO829T/\n\u251c\u2500\u2500 amber\n\u2502   \u251c\u2500\u2500 amber.version\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.pcf\n\u2502   \u251c\u2500\u2500 COLO829T.amber.baf.tsv.gz\n\u2502   \u2514\u2500\u2500 COLO829T.amber.qc\n\u251c\u2500\u2500 cobalt\n\u2502   \u251c\u2500\u2500 cobalt.version\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.gc.median.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.cobalt.ratio.pcf\n\u2502   \u2514\u2500\u2500 COLO829T.cobalt.ratio.tsv.gz\n\u251c\u2500\u2500 COLO829T.bam\n\u251c\u2500\u2500 COLO829T.bam.bai\n\u251c\u2500\u2500 gridss\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gridss.unfiltered.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.fragment_lengths\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.junctions.csv\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam\n\u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.bai\n\u2502   \u2514\u2500\u2500 gridss\n\u2502       \u251c\u2500\u2500 COLO829T.bam.gridss.working\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.alignment_summary_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.coverage.blacklist.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.downsampled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.excluded_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.subsetCalled_0.bed\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.sv.bam.bai\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.assembly.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 COLO829T.gridss.raw.vcf.gz.gridss.working\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.gridss.raw.vcf.gz.allocated.vcf.idx\n\u2502       \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.gridss.working\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.cigar_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.computesamtags.changes.tsv\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.idsv_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_histogram.pdf\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.insert_size_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.mapq_metrics\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam\n\u2502       \u2502   \u251c\u2500\u2500 COLO829T.sv_prep.sorted.bam.sv.bam.csi\n\u2502       \u2502   \u2514\u2500\u2500 COLO829T.sv_prep.sorted.bam.tag_metrics\n\u2502       \u251c\u2500\u2500 fbrando\n\u2502       \u251c\u2500\u2500 gridss.full.20230113_110106.login01.6901.log\n\u2502       \u2514\u2500\u2500 libsswjni.so\n\u251c\u2500\u2500 gripss\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.filtered.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.gripss.somatic.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.gripss.somatic.vcf.gz.tbi\n\u251c\u2500\u2500 lilac\n\u2502   \u251c\u2500\u2500 COLO829T.candidates.coverage.csv\n\u2502   \u251c\u2500\u2500 COLO829T.lilac.csv\n\u2502   \u2514\u2500\u2500 COLO829T.lilac.qc.csv\n\u251c\u2500\u2500 linx\n\u2502   \u251c\u2500\u2500 plot_data\n\u2502   \u2514\u2500\u2500 plots\n\u251c\u2500\u2500 pave\n\u2502   \u251c\u2500\u2500 COLO829T.sage.pave.vcf.gz\n\u2502   \u2514\u2500\u2500 COLO829T.sage.pave.vcf.gz.tbi\n\u251c\u2500\u2500 purple\n\u2502   \u251c\u2500\u2500 circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.baf.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.circos.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.cnv.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.indel.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.conf\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.error\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png.out\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.link.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.map.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.ratio.circos\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.snp.circos\n\u2502   \u2502   \u251c\u2500\u2500 gaps.txt\n\u2502   \u2502   \u2514\u2500\u2500 null.ratio.circos\n\u2502   \u251c\u2500\u2500 COLO829T.driver.catalog.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.gene.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.cnv.somatic.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.range.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.purity.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.qc\n\u2502   \u251c\u2500\u2500 COLO829T.purple.segment.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.clonality.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.hist.tsv\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.somatic.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz\n\u2502   \u251c\u2500\u2500 COLO829T.purple.sv.vcf.gz.tbi\n\u2502   \u251c\u2500\u2500 plot\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.input.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.clonality.png\n\u2502   \u2502   \u251c\u2500\u2500 COLO829T.somatic.png\n\u2502   \u2502   \u2514\u2500\u2500 COLO829T.somatic.rainfall.png\n\u2502   \u2514\u2500\u2500 purple.version\n\u2514\u2500\u2500 sage\n    \u251c\u2500\u2500 COLO829T.sage.bqr.tsv\n    \u251c\u2500\u2500 COLO829T.sage.exon.medians.tsv\n    \u251c\u2500\u2500 COLO829T.sage.gene.coverage.tsv\n    \u251c\u2500\u2500 COLO829T.sage.vcf.gz\n    \u2514\u2500\u2500 COLO829T.sage.vcf.gz.tbi\n</code></pre>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#sample-data-directory-output-directory_2","title":"<code>sample data</code> directory = output directory","text":"<p>The <code>sample_data</code> directory must have:</p> <ul> <li>a directory named as the sample's tumorId</li> <li>tumor and reference fastq files in the sample's directory, named as tumorId and referenceId.fastq.gz</li> </ul> <p>i.e. see folder tree below</p> <pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 referenceId.fastq.gz\n      \u2514\u2500\u2500 tumorId.fastq.gz\n</code></pre> <p>The pipeline searches for the files that match the regex <code>tumorId.*.fastq.gz</code> and <code>referenceId.*.fastq.gz</code> in order to perform the alignment. Therefore, you could provide several fastq.gz files to align as in the example below: </p> <p><pre><code>sample_data\n \u2514\u2500\u2500 tumorID\n      \u251c\u2500\u2500 referenceId_L001_R1_001.fastq.gz\n      \u251c\u2500\u2500 referenceId_L001_R2_001.fastq.gz\n      \u251c\u2500\u2500 tumorId_L001_R1_001.fastq.gz\n      \u2514\u2500\u2500 tumorId_L001_R2_001.fastq.gz\n</code></pre> the pipeline will match all the fastq files that start with the <code>referenceId</code> or <code>tumorId</code> and end with <code>fastq.gz</code>, then it aligns them to the reference genome.</p>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#gen_version-parameter_2","title":"<code>gen_version</code> parameter","text":"Genome version Genome file V38 <code>./ref_data_dir/V38/ref_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna</code> V37 <code>./ref_data_dir/V37/ref_genome/Homo_sapiens.GRCh37.GATK.illumina.fasta</code>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#threads-and-memory-parameters_2","title":"<code>threads</code> and <code>memory</code> parameters","text":"<ul> <li><code>threads</code> number of threads used for each component</li> <li><code>memory</code>: GB allocated to each component (default=12GB)</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#example_2","title":"Example","text":"<p>my directory:</p> <p><pre><code>pipeline/test_data/\n \u2514\u2500\u2500 ALL-280622\n      \u251c\u2500\u2500 ALL-280622-DIAGNOSTIC-DNA_1.fastq.gz\n      \u2514\u2500\u2500 ALL-280622-DIAGNOSTIC-DNA_2.fastq.gz\n</code></pre> Run: </p> <pre><code>./scripts/run_pipeline test_data \"ALL-280622\" V37 WGS 8 16\n</code></pre> <p>WGS without reference</p> <p>as this example, it is possible to use WGS without a non-tumoral reference. the output is the following: </p> Example <pre><code>TBC\n</code></pre>"},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#reference","title":"Reference","text":""},{"location":"Methods/Sequencing/Variant_callers/Hmftools_pipeline/#source","title":"Source","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/","title":"MosaicForecast","text":"<p>A machine learning method that leverages read-based phasing and read-level features to accurately detect mosaic SNVs (SNPs, small indels) from NGS data. It builds on existing algorithms to achieve a multifold increase in specificity.</p>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#description","title":"Description","text":"<p>MosaicForecast is a machine learning-based variant caller designed for the sensitive detection of postzygotic mosaic single nucleotide variants (mSNVs) from whole-genome or whole-exome sequencing data. Developed by the Park Lab at Harvard Medical School, MosaicForecast integrates read-level features and a random forest classifier to distinguish mosaic mutations from germline variants and sequencing artifacts with high precision.</p>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#dependency","title":"Dependency","text":""},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#required-interpreter-versions","title":"Required Interpreter Versions","text":"<ul> <li>Python version 3.6+</li> <li>R version 3.5+</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#installation-of-dependencies","title":"Installation of Dependencies","text":"<ol> <li>We have created a docker image with all dependencies installed: https://hub.docker.com/r/yanmei/mosaicforecast     Usage:         docker image pull yanmei/mosaicforecast:0.0.1         docker run -v ${your_local_directory}:/MF --rm -it yanmei/mosaicforecast:0.0.1 /bin/bash         gunzip hs37d5.fa.gz         Phase.py /MF/demo/ /MF/demo/phasing hs37d5.fa /MF/demo/test.input 20 k24.umap.wg.bw 4</li> </ol> <p>!!! note         Please note that \"${your_local_directory}:/MF\" is the absolute path of your local mosaicforecast directory.         After attaching your local MF directory to the docker image, you would be able to read         and write from that directory in your docker image. The attached directory in the docker image would be \"/MF\". 2. You could also install conda first (https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh), and   then create an environment using conda through this command:</p> <pre><code>conda env create --name MF --file environment.yaml\n</code></pre> <p>The environment 'MF' could be activated through this command:</p> <pre><code>conda activate MF\n</code></pre> <p>Other dependencies and resources could be downloaded though running:  </p> <pre><code>bash downloads.sh\n</code></pre> <p>You can also set up the environment manually, even though it is not recommended. There's a list of packages in the git-repository</p>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#resources","title":"Resources","text":"<p>You will find how to download them in the git-repository:</p> <ul> <li>Human reference genome:</li> <li>Mappability score: Umap score (k=24)</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#regions-to-filter-out","title":"Regions to filter out","text":"<ul> <li>Segmental Duplication regions (should be removed before calling all kinds of mosaics):  </li> <li>Regions enriched for SNVs with &gt;=3 haplotypes (should be removed before calling all kinds of mosaics):  </li> <li>Simple repeats (should be removed before calling mosaic INDELS):  </li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#population-allele-frequency","title":"Population allele frequency","text":"<ul> <li>gnomAD datasets (recommend to remove variants with population MAF&gt;0.001%):  </li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#usage","title":"Usage","text":""},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#phasing","title":"Phasing","text":"<p>Usage:</p> <pre><code>python Phase.py bam_dir output_dir ref_fasta input_positions min_dp_inforSNPs Umap_mappability(bigWig file,k=24) \nn_threads_parallel sequencing_file_format(bam/cram)\n</code></pre> <p>Note:</p> <ol> <li>Name of bam files should be \"sample.bam\" under the bam_dir, and there should be index files under the same directory (samtools index sample.bam).</li> <li>There should be a fai file under the same dir of the fasta file (samtools faidx input.fa).</li> <li>File format of the input_positions: chr pos-1 pos ref alt sample, sep=\\t</li> <li>The \"min_dp_inforSNPs\" is the minimum depth of coverage of trustworthy neaby het SNPs, can be set to 20.</li> <li>The program to extract mappability score: \"bigWigAverageOverBed\" should be downloaded and installed, and its path should be added to the PATH environment variable.</li> </ol> <p>Demo:</p> <pre><code>python Phase.py demo demo/phasing ${human_g1k_v37_decoy.fasta} demo/test.input 20 ${k24.umap.wg.bw} 2\n</code></pre> <p>Output:</p> <p><code>output_dir/all.phasing</code></p> sample chr pos ref alt phasing conflicting_reads mappability variant_type test 12 52644508 C T hap=3 0 1.0 SNP test 15 75918044 G A hap=3 0 1.0 SNP test 1 1004865 G C hap=3 0 1.0 SNP test 1 2591769 AG A hap&gt;3 1 0.0 DEL test 1 33801576 TTTGTTG T hap=2 0 0.583333 DEL <pre><code>hap=2: likely het variants\nhap=3: likely mosaic variants\nhap&gt;3: likely cnv/repeat\nconflicting_reads: number of read pairs supporting both ref and alt alleles.\n\nIntermediate files:\n1. output_dir/all.merged.inforSNPs.pos: all nearby inforSNPs of candidate mosaics.\n2. output_dir/all_2x2table: 2x2 tables by all nearby inforSNPs.\n3. output_dir/all.phasing_2by2: Phasing results of mosaics and all nearby inforSNPs (2x2 table).\n4. output_dir/multiple_inforSNPs.log: Phasing results of different pairs of inforSNPs.\n</code></pre>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#extraction-of-read-level-features","title":"Extraction of read-level features","text":"<p>Usage:</p> <pre><code>python ReadLevel_Features_extraction.py input.bed output_features bam_dir ref.fa Umap_mappability(bigWig file,k=24) n_jobs_parallel sequencing_file_format(bam/cram)\n</code></pre> <p>Note:</p> <ol> <li>Names of bam files should be \"sample.bam\" under the bam_dir, and there should be index files under the same directory   (samtools index sample.bam). Cram files are also supported.</li> <li>There should be a fai file under the same dir of the fasta file (samtools faidx input.fa)</li> <li>File format of the input.bed: chr pos-1 pos ref alt sample, sep=\\t</li> <li>We did not use gnomad population AF as an feature (instead we use it to filter), but you can use it to train your   model if you have interest in common variants</li> <li>The program to extract mappability score: \"bigWigAverageOverBed\" should be downloaded and installed, and its path   should be added to the PATH environment variable.</li> </ol> <p>Demo:</p> <pre><code>python ReadLevel_Features_extraction.py demo/test.input demo/test.features demo ${ref.fa} ${k24.umap.wg.bw} 2 bam  \n</code></pre> <p>Output:</p> <p>A list of read-level features for each input site.</p> id dp_p conflict_num mappability type length GCcontent ref_softclip alt_softclip querypos_p leftpos_p seqpos_p mapq_p baseq_p baseq_t ref_baseq1b_p ref_baseq1b_t alt_baseq1b_p alt_baseq1b_t sb_p context major_mismatches_mean minor_mismatches_mean mismatches_p AF dp mosaic_likelihood het_likelihood refhom_likelihood althom_likelihood mapq_difference sb_read12_p dp_diff test\\~11\\~40316580\\~C\\~T 0.3183008162818 0 1 SNP 0 0.476190476190476 0.0240384615384615 0 0.1582 0.16521 0.68821 NA 0.91657 -0.57364 0.98911 0.21893 0.67576 -0.8528 0.69934 GGA 0.00878 0.0144466666666667 0.29396 0.028 214 0.999414559235067 3.90999117967593e-49 0.000585440764932926 0 0 0.69142 -12.8571 test\\~12\\~52644508\\~C\\~T 0.197545792452075 0 1 SNP 0 0.571428571428571 0.0208333333333333 0 0.19325 0.20057 0.88251 NA 0.11764 -0.95448 0.31536 0.6827 0.31601 0.58756 0.13401 CGC 0.01236 0.0127266666666667 0.17424 0.054 203 0.999999999985368 5.11687178601205e-39 1.46319954019795e-11 0 0 0.36124 -12.8571"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#genotype-prediction","title":"Genotype Prediction","text":"<p>Usage:</p> <p>Rscript Prediction.R input_file(feature_list) model_trained model_type(Phase|Refine) output_file(predictions)</p> <p>Note:</p> <ol> <li>The \"input_file\" is a list of read-level features obtained in the last step.</li> <li>The \"model_trained\" is the pre-trained RF model to predict genotypes.</li> <li>If you trained model with refined-genotypes (mosaic, het, refhom, repeat), then the \"model_type\" is \"Refine\";   otherwise if you trained model with Phasing (hap=2, hap=3, hap&gt;3), then the \"model_type\" is \"Phase\".</li> <li>We also added annotations of additional filtrations: Predicted mosaics with extra-high read depths (&gt;=2X),   sites with &gt;=1.5X read depths and &gt;=20% AF were marked as \"low-confidence\"; predicted mosaics with only one alt   allele and &lt;1% AF were marked as \"cautious\".  </li> </ol> <p>You may use our models trained with brain WGS data for SNPs (paired-end read at 50-250X read depths, we train our models based on Mutect2-PON callings. To our experience, the models were pretty robust across different depths, but the best strategy would be using a model with similar depth with your data):</p> <ul> <li>models_trained/50xRFmodel_addRMSK_Refine.rds</li> <li>models_trained/100xRFmodel_addRMSK_Refine.rds</li> <li>models_trained/150xRFmodel_addRMSK_Refine.rds</li> <li>models_trained/200xRFmodel_addRMSK_Refine.rds</li> <li>models_trained/250xRFmodel_addRMSK_Refine.rds</li> </ul> <p>We also pre-trained a model for mosaic deletions (using paired-end read at 250X, with phasing information):</p> <ul> <li>models_trained/deletions_250x.RF.rds</li> </ul> <p>Demo:</p> <pre><code>Rscript Prediction.R demo/test.SNP.features models_trained/250xRFmodel_addRMSK_Refine.rds Refine demo/test.SNP.predictions   \nRscript Prediction.R demo/test.DEL.features models_trained/deletions_250x.RF.rds Phase demo/test.DEL.predictions\n</code></pre> <p>Output:</p> <p>Genotype predictions for all input sites.</p> id AF dp prediction het mosaic refhom repeat test\\~11\\~40316580\\~C\\~T 0.028 214 mosaic 0.002 0.958 0 0.04 test\\~12\\~52644508\\~C\\~T 0.054 203 mosaic 0.002 0.982 0 0.016 test\\~15\\~75918044\\~G\\~A 0.036 193 mosaic 0.006 0.812 0 0.182 test\\~1\\~1004865\\~G\\~C 0.085 212 mosaic 0.006 0.988 0 0.006 <ol> <li>prediction: genotype predictions including refhom, het, mosaic and repeat.</li> <li>het/mosaic/refhom/repeat: genotyping probabilities for each genotype.</li> </ol>"},{"location":"Methods/Sequencing/Variant_callers/MosaicForecast/#reference","title":"Reference","text":"<ul> <li>git-repository</li> <li>Joan Enric</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Mutect2/","title":"Mutect2","text":""},{"location":"Methods/Sequencing/Variant_callers/Mutect2/#description","title":"Description","text":"<p>Mutect2 is a variant caller that calls somatic SNVs and indels via local assembly of haplotypes. It is included into Genome Analysis Toolkit a collection of command-line tools for analyzing high-throughput sequencing data with a primary focus on variant discovery.</p> <p>It can be used with two modes:</p> <ol> <li>tumor-normal mode where a tumor sample is matched with a normal sample in analysis.</li> <li>tumor-only mode where a single sample's alignment data undergoes analysis</li> </ol>"},{"location":"Methods/Sequencing/Variant_callers/Mutect2/#requirements","title":"Requirements","text":"<p>To run Mutect2, the program required the following files:</p> <p>-Tumor bam file. -Normal bam file. (Only required in the tumor-normal mode) -reference file in fasta format.</p> <p>Mutect2 does not require a germline resource nor a panel of normals (PoN) to run, although both are recommended. -germline resource: -panel of normals vcf file. To create a PoN, call on each normal sample in this mode, then use CreateSomaticPanelOfNormals to generate the PoN. </p>"},{"location":"Methods/Sequencing/Variant_callers/Mutect2/#usage","title":"Usage","text":"<p>(i) Tumor with matched normal</p> <pre><code>     gatk Mutect2 \\\n     -R reference.fa \\\n     -I tumor.bam \\\n     -I normal.bam \\\n     -normal normal_sample_name \\\n     --germline-resource af-only-gnomad.vcf.gz \\\n     --panel-of-normals pon.vcf.gz \\\n     -O somatic.vcf.gz\n</code></pre> <p>(ii) Tumor-only mode</p> <pre><code>    gatk Mutect2 \\\n    -R reference.fa \\\n    -I sample.bam \\\n    --germline-resource af-only-gnomad.vcf.gz \\\n    --panel-of-normals pon.vcf.gz \\\n    -O single_sample.vcf.gz\n</code></pre>"},{"location":"Methods/Sequencing/Variant_callers/Mutect2/#best-practises","title":"Best Practises","text":"<p>Mutect2 can be also used together with other GATK tools for complete workflows. Check for Best Practices Workflows</p> <p>Somatic short variant discovery workflow includes the following steps:</p> <ul> <li>Call candidate variants: Mutect2</li> <li>Calculate Contamination:</li> <li>Learn Orientation Bias Artifacts</li> <li>Filter Variants</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Mutect2/#reference","title":"Reference","text":"<ul> <li>Mutect2 official webpage</li> <li>Joan Enric</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Platinum/","title":"Platinum","text":""},{"location":"Methods/Sequencing/Variant_callers/Platinum/#description","title":"Description","text":""},{"location":"Methods/Sequencing/Variant_callers/Platinum/#reference","title":"Reference","text":""},{"location":"Methods/Sequencing/Variant_callers/Sarek/","title":"Sarek","text":"<p>Sarek is a pipeline built using Nextflow designed to detect variants in whole or targeted sequencing data. It works on any species with a reference genome and allows for germline, tumor only and tumor-normal pair variant calling</p>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#description","title":"Description","text":"<p>Sarek workflow consists of the following steps:</p> <ul> <li>Preprocessing (based on GATK4 Best Practices): includes sequencing quality control, reads alignment (BWA), mark duplicates and base recalibration.</li> <li>Variant calling. You can choose any caller, but here are some recommendations:<ul> <li>HaplotypeCaller for germline SNVs and indels.</li> <li>Mutect2 and Strelka for somatic SNVs and indels.</li> <li>Manta for structural variants (SVs).</li> <li>ASCAT for copy number variants (CNVs)</li> </ul> </li> <li>Annotation: you can choose snpEff, VEP or merge both.</li> </ul> <p>Note sarek allows to run the pipeline starting from any step.</p>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#installation","title":"Installation","text":"<p>You can follow the instructions in Sarek's webpage. Take this into account if you plan to use sarek in the cluster:</p> <ol> <li>Create a conda environment to install the latest java version (openjdk specifically).</li> <li>Preferably, use Singularity as container. It is not necessary to install it as it is already installed in the cluster.</li> <li> <p>Better download and run the pipeline test specifying the version, entering the <code>-r</code> argument:</p> <pre><code>nextflow run nf-core/sarek -r 3.1.2 -profile test,singularity\n</code></pre> </li> <li> <p>Create a config file (you can name it <code>sarek.conf</code>) to specify the executor and Singularity cache directory. In this file you can also add any changes in the configuration specification of the pipeline steps (e.g.: memory limit). Example:</p> <pre><code>executor {\n    name = 'slurm'\n    queueSize = 25\n}\n\nsingularity {\n    cacheDir = '/home/$USER/singcache'\n}\n\nprocess {\n    withName: 'NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES' {\n        cpus = 28\n        memory = 200.GB\n    }\n}\n</code></pre> </li> </ol>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#usage","title":"Usage","text":"<p>To run the pipeline you can follow the instruction in sarek's webpage, first activating the conda environment with java installed. Here is an example of a command to run the analysis from alignment generation to variant annotation, calling germline and somatic SNVs, SVs and CNVs:</p> <pre><code>conda activate java\nnextflow run nf-core/sarek -r 3.1.2 -profile singularity -c /path/to/sarek.conf \\\n    --input input.csv --genome GATK.GRCh38 --igenomes_base /path/to/igenomes/ \\\n    --tools 'haplotypecaller,strelka,ascat,manta,mutect2,vep' \n</code></pre> <p>To start the pipeline from a different step you will have to include the <code>--step</code> flag in the command. If you only want to generate the alignments, omit the <code>--tools</code> flag. To avoid pipeline freezing, it is highly recommended to use a local version of iGenomes instead of pulling it from AWS during pipeline execution.</p>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#create-inputcsv","title":"Create <code>input.csv</code>","text":"<p>Sarek uses a comma-separated samplesheet with information about the samples to run the pipeline. Here you specify information as the patient-id, sex, status (normal/tumor), etc. In sarek's webpage you can find examples of the mandatory fields required in the samplesheet depending on the pipeline step from which you start the analysis.</p>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#main-outputs","title":"Main outputs","text":"<p>If you run the entire pipeline, you will end with many files. From those, we will highlight:</p> <ul> <li>In  <code>results/preprocessing/recalibrated/</code> the final alignments in CRAM format per sample.</li> <li>In <code>results/variant_calling/</code> the VCFs per caller and sample.</li> <li>In <code>results/annotation/</code> the annotated VCFs per caller and sample.</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#links","title":"Links","text":"<ul> <li>Sarek official webpage</li> <li>As an output example, the data in <code>/data/bbg/datasets/all_aecc_pediatric</code> was generated using sarek version 3.1.1. Feel free to give it a look if needed.</li> </ul>"},{"location":"Methods/Sequencing/Variant_callers/Sarek/#reference","title":"Reference","text":"<ul> <li>Raquel Blanco</li> <li>Monica Sanchez</li> <li>Miguel Grau</li> </ul>"},{"location":"Methods/Sequencing/singleCell/ArchR_scATAC-seq/","title":"ArchR: Processing scATAC sequencing data","text":"<p>ArchR is a R package used for the analysis of scATAC sequencing data. After processing raw scRNAseq FASTQ files using CellRanger.</p>"},{"location":"Methods/Sequencing/singleCell/ArchR_scATAC-seq/#installation","title":"Installation","text":"<p>Installation requires the packages: devtools, and Bioconductor packages.</p> <ol> <li> <p>Install devtools</p> <pre><code>if (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\n</code></pre> </li> <li> <p>Install Bioconductor</p> <pre><code>if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\n</code></pre> </li> <li> <p>Install ArchR</p> <pre><code>devtools::install_github(\"GreenleafLab/ArchR\", ref=\"master\", repos = BiocManager::repositories())\n</code></pre> </li> </ol> <p>Alternatively, Seurat is a frequently used R package to analyze both scRNA and scATAC sequencing data.</p>"},{"location":"Methods/Sequencing/singleCell/ArchR_scATAC-seq/#references","title":"References","text":"<ul> <li>Article: https://www.nature.com/articles/s41588-021-00790-6</li> <li>Github page: https://github.com/GreenleafLab/ArchR</li> </ul>"},{"location":"Methods/Sequencing/singleCell/ArchR_scATAC-seq/#reference","title":"Reference","text":"<ul> <li>Axel Rosendahl Huber</li> </ul>"},{"location":"Methods/Sequencing/singleCell/CellRanger/","title":"CellRanger","text":"<p>CellRanger is a software package to perform data pre-processing for single cell data. Amongst multiple functions, the main steps the pipeline performs:</p> <ul> <li>Sample demultiplexing</li> <li>barcode processing</li> <li>single cell 3' and 5' gene counting</li> </ul>"},{"location":"Methods/Sequencing/singleCell/CellRanger/#installation","title":"installation","text":"<p>To install CellRanger, follow the instructions provided in the following link</p> <p>After running CellRanger, processed files can be used using analytical packages such as ArchR or Seurat</p>"},{"location":"Methods/Sequencing/singleCell/CellRanger/#reference","title":"Reference","text":"<ul> <li>Axel Rosendahl Huber</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Mosaic/","title":"Mosaic - python package","text":"<p>This package provides a set of tools to analyze data produced by Tapestri instruments (<code>.h5</code> files). It is NOT open source, although data can be exctrated and analysis can be held either using mosaic functions or custom functions.</p> <p>package version</p> <p>This wiki is based on Mosaic v2.2, the package is mantained and constantly updated by missionbio, therefore some things maybe be changed. If you are using a new version of the package please refer to the official documentation.</p>"},{"location":"Methods/Sequencing/singleCell/Mosaic/#installation","title":"Installation","text":"<p>The package is available in conda.</p> <pre><code>conda create --name mosaic -c missionbio -c plotly -c conda-forge missionbio.mosaic notebook\nconda activate mosaic\n</code></pre> Tip - Installing from exported environment <p>Another way of installing mosaic with some other packages used during the first analysis of the data is with the exported env: <code>/data/bbg/projects/scell_tall/mosaic180123.txt</code>.</p> <pre><code>conda create --name mosaic --file /data/bbg/projects/scell_tall/mosaic180123.txt\n</code></pre>"},{"location":"Methods/Sequencing/singleCell/Mosaic/#jupyter-tutorials","title":"Jupyter tutorials","text":"<p>How do I access a jupyter notebook? </p> <p>please, if you are having hard time accessing a Jupyter notebook, refer to this guide.</p> <p>Data accessibility</p> <p>You might need additional accessibility to run some of the analysis available in the jupyter notebooks.</p> <p>In the folder <code>/data/bbg/projects/scell_tall/LOPEBIG_44_analysis/</code> there are four notebooks that were used to do some of the initial analysis on data. In these notebooks you can find some of mosaic built-in functions and some custom functions.</p> <p>Additionally you can find some other notebooks in the section below: Link and in the folder <code>/data/bbg/projects/scell_tall/LOPEBIG_44_analysis/</code></p>"},{"location":"Methods/Sequencing/singleCell/Mosaic/#faq","title":"FAQ","text":"<p>Since there is no section with FAQ on mosaic documentation, here we collect our questions and the answers missionbio support provided. If you intend to ask more questions to their support, please update this section.</p> How <code>min_prc_cells</code> and <code>min_mut_prct_cells</code> are computed? <p>In mosaic they are calculated on the total numbers of cells in the dataset. Differently from Tapestri Insights where they are computed on the percentage of genotyped.</p> <p>??? question \"Why do I get different number of variants when uploading samples in Tapestri Insight and when loading a merged <code>.h5</code> file in Mosaic?\"     Still waiting for a response</p> How is the data filtered by the <code>ms.load(\"path/to/.h5\", raw=False, apply_filter=True)</code> function? <ul> <li>The <code>raw</code> parameter set to False discards empty barcodes</li> <li>The <code>apply_filter</code> parameter set to True loads only the variants that meets the</li> </ul> <p>Tapestri Insights advanced filters.</p> <pre><code>We have a final matrix that excludes all those cells or variants that do not meet the filters and a\n**filter layer** called `FILTER_MASK` filled with 1 and 0 that excludes the genotypes (single cell in the matrix)\nthat do not meet the filters.\n\n[![Advance Filtering Tapestri](../../../assets/images/AdvanceFilteringTapestri.png)](https://support.missionbio.com/hc/en-us/article_attachments/360074948754/Tapestri_Insights___Advanced_Filtering_Explained.pdf)\n</code></pre>"},{"location":"Methods/Sequencing/singleCell/Mosaic/#additional-resources","title":"Additional resources","text":"<p>MissionBio provides some video tutorials on their website: Mosaic tutorials</p> <p>Additionally, the company provided a personal training course, the video lessons can be found here:</p> <pre><code>/data/bbg/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-3_1-Mosaic.mp4\n\n/data/bbg/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-3_2-Mosaic.mp4\n</code></pre>"},{"location":"Methods/Sequencing/singleCell/Mosaic/#links","title":"Links","text":"<ul> <li>Mosaic documentation</li> <li>Mosaic GitHub repo</li> <li>Mosaic Jupyter tutorial notebook</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Mosaic/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> <li>Raquel Blanco</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Seurat_scRNA_seq/","title":"Seurat","text":"<p>Seurat is a widely used R package to process single-cell RNA and scATAC-seq data.</p>"},{"location":"Methods/Sequencing/singleCell/Seurat_scRNA_seq/#installation","title":"Installation","text":"<pre><code>install.packages('Seurat')\nlibrary(Seurat)\n</code></pre> <p>For further usage, please refer to the Seurat website</p>"},{"location":"Methods/Sequencing/singleCell/Seurat_scRNA_seq/#reference","title":"Reference","text":"<ul> <li>Axel Rosendahl Huber</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Tapestri_insight/","title":"Tapestri insights","text":"<p>Compatibiliy</p> <p>The software is NOT available for Linux, you need a Windows machine or a VM running windows on your Linux machine in order to use it.</p> <p>Don't panic! This software is not a mandatory step to analyse the data. Indeed, one can produce the same  analysis by using Mosaic package</p>"},{"location":"Methods/Sequencing/singleCell/Tapestri_insight/#installation","title":"Installation","text":"<p>Download and installation guide is available here.</p>"},{"location":"Methods/Sequencing/singleCell/Tapestri_insight/#usage","title":"Usage","text":"<p>Tapestri insight has a GUI and does not require command line nor programming skills to use it.</p> <p>The file type supported are <code>.loom</code> files, that can be found in the list of ouput file from the Tapestri pipeline summary page of the analysis, in the user private area.</p> <p>An optional input could be a whitelist file with known chromosomal locations with pathogenic variants that, even if they do not pass the built-in quality filters, they are still carried on for the analysis.</p>"},{"location":"Methods/Sequencing/singleCell/Tapestri_insight/#additional-resources","title":"Additional resources","text":"<p>MissionBio provided a personal training course, the video lesson on Tapestri Insights can be found here:</p> <pre><code>/data/bbg/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-2-TapestriInsight.mp4\n</code></pre>"},{"location":"Methods/Sequencing/singleCell/Tapestri_insight/#links","title":"Links","text":"<ul> <li>Tapestri Insights Demo</li> <li>Tapestri Insights: Visualize Clones </li> </ul>"},{"location":"Methods/Sequencing/singleCell/Tapestri_insight/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/","title":"Tapestri pipeline","text":"<p>Tapestri pipeline is designed for processing single-cell DNA and DNA+protein sequencing data generated on the Tapestri Platform</p>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#accessing-the-platform","title":"Accessing the platform","text":"<p>The pipeline is currently used online, accessible from the personal user portal on their website.</p>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#usage","title":"Usage","text":"<p>Clicking on the <code>Start Run</code> button here initializes the Run set-up.</p> <p></p>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#options-and-files","title":"Options and files","text":"<p>There are three different options that requires different files:</p> Options Reference Genome Panel FASTQ Notes 1. DNA v2.0.2 yes 1 DNA DNA Possibility to stop the run after cells.bam file is produced 2. DNA + Protein v2.0.2 yes 1 DNA + 1 Protein DNA + Protein The number of DNA and Protein FASTQ files must be the same 3. Merge Runs v1.0 no no no at least two .h5 files (recommended no more than 5) <ul> <li>Reference Genome. Custom reference file must be in <code>.fa.zip</code> format. As of now, there are h19 and mouse reference genomes on Tapestri portal.</li> <li>Panels. Both DNA and Protein can be either customed or taken from their Designer catalog. Custom Panels files must be in <code>zip</code> format.</li> <li>FASTQ files. They must be in <code>.fastq</code>, <code>.fastq.gz</code>, <code>.fq</code> or <code>.fq.gz</code> format. The number of files must be even.</li> <li>.h5 files . Tapestri Pipeline output file of one run. i.e. If we have multiple runs of the sample patients but with different stage of the disease we can merge the runs to produce an .h5 file with both samples.</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#output","title":"Output","text":"<p>At the end of the run we end up with a list of files. A detailed list of output files is found here. Here we present three files:</p> File Extension Notes report.html <code>.html</code> It contains the run details with a summary of the most important specs. Merge Runs and DNA only option do not produce this report. cells.loom <code>.loom</code> It consists of a numerical genotype (NGT) matrix that has 4 other layers. It contains metadata for all the variants. It's the file used as input for Tapestri Insight. dna+protein.h5 <code>.h5</code> It contsins data for one or more run. Each run contains data for one or more assays (eg. DNA assay, CNV assay, Protein assay) associated with all the barcodes. It's the file used as input for Mosaic package."},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#additional-resources","title":"Additional resources","text":"<p>MissionBio provided a personal training course, the video lesson on Tapestri Pipeline can be found here:</p> <pre><code>/data/bbg/projects/scell_tall/LOPEBIG_44_analysis/mosaic/Video_Trainings/MissionBio-1-TapestriPipeline.mp4\n</code></pre>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#links","title":"Links","text":"<ul> <li>Tapestri Pipeline user guide </li> <li>Pipeline output files overview </li> <li>Video - Tapestri Pipeline: Processing DNA + Protein sequencing data</li> <li>FAQs</li> </ul>"},{"location":"Methods/Sequencing/singleCell/Tapestri_pipeline/#reference","title":"Reference","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Methods/Signature/DeconstructSigs/","title":"DeconstructSigs","text":""},{"location":"Methods/Signature/DeconstructSigs/#description","title":"Description","text":"<p>The deconstructSigs package is an extension for R that allows to quantify presence and prevalence of known mutational signatures in individual tumor samples. It determines the linear combination of pre-defined signatures that most accurately reconstructs the mutational profile of the input tumor sample. This method uses a multiple linear regression model.</p>"},{"location":"Methods/Signature/DeconstructSigs/#getting-started","title":"Getting started","text":"<pre><code>library(deconstructSigs)\n</code></pre>"},{"location":"Methods/Signature/DeconstructSigs/#input","title":"Input","text":"<p>The input for the method is a dataframe with the mutational data containing the following columns:</p> <ul> <li>sample identifier (sample.id)</li> <li>chromosome (chr)</li> <li>base position (pos)</li> <li>reference base (ref)</li> <li>alternate base (alt)</li> </ul>"},{"location":"Methods/Signature/DeconstructSigs/#main-functions","title":"Main functions","text":""},{"location":"Methods/Signature/DeconstructSigs/#muttosigsinput","title":"<code>mut.to.sigs.input()</code>","text":"<p>Converts input dataframe to a dataframe with 3nt context mutational frequencies. The output is a data frame with n rows (corresponding to the number of samples analyzed) and 96-columns (corresponding to all possible 96 3nt mutational contexts).</p> <pre><code>sigs.input &lt;- mut.to.sigs.input(mut.ref = input_dataframe, \n                                sample.id = \"Sample\", \n                                chr = \"chr\", \n                                pos = \"pos\", \n                                ref = \"ref\", \n                                alt = \"alt\")\n</code></pre>"},{"location":"Methods/Signature/DeconstructSigs/#whichsignatures","title":"<code>whichSignatures()</code>","text":"<p>Reconstruct mutational profile of a given tumour using input set of signatures. It requires two data frames as an input. The first one containing the frequencies of mutations in the sample (created by mut.to.sigs.input() or generated by the user manually). The second one with the mutational profiles of the reference set of signatures that should be used for reconstruction. Two sets of reference signatures are supplied by the package (signatures.nature2013 and signatures.cosmic) but it is also possible to use custom reference set.</p> <pre><code>sigs.output = whichSignatures(tumor.ref = sigs.input, \n                       signatures.ref = signatures.cosmic, \n                       sample.id = 2,\n                       contexts.needed = TRUE)\n</code></pre> <p>Important!</p> <p>If the input data frame contains the raw number of mutations then it should be normalized. The minimum required normalization for the function to work is the relative frequency of the mutations not the raw counts (so that the sum in the row is equal to 1). For this you can simply set <code>contexts.needed = TRUE</code> when running <code>whichSignatures()</code>. Further normalization for the 3nt context of target region can be done using <code>tri.counts.method</code> parameter. Possible values are 'exome', 'genome', 'exome2genome' or the data frame with the corresponding numbers of contexts.</p> <p>Optional parameters for reconstruction:</p> <ul> <li><code>associated</code> -- vector of signatures (limits the reconstruction to listed signatures)</li> <li><code>signatures.limit</code> -- number (limit the number of signatures present in the reconstruction)</li> <li><code>signature.cutoff</code> -- proportion (cutoff to discard signatures with weight less than this amount)</li> </ul> <p>Main output - data frame with the weights of input reference signatures in the tumour sample.</p>"},{"location":"Methods/Signature/DeconstructSigs/#plotsignatures","title":"<code>plotSignatures()</code>","text":"<p>Visualize the result from the <code>whichSignatures()</code>.</p> <pre><code>plotSignatures(plot_example, sub = 'example')\n</code></pre>"},{"location":"Methods/Signature/DeconstructSigs/#reference","title":"Reference","text":"<p>Rosenthal, R., McGranahan, N., Herrero, J. et al. deconstructSigs: delineating mutational processes in single tumors distinguishes DNA repair deficiencies and patterns of carcinoma evolution. Genome Biol 17, 31 (2016). https://doi.org/10.1186/s13059-016-0893-4</p>"},{"location":"Methods/Signature/HDP_based_techniques/","title":"HDP-based signature extraction","text":"<p>There are different tools to extract signatures using the Hierarchical Dirichlet process principles:</p>"},{"location":"Methods/Signature/HDP_based_techniques/#msighdp","title":"mSigHDP","text":"<p>(https://github.com/steverozen/mSigHdp)</p> <p>In general we should not be using mSigHDP, from discussions we have had internally and with some other users in other labs the easiest to use Hierarchical Dirichlet process (HDP)-based method for extracting signatures is the original HDP.</p> <p>Probably this one is more efficient than the original implementation but the parameters need to be adjusted, and we as basic users probably do not have the expertise to do so in an informed enough way, so initially I would suggest to keep it simple and go to the original method, see below.</p>"},{"location":"Methods/Signature/HDP_based_techniques/#hdp","title":"HDP","text":"<p>This is the original HDP implementation for signature extraction. (see here: https://github.com/nicolaroberts/hdp)</p> <p>The easiest way to run it is through the HDP wrapper written by McGranahanLab available here: https://github.com/McGranahanLab/HDP_sigExtraction/tree/main</p> <p>There is an explanation on how to generate the inputs for this wrapper, but you still have to install the original HDP as a dependency.</p>"},{"location":"Methods/Signature/HDP_based_techniques/#installing-hdp","title":"Installing HDP","text":"<p>This tool was developed in an early R version and some default compiling mechanisms have changed since then.</p> <p>You will find installation instructions in the original repo but some problems might appear while following them.</p> <p>These are some instructions that can help troubleshoot the process of installing HDP:</p> <p>When installing it in newer R versions you would find an issue with the installation that can be solved by:</p> <p>To ensure that the compiler uses the <code>-fcommon</code> flag when building your shared object (.so file), you need to modify the compilation process in a way that this flag is passed to the C compiler. Since the file you provided is ${R_HOME}/share/make/shlib.mk, and it controls the linking of shared libraries, the best approach is to add he <code>-fcommon</code> flag to the compilation or linking stage.</p> <p>Here are the steps you should follow:</p> <ol> <li>Modify the Makeconf file (preferred) The <code>shlib.mk</code> file includes <code>${R_HOME}/etc${R_ARCH}/Makeconf</code>, which typically contains the compilation and linking flags for R packages. You can modify the Makeconf file to add the <code>-fcommon</code> flag to the <code>CFLAGS</code> or <code>LDFLAGS</code> (for linking). Locate the Makeconf file. This is typically found in: Linux/MacOS: <code>$(R_HOME)/etc/Makeconf</code> Windows: May vary depending on your setup.</li> </ol> <p>Edit the <code>Makeconf</code> file and add <code>-fcommon</code> to the <code>CFLAGS</code> variable, which controls the flags for the C compiler. For example:</p> <p><code>CFLAGS = -g -O2 -fcommon</code> Or, if you want to pass it during the linking stage (though usually CFLAGS is enough):</p> <p><code>LDFLAGS = -fcommon</code></p>"},{"location":"Methods/Signature/HDP_based_techniques/#reference","title":"Reference","text":"<p>Source:</p> <ul> <li>Result after some hours of web searching + chatGPT probably</li> </ul> <p>Contributor:</p> <ul> <li>Ferriol Calvet</li> </ul>"},{"location":"Methods/Signature/SigProfiler/","title":"SigProfiler","text":"<p>SigProfiler is a suite of tools for extracting mutational signatures developed mostly by people at the Ludmil Alexandrov's lab (see https://github.com/AlexandrovLab).</p> <p>You can check for detailed information and documentation here: https://osf.io/t6j7u/wiki/home/</p> <p>I will proceed to discuss some details on specific recommended usage that is probably not documented but that can make a difference for the final results.</p>"},{"location":"Methods/Signature/SigProfiler/#running-sigprofilerextractor","title":"Running SigProfilerExtractor","text":"<p>There are instructions on how to run it here: https://github.com/AlexandrovLab/SigProfilerExtractor</p>"},{"location":"Methods/Signature/SigProfiler/#input","title":"Input","text":"<p>You can provide your VCFs or mutation matrices to SigProfilerExtractor.</p> <p>Note that if you want to get a decomposition of the extracted signatures into known COSMIC signatures, you need to make sure that the regions you sequenced have a trinucleotide composition similar to that of the whole-genome or whole-exome, since this two are the options that the tool can assume for normalization of the counts for the posterior. Otherwise it makes no sense to use this automatic decomposition.</p>"},{"location":"Methods/Signature/SigProfiler/#how-it-works-and-what-to-take-into-account","title":"How it works and what to take into account?","text":"<ul> <li>When you run SigProfilerExtractor you get multiple different sets of extracted signatures based on the different</li> <li>range that you asked for.</li> <li>Then it automatically selects one of those sets based on its internal criteria.</li> <li>It then takes the signatures and the mutational profile of each of the provided samples and it assigns the</li> <li>extracted signatures to the samples.</li> </ul> <p>this assignment of the extracted signatures is of low quality and it should not be trusted</p> <p>Once you get the set of extracted signatures that you trust, you should use them through SigProfilerAssignment to get the proportion of mutations of each sample that correspond to each signature. Doing this in these two steps increases the accuracy of the result. To be more specific you should provide the extracted set of signatures as the signature_database option of this second tool. </p> <ul> <li>Another output that the tool provides is to do a deconstruction of those extracted signatures into the known COSMIC signatures if you asked for that. But there is no magic trick here, or at least not that I know. (look at the comment for the input in case you are interested in getting this)</li> </ul>"},{"location":"Methods/Signature/SigProfiler/#scripts-that-may-be-used-if-interested","title":"Scripts that may be used if interested","text":"<p>Full credit to @efigb for the script</p> <pre><code>#!/usr/bin/env python3\n\n\"\"\"\nRun SigProfilerExtractor with the specified parameters.\n\"\"\"\n\n# Remember to use conda environment sigproext which has SigProfilerExtractor module installed\n\nimport click\nfrom SigProfilerExtractor import sigpro as sig\n\n@click.command()\n@click.argument('input_type', default='matrix')  # When argument parameter is not needed to be specified in the command\n@click.argument('output_dir')                     \n@click.argument('input')                           \n@click.option('--ref_genome', default='GRCh38', help='Reference genome to use') # When option parameter has to be specified in the command\n@click.option('--max_sig', default=10, help='Maximum number of signatures')\n@click.option('--nmf_replicates', default=100, help='Number of NMF replicates')\n@click.option('--cpu', default=-1, help='Number of processors to be used to extract the signatures. Default value will use all available processors, which may cause a memory error.')\n\n# Run SigProfilerExtractor\ndef main(input_type, output_dir, input, ref_genome, max_sig, nmf_replicates, cpu):\n\n    sig.sigProfilerExtractor(\n        input_type=input_type,\n        output=output_dir,\n        input_data=input, \n        reference_genome=ref_genome,\n        minimum_signatures=1,\n        maximum_signatures=max_sig,\n        nmf_replicates=nmf_replicates,\n     cpu=cpu\n    )\n\nif __name__ == \"__main__\":\n    main()\n\n### EXAMPLE OF COMMAND: python3 sigprofiler_extractor2.py matrix ./output_dir count_matrix_wgs_20240821.txt --ref_genome GRCh38 --max_sig 10 --nmf_replicates 100 --cpu 10\n</code></pre>"},{"location":"Methods/Signature/SigProfiler/#reference","title":"Reference","text":"<p>Sources:</p> <ul> <li>Informal discussions with people that knows.</li> <li>Rocio Chamorro (did some tests and confirmed this)</li> <li>Elisabet Figuerola (provided the script)</li> </ul> <p>Contributors:</p> <ul> <li>Ferriol Calvet</li> </ul>"},{"location":"Methods/Signature/SigProfilerJulia/","title":"SigProfilerJulia","text":""},{"location":"Methods/Signature/SigProfilerJulia/#description","title":"Description","text":""},{"location":"Methods/Signature/SigProfilerJulia/#reference","title":"Reference","text":""},{"location":"Methods/Signature/TrinucleotideOrdering/","title":"Trinucleotide context ordering","text":"<p>Code to generate a list of 96 trinucleotide contexts with a pyrimidine reference in the most widely used ordering. This is the ordering in which trinucleotide contexts are typically displayed in the mutational profiles plots representing the single-base substitution mutational signatures by COSMIC.</p>"},{"location":"Methods/Signature/TrinucleotideOrdering/#python-code","title":"Python code","text":"<pre><code>from itertools import product\n\nsubs = [''.join(z) for z in product('CT', 'ACGT') if z[0] != z[1]]  \nflanks = [''.join(z) for z in product('ACGT', repeat=2)]  \ncontexts = sorted([(s, f) for s, f in product(subs, flanks)], key=lambda x: (x[0], x[1]))  \ncontexts_with_format = [f[0] + s[0] + f[1] + '&gt;' + s[1] for s, f in contexts]\n\n## Full list in Python format\n\nfull_list = [\n    'ACA&gt;A', 'ACC&gt;A', 'ACG&gt;A', 'ACT&gt;A', 'CCA&gt;A', 'CCC&gt;A', 'CCG&gt;A', 'CCT&gt;A', 'GCA&gt;A', 'GCC&gt;A', 'GCG&gt;A', 'GCT&gt;A', 'TCA&gt;A', 'TCC&gt;A', \n    'TCG&gt;A', 'TCT&gt;A', 'ACA&gt;G', 'ACC&gt;G', 'ACG&gt;G', 'ACT&gt;G', 'CCA&gt;G', 'CCC&gt;G', 'CCG&gt;G', 'CCT&gt;G', 'GCA&gt;G', 'GCC&gt;G', 'GCG&gt;G', 'GCT&gt;G',\n    'TCA&gt;G', 'TCC&gt;G', 'TCG&gt;G', 'TCT&gt;G', 'ACA&gt;T', 'ACC&gt;T', 'ACG&gt;T', 'ACT&gt;T', 'CCA&gt;T', 'CCC&gt;T', 'CCG&gt;T', 'CCT&gt;T', 'GCA&gt;T', 'GCC&gt;T',\n    'GCG&gt;T', 'GCT&gt;T', 'TCA&gt;T', 'TCC&gt;T', 'TCG&gt;T', 'TCT&gt;T', 'ATA&gt;A', 'ATC&gt;A', 'ATG&gt;A', 'ATT&gt;A', 'CTA&gt;A', 'CTC&gt;A', 'CTG&gt;A', 'CTT&gt;A',\n    'GTA&gt;A', 'GTC&gt;A', 'GTG&gt;A', 'GTT&gt;A', 'TTA&gt;A', 'TTC&gt;A', 'TTG&gt;A', 'TTT&gt;A', 'ATA&gt;C', 'ATC&gt;C', 'ATG&gt;C', 'ATT&gt;C', 'CTA&gt;C', 'CTC&gt;C',\n    'CTG&gt;C', 'CTT&gt;C', 'GTA&gt;C', 'GTC&gt;C', 'GTG&gt;C', 'GTT&gt;C', 'TTA&gt;C', 'TTC&gt;C', 'TTG&gt;C', 'TTT&gt;C', 'ATA&gt;G', 'ATC&gt;G', 'ATG&gt;G', 'ATT&gt;G',\n    'CTA&gt;G', 'CTC&gt;G', 'CTG&gt;G', 'CTT&gt;G', 'GTA&gt;G', 'GTC&gt;G', 'GTG&gt;G', 'GTT&gt;G', 'TTA&gt;G', 'TTC&gt;G', 'TTG&gt;G', 'TTT&gt;G']\n</code></pre>"},{"location":"Methods/Signature/TrinucleotideOrdering/#reference","title":"Reference","text":"<ul> <li>Ferran Mui\u00f1os</li> </ul>"},{"location":"Methods/Signature/mSignAct/","title":"mSigAct","text":""},{"location":"Methods/Signature/mSignAct/#description","title":"Description","text":"<p>mSigAct is a signature fitting tool (R) to evaluate the improvement of the fitting in a given sample when adding an additional signature (test signature). It was developed by Steve Rozen lab.</p>"},{"location":"Methods/Signature/mSignAct/#getting-started","title":"Getting started","text":"<p>Load the main library:</p> <pre><code>library(pbmcapply)\n</code></pre> <p>Load mSigTools and mSigAct:</p> <pre><code>setwd(\"/path/to/mSigAct\")\nsource(\"/path/to/mSigAct/mSigAct.v0.10.R\")\nsource(\"/path/to/mSigAct/mSigTools.v0.13.R\")\n</code></pre>"},{"location":"Methods/Signature/mSignAct/#input","title":"Input","text":"<p>You need to prepare several input files:</p> <ul> <li>signature_file: A matrix table with the probabilities of the signatures (from COSMIC) that you want to test. The last column is the test signature. (96 channels)</li> <li>mutation_file: A matrix table with the counts from the samples that you want to analize (96 channels)</li> </ul> <p>Format for the signature_file:</p> <pre><code>index SBS1 SBS5 SBS18 SBS40 SBS31\nACAA 0.0008861572308774719 0.01199760047990402 0.05153385774453816 0.02820375109889615 0.0095349845981014\nACCA 0.002280404612190334 0.009438112377524496 0.015810387424537924 0.013401782437064129 0.018490274115815084\nACGA 0.000177031410683197 0.001849630073985203 0.002431597559596655 0.0029203884116587504 0.001659127299040705\nACTA 0.0012802271507033455 0.006608678264347131 0.02141406904336149 0.014801968661832024 0.006276698456611824\nCCAA 0.00031205536798394046 0.007428514297140572 0.07404864996302571 0.020802766767980138 0.010694374758876832\nCCCA 0.0017903176560617096 0.006138772245550889 0.019612885665882485 0.014701955360062887 0.011593901607754323\n...\n</code></pre> <p>Format for the mutation_file:</p> <pre><code>index sample_0 sample_1 sample_2 sample_3\nACAA 1 0 1 1\nACCA 0 1 1 1\nACGA 0 0 0 0\nACTA 0 1 0 1\nCCAA 3 1 1 0\nCCCA 0 1 1 0\n...\n</code></pre> <p>Important!! The order of the rows should be this one:</p> <pre><code>['ACAA', 'ACCA', 'ACGA', 'ACTA', 'CCAA', 'CCCA', 'CCGA', 'CCTA', 'GCAA', 'GCCA', 'GCGA', 'GCTA', 'TCAA', 'TCCA', 'TCGA', 'TCTA', 'ACAG', 'ACCG', 'ACGG', 'ACTG', 'CCAG', 'CCCG', 'CCGG', 'CCTG', 'GCAG', 'GCCG', 'GCGG', 'GCTG', 'TCAG', 'TCCG', 'TCGG', 'TCTG', 'ACAT', 'ACCT', 'ACGT', 'ACTT', 'CCAT', 'CCCT', 'CCGT', 'CCTT', 'GCAT', 'GCCT', 'GCGT', 'GCTT', 'TCAT', 'TCCT', 'TCGT', 'TCTT', 'ATAA', 'ATCA', 'ATGA', 'ATTA', 'CTAA', 'CTCA', 'CTGA', 'CTTA', 'GTAA', 'GTCA', 'GTGA', 'GTTA', 'TTAA', 'TTCA', 'TTGA', 'TTTA', 'ATAC', 'ATCC', 'ATGC', 'ATTC', 'CTAC', 'CTCC', 'CTGC', 'CTTC', 'GTAC', 'GTCC', 'GTGC', 'GTTC', 'TTAC', 'TTCC', 'TTGC', 'TTTC', 'ATAG', 'ATCG', 'ATGG', 'ATTG', 'CTAG', 'CTCG', 'CTGG', 'CTTG', 'GTAG', 'GTCG', 'GTGG', 'GTTG', 'TTAG', 'TTCG', 'TTGG', 'TTTG']\n</code></pre> <p>Import the matrices:</p> <pre><code>sigs &lt;- as.matrix(read.table(signature_file, sep = '\\t', header = T, row.names = 1))\nmuts &lt;- as.matrix(read.table(mutation_file, sep = '\\t', header = T, row.names = 1))\n</code></pre> <p>Prepare some variables:</p> <pre><code>outpath &lt;- '/path/to/output/folder/'\ncores &lt;- 'cores_to_use'\n\nname_file &lt;- basename(mutation_file)\n\noutpath_name &lt;- paste(outpath, name_file, sep ='/')\ndir.create(outpath_name, showWarnings = FALSE)\n\nnames_sigs &lt;- colnames(sigs)\nprint(colnames(sigs))\ntarget_signature &lt;- names_sigs[length(names_sigs)]\n</code></pre> <p>Run the function:</p> <pre><code>mSigAct &lt;- process.one.group(muts, sigs,\n                             target.sig.name = target_signature,\n                             path.root = outpath_name,\n                             obj.fun = obj.fun.nbinom.maxlh,\n                             nbinom.size=10, ## = dispersion parameter\n                             mc.cores=cores) ## = number of cores to use\n</code></pre>"},{"location":"Methods/Signature/mSignAct/#output","title":"Output","text":"<p>Extract p-values and exposures and calculate a bonferroni adjusted p-pvalue.\\ Build the output table.\\ Save the output.</p> <pre><code>pval&lt;-mSigAct$pval\n\napval&lt;-p.adjust(pval,\"bonferroni\")\nexposure&lt;-mSigAct$exposure\ndf&lt;-t(rbind(pval,apval,exposure))\ndf&lt;-df[order(df[,1],decreasing = F),]\n\nname_outfile &lt;- paste(\"results\", name_file, \"mSigAct\", target_signature, \"tsv\", sep ='.')\nwrite.table(df, file = paste(outpath, name_outfile, sep =\"/\"), sep ='\\t')\n\nname_outfile &lt;- paste(\"results\", name_file, \"mSigAct\", target_signature, \"rds\", sep ='.')\nsaveRDS(mSigAct, file = paste(outpath, name_outfile, sep =\"\\t\"))\n</code></pre>"},{"location":"Methods/Signature/mSignAct/#reference","title":"Reference","text":"<p>Alvin W. T. Ng et al. ,Aristolochic acids and their derivatives are widely implicated in liver cancers in Taiwan and throughout Asia.Sci. Transl. Med.9,eaan6446(2017).DOI:10.1126/scitranslmed.aan6446</p> <p>BBglab membes to ask to:</p> <ul> <li>Monica</li> <li>Ferran</li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/","title":"Statistical methods: quick reference","text":""},{"location":"Methods/Statistics/QuickReferenceMethods/#description","title":"Description","text":"<p>This is a quick reference of statistical tools that are recurrently used, the bread and butter of the lab's downstream analyses when it comes to statistical testing.</p> <p>Consider adding methods that have high practical value, i.e. they address common problems and have been proven handy in our projects.</p>"},{"location":"Methods/Statistics/QuickReferenceMethods/#table-of-contents","title":"Table of contents","text":"<ul> <li>Regression Methods</li> <li>Statistical Tests</li> <li>More Examples</li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/#regression-methods","title":"Regression Methods","text":""},{"location":"Methods/Statistics/QuickReferenceMethods/#ordinary-least-squares-ols","title":"Ordinary least squares (OLS)","text":"<p>Setting: Response variable and covariates as linear predictors. It assumes dispersion is independent of the mean (homoscedasticity).</p> <p>References:</p> <ul> <li>scipy.stats.linregress</li> <li>statsmodels.regression.linear_model.OLS </li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/#robust-linear-model-irls","title":"Robust linear model (IRLS)","text":"<p>Setting: Same as OLS, but it redistributes weights across data in order to reduce the influence of outliers.</p> <p>References:</p> <ul> <li>statsmodels.RLM</li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/#linear-mixed-effects-models-lmm","title":"Linear mixed-effects models (LMM)","text":"<p>Setting: Whenever regression samples are not independent, it allows fitting specific intercept and/or slope components for the levels the categorical variables of interest.</p> <p>References:</p> <ul> <li>statsmodels.mixedlm</li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/#logistic","title":"Logistic","text":"<p>Setting: Linear model with binary response variable.</p> <p>References:</p> <ul> <li>sklearn.LogisticRegression</li> <li>statsmodels.Logit</li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/#poisson-negative-binomial","title":"Poisson / Negative Binomial","text":"<p>Setting: The response variable represents counts (integers). Assumes mean is equal to dispersion (Poisson) or fits an overdispersion parameter (Negative Binomial).</p> <p>References:</p> <ul> <li>statsmodels.GLM</li> </ul>"},{"location":"Methods/Statistics/QuickReferenceMethods/#cox-proportional-hazards","title":"Cox (proportional hazards)","text":"<p>Setting: Survival analysis. The response variable represents time-to-event values.</p> <p>References:</p> <ul> <li>statsmodels.smf</li> </ul>   ## *Statistical tests*  ### Mann-Whitney-Wilcoxon  **Setting:** Non-parametric group comparison test. It is the right test to measure distribution shifts.  **References:**  - [scipy.stats.mannwhitneyu](https://docs.scipy.org/doc/scipy-1.15.2/reference/generated/scipy.stats.mannwhitneyu.html)  **Fun note:** Note that the U-statistic resulting from the test can be converted into an AUC-ROC if divided by the product of the sample sizes of the respective samples being compared.  ### Fisher test  **Setting:** Analysis of 2x2 contingency tables  **References:**  - [scipy.stats.fisher_exact](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html)  ### Chi-squared / G-test  **Setting:** Analysis of general contingency tables. They represent both sides of the same underlying idea, but the G-test would be the more preferable in general, simply because Chi-square is obtained as quadratic approximation of the G-test likelihood ratio statistic.  **References:**  - [scipy.stats.chi2_contingency](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html)  **Fun note:** With the Python function `scipy.stats.chi2_contingency` you can compute both Chi-squared (default) and G-test (with the option `lambda_=\"log_likelihood\"`).  ## *Multiple test correction*  ### Benjamini-Hochberg FDR  **Setting:** Computes q-values: rejecting values with q-values $\\alpha$ or lower, ensures a false-discovery rate $=\\alpha$.  **References:**  - [statsmodels.stats.multitest](https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.fdrcorrection.html)    ## More examples  -"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/","title":"Advice/Tips data visualization","text":""},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#points-of-view-presentation-link","title":"Points of view presentation: link","text":"<p>Covering</p> <ul> <li>Layout</li> <li>Gestalt principles (how people organize visual information)</li> <li>Color usage</li> <li>Plot specific parts<ul> <li>Heatmaps</li> <li>Networks</li> </ul> </li> <li>Plot horrors</li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#bbgallery-of-plots-legacy-from-bbgcloud","title":"BBGallery of plots (legacy from BBGcloud)","text":"<ul> <li>BBGplots</li> <li>Matplotlib</li> <li>Bokeh</li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#other-galleries","title":"Other galleries","text":"<ul> <li>The R Graph Gallery: A collection of charts made with R, including reproducible code.</li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#other-resources","title":"Other resources","text":"<ul> <li>awesome-genome-visualization: A curated list   of tools and libraries for genome visualization, including interactive and static visualization options.</li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#color-usage","title":"Color usage","text":""},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#perception-of-color","title":"Perception of color","text":"<p>Some important considerations when selecting a color palette include:</p> <ul> <li>Does the palette retain its integrity when printed in black and white? Gradients are a good idea to satisfy this criteria, e.g. the <code>viridis</code> palettes.  </li> <li>Are people with colorblindness able to understand it? Color blindness simulators. Can help with that. Also, the <code>colorblindr</code> R package can simulate color blindness to ggplot objects.</li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#color-palettes","title":"Color palettes","text":"<ul> <li>ColoRBrewer: Classic tool for picking colorblind-safe and print-friendly palettes.</li> <li>Viridis: Perceptually uniform and colorblind-friendly.</li> <li>Color Palette Finder (Also accessible with the <code>paletteer</code> R package).</li> <li>Coolors: Nice tool to explore and create palettes (but limited to ~5 colors).</li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#creating-reusable-themes-in-r","title":"Creating reusable themes in R","text":"<p>Creating reusable themes in R is a great way to ensure consistency and save time across visualizations.</p>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#1-create-a-custom-theme-function","title":"1. Create a Custom Theme Function","text":"<pre><code>library(ggplot2)\n\ntheme_tidy &lt;- function(base_size = 12, base_family = \"sans\") {\n  theme_minimal(base_size = base_size, base_family = base_family) +\n    theme(\n      plot.title = element_text(face = \"bold\", size = base_size + 2, hjust = 0.5),\n      plot.subtitle = element_text(size = base_size),\n      plot.caption = element_text(size = base_size - 2, color = \"gray40\"),\n      axis.title = element_text(face = \"bold\"),\n      axis.text = element_text(size = base_size - 1),\n      panel.grid.major = element_line(color = \"gray85\"),\n      panel.grid.minor = element_blank(),\n      legend.position = \"bottom\",\n      legend.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\", size = base_size),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n}\n</code></pre>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#2-apply-to-any-ggplot","title":"2. Apply to any ggplot","text":"<pre><code>ggplot(mtcars, aes(x = mpg, y = hp)) +\n  geom_point(color = \"#2c7fb8\") +\n  labs(title = \"Horsepower vs. MPG\") +\n  theme_tidy()\n</code></pre>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#3-make-it-reusable-across-projects","title":"3. Make It Reusable Across Projects","text":"<p>Save theme and palette functions in a script (e.g., <code>mythemes.R</code>) and source it:</p> <pre><code>source(\"path/to/mythemes.R\")\n</code></pre>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#4-set-your-theme-globally","title":"4. Set Your Theme Globally","text":"<p>Use your theme across all plots in a session:</p> <pre><code>theme_set(theme_tidy())\n</code></pre>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#colour-resources-for-plots","title":"Colour resources for plots","text":"<ul> <li>Predefined color names<ul> <li>Matplotlib</li> </ul> </li> <li>Predefined color palettes<ul> <li>Seaborn palettes</li> <li>Seaborn palettes tutorial</li> <li>Matplotlib palettes</li> </ul> </li> <li>Generate customized palettes<ul> <li>iWantHue: generate and refine palettes of 1 to more than 30 optimally   distinct colors, with option to set up the number of colours and hue. Option to export the colour IDs from your   palettes as json or lists in different formats (rgb, hex...) and save your own palettes when registered.</li> <li>coolors: set up from 1 to 10 colour palettes searching for colours, topics,   styles, popularity, hex values... Possible to save your own palettes when registered.</li> </ul> </li> </ul>"},{"location":"Plots_and_scripts/Advice_Tips_data_visualization/#reference","title":"Reference","text":"<ul> <li>Axel Rosendahl Huber</li> <li>Elisabet Figuerola</li> <li>Laura Torrens</li> </ul>"},{"location":"Plots_and_scripts/GridSpec_for_dummies/","title":"GridSpec for dummies","text":"<p>When we want to create a figure that includes more than one plot, while having the possibility to manage each of these individually, we can normally achieve this by using the <code>pyplot.subplots</code> command. However, there are complex cases that <code>pyplot.subplots</code> cannot handle. In these cases, we should use the <code>GridSpec</code> function, also from <code>matplotlib</code>.  </p> <p>It can be difficult to generate the desired grid and trying to allocate each of the plot to its correct coordinates, specially, when the plot spans multiple rows or columns. I recommend to first draw a draft of the desired plot layout. And then divide the figure in rows (red lines) and columns (blue lines) as the example. Like this, you know the particular coordinates for a plot. Then, the coding is really easy.</p> <p></p> <pre><code>#Import necessary libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\n#Initialise figure\nfig = plt.figure(layout = \"constrained\")\n\n#Initialise Gridspec (number of rows, number of columns, figure)\ngs = GridSpec(4, 2, figure = fig)\n\n#Add grids to add the plots later\n#Plot A spans row 0 and column 0\nax_a = fig.add_subplot(gs[0, 0]) \nax_b = fig.add_subplot(gs[0, 1])\n#Plot C spans rows 1 and 2; and column 0\nax_c = fig.add_subplot(gs[1:3, 0]) #Remember the slicing rules in Python\nax_d = fig.add_subplot(gs[1, 1])\nax_e = fig.add_subplot(gs[2, 1])\n#Plot D spans row 3 and all the columns\nax_f = fig.add_subplot(gs[3, 0:])\n</code></pre> <p></p> <p>Author: Olivia Dove Estrella</p>"},{"location":"Plots_and_scripts/Heatmap/","title":"Heatmap plot","text":"<p>Example of how to generate heatmaps (or clustermaps) with multiple annotations (rows and/or columns) using Seaborn.  </p> <p>Heatmaps are pretty easy to generate, however if we are interested in adding more than two annotations (column/s and/or row/s) the process might not be so trivial (especially to add the legends). If this is your objective, here you could find a couple of useful tips. Please check Seaborn documentation if you didn't do so.</p>"},{"location":"Plots_and_scripts/Heatmap/#import-packages-and-load-toy-data","title":"Import packages and load toy data","text":"<pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom sklearn.datasets import load_iris\n\n# Load the dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Add some categorical features\ndf[\"species\"] = iris.target\ntarget_map = {0 : \"Iris Setosa\",\n              1 : \"Iris Versicolor\",\n              2 : \"Iris Virginica\"}\ndf[\"species\"] = df.species.map(target_map)\ndf[\"genus\"] = \"Iris\"\ndf[\"family\"] = \"Iridaceae\"\n</code></pre>"},{"location":"Plots_and_scripts/Heatmap/#simple-heatmap","title":"Simple heatmap","text":"<pre><code>sns.clustermap(df.drop(columns=[\"species\", \"genus\", \"family\"]),\n               yticklabels=False,\n               xticklabels=True,\n               cmap = \"RdBu_r\")\nplt.show()\n</code></pre>"},{"location":"Plots_and_scripts/Heatmap/#heatmap-with-individual-annotations","title":"Heatmap with individual annotations","text":"<p>We can add the first two annotation legends (rows and/or cols) using customized handles (custom artist objects that can be added to a legend in a Seaborn plot), <code>g.ax_col_dendrogram</code> and <code>g.ax_row_dendrogram</code>.</p> <pre><code>def get_annotation_colors(series, cmap):\n\n    lut = dict(zip(series.unique(), sns.color_palette(cmap)))\n    colors = series.map(lut)\n\n    return lut, colors\n</code></pre> <pre><code>## Annotations\n\n# Annotation row\nannotation_row = df.pop(\"species\")\nlut_row, row_colors = get_annotation_colors(annotation_row, \"tab10\")\n\n# Annotation col\nannotation_col = pd.Series([col.split(\" \")[0] for col in df.columns], name=\"Feature\")\nlut_col, col_colors = get_annotation_colors(annotation_col, \"Paired\")\n\n## Clustermap\n\ng = sns.clustermap(df,\n                   yticklabels=False,\n                   xticklabels=True,\n                   cmap = \"RdBu_r\",\n                   row_colors = [row_colors],\n                   col_colors = [col_colors])\n\n## Legends\n\n# Annotation col\nhandles = [Patch(facecolor=lut_col[name]) for name in lut_col]\ng.ax_col_dendrogram.legend(handles, lut_col, \n                           title=\"Feature type\", \n                           ncol=1,\n                           title_fontsize=12, fontsize=10,\n                           bbox_to_anchor=(1.13, .8), \n                           bbox_transform=plt.gcf().transFigure)\n\n# Annotation row\nhandles = [Patch(facecolor=lut_row[name]) for name in lut_row]\ng.ax_row_dendrogram.legend(handles, lut_row, \n                           title=\"Species\", \n                           ncol=1,\n                           title_fontsize=12, fontsize=10,\n                           bbox_to_anchor=(.01, .76), \n                           bbox_transform=plt.gcf().transFigure)\n\nplt.show()\n</code></pre> <p></p>"},{"location":"Plots_and_scripts/Heatmap/#heatmap-with-multiple-annotations","title":"Heatmap with multiple annotations","text":"<p>As shown in the following example, it is possible to assign to <code>row_colors</code> or <code>col_colors</code> (parameters of <code>sns.clustermap</code>) a dataframe including all annotations we want. Furthremore, the legend of any extra annotation other than the first two can be added using the following function.</p> <pre><code>def add_extra_legend(g, series, lut, title, bbox_to_anchor=(.01, 0.5)):\n\n    xx = []\n    for label in list(series.unique()):\n        x = g.ax_row_dendrogram.bar(0, 0, color=lut[label], label=label, linewidth=0)\n        xx.append(x)\n    legend = plt.legend(xx, \n                        series.unique(), \n                        title = title, \n                        bbox_to_anchor = bbox_to_anchor, \n                        bbox_transform = plt.gcf().transFigure)\n    plt.gca().add_artist(legend)\n</code></pre> <pre><code>## Annotations\n\n# Annotation row_1\nannotation_row1 = df.pop(\"species\")\nlut_row1, row_colors1 = get_annotation_colors(annotation_row1, \"tab10\")\n\n# Annotation row_2\nannotation_row2 = df.pop(\"genus\")\nlut_row2, row_colors2 = get_annotation_colors(annotation_row2, \"pastel\")\n\n# Annotation row_3\nannotation_row3 = df.pop(\"family\")\nlut_row3, row_colors3 = get_annotation_colors(annotation_row3, \"Set2\")\n\n# Annotation row df\nrow_colors = pd.DataFrame({\"Species\" : row_colors1,\n                           \"Genus\" : row_colors2,\n                           \"Family\" : row_colors3}) \n\n# Annotation col\nannotation_col = pd.Series([col.split(\" \")[0] for col in df.columns])\nlut_col, col_colors = get_annotation_colors(annotation_col, \"Paired\")\n\n\n## Clustermap\ng = sns.clustermap(df,\n                   yticklabels=False,\n                   xticklabels=True,\n                   cmap = \"RdBu_r\",\n                   row_colors = row_colors,\n                   col_colors = [col_colors])\n\n## Legends\n\n# Annotation col\nhandles = [Patch(facecolor=lut_col[name]) for name in lut_col]\ng.ax_col_dendrogram.legend(handles, lut_col, \n                           title=\"Feature type\", \n                           bbox_to_anchor=(1.13, .8), \n                           bbox_transform=plt.gcf().transFigure)\n\n# Annotation row 1\nhandles = [Patch(facecolor=lut_row1[name]) for name in lut_row1]\ng.ax_row_dendrogram.legend(handles, lut_row1, \n                            title=\"Species\", \n                            bbox_to_anchor=(.01, .76), \n                            bbox_transform=plt.gcf().transFigure)\n\n# Annotation row 2\nadd_extra_legend(g, annotation_row2, lut_row2, \"Genus\", bbox_to_anchor=(.01, .65))\n\n# Annotation row 3\nadd_extra_legend(g, annotation_row3, lut_row3, \"Family\", bbox_to_anchor=(.01, .58))\n\nplt.show()\n</code></pre> <p></p>"},{"location":"Plots_and_scripts/Heatmap/#reference","title":"Reference","text":"<ul> <li>Stefano Pellegrini</li> </ul>"},{"location":"Plots_and_scripts/HierarchicalClustering/","title":"Hierarchical Clustering","text":""},{"location":"Plots_and_scripts/HierarchicalClustering/#python","title":"Python","text":"<pre><code>import numpy as np\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\n\n# condensed distance array\nydist = np.array([662., 877., 255., 412., 996., 295., 468., 268., 400., 754., 564., 138., 219., 869., 669.])\n\n# linkage object\nZ = hierarchy.linkage(ydist, 'single')\n\n# compute and plot dendrogram\ndn = hierarchy.dendrogram(Z)\n</code></pre>"},{"location":"Plots_and_scripts/HierarchicalClustering/#reference","title":"Reference","text":"<ul> <li>Ferran Mui\u00f1os</li> </ul>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/","title":"IntOGen - boostDM plots","text":"<p>Examples of how to generate different types of plots found in the IntOGen + boostDM suite.</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#intogen","title":"IntOGen","text":"<p>TBC (Pending)</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#boostdm","title":"boostDM","text":"<p>All the code for figures of the boostDM paper are included in the following public repo: boostDM paper analyses repo. Find below references to the publicly available repo as well as to example scripts stored in the bbg-cluster <code>/workspace</code>.</p>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#precision-recall-curves","title":"Precision-Recall Curves","text":"<ul> <li>Public repo</li> </ul> <p>Figure 1 code, Manuscript Figure 1 PDF</p> <ul> <li>BBG Workspace</li> </ul> <p>Notebook:</p> <pre><code>/data/bbg/projects/boostdm_ch/notebooks/benchmarks-precision-recall.ipynb\n</code></pre>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#radar-plots","title":"Radar plots","text":"<ul> <li> <p>Public repo</p> <p>Figure 2 code, Manuscript Figure 2 PDF</p> </li> <li> <p>BBG Workspace</p> <p>Notebook:</p> <pre><code>/data/bbg/projects/boostdm_ch/notebooks/run8_20220705_feature-landscape/BoostDM-CH_run20220705_blueprints_SDM_JER.ipynb\n</code></pre> </li> </ul>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#blueprints","title":"Blueprints","text":"<ul> <li> <p>Public repo</p> <p>Figure 3 code, Manuscript Figure 3 PDF</p> </li> <li> <p>BBG Workspace</p> <p>Notebook:</p> <pre><code>/data/bbg/projects/boostdm_ch/notebooks/run8_20220705_feature-landscape/BoostDM-CH_run20220705_blueprints_SDM_JER.ipynb\n</code></pre> </li> </ul>"},{"location":"Plots_and_scripts/Intogen-BoostDM%20plots/#reference","title":"Reference","text":"<ul> <li>Ferran Mui\u00f1os</li> <li>Santi Demajo</li> <li>Joan Enric Ramis</li> </ul>"},{"location":"Plots_and_scripts/Mutational_profile/","title":"Mutational profile","text":"<p>The function <code>plot_signature</code>  will allow you to plot the mutational profile of a sample given the vector of 96 channels (see y axis in the example figure) with the frequencies of each nucleotide change. It takes as input the the vector with the mutations frequency  (<code>profile</code>) and the title of the plot (<code>title</code>).</p> <p>The function <code>minor_tick_labels</code>  is needed to generate the labels of the plot.</p>"},{"location":"Plots_and_scripts/Mutational_profile/#example","title":"Example","text":""},{"location":"Plots_and_scripts/Mutational_profile/#function","title":"Function","text":"<pre><code>import seaborn as sns\nimport numpy as np\n\ndef minor_tick_labels():\n    major_labels = ['C&gt;A', 'C&gt;G', 'C&gt;T', 'T&gt;A', 'T&gt;C', 'T&gt;G']\n    flanks = ['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT',\n              'GA', 'GC', 'GG', 'GT', 'TA', 'TC', 'TG', 'TT']\n    minor_labels = []\n    for subs in major_labels:\n        for flank in flanks:\n            minor_labels.append(flank[0] + subs[0] + flank[1])\n    return minor_labels\n\ndef plot_signature(profile, title=None):\n    \"\"\"\n    Args:\n        profile: 96-array in lexicographic order\n        title: string\n\n    Returns:\n        produces the signature bar plot\n    \"\"\"\n\n    fig, ax = plt.subplots(figsize=(15, 2))\n    total = np.sum(profile)\n    if abs(total - 1) &gt; 0.01:\n        profile = profile / total\n    sns.set(font_scale=1.5)\n    sns.set_style('white')\n\n    # bar plot\n    barlist = ax.bar(range(96), profile)\n    color_list = ['#72bcd4', 'k', 'r', '#7e7e7e', 'g', '#e6add8']\n    for category in range(6):\n        for i in range(16):\n            barlist[category * 16 + i].set_color(color_list[category])\n    ax.set_xlim([-0.5, 96])\n    ymax = np.max(profile) * 1.2\n    ax.set_ylim(0, ymax)\n\n    # ax.set_ylabel('subs rel freq')\n    labels = ['C&gt;A', 'C&gt;G', 'C&gt;T', 'T&gt;A', 'T&gt;C', 'T&gt;G']\n    major_ticks = np.arange(8, 8 + 16 * 5 + 1, 16)\n    minor_ticks = np.arange(0.2, 96.2, 1)\n    ax.tick_params(length=0, which='major', pad=30, labelsize=12)\n    ax.tick_params(length=0, which='minor', pad=5, labelsize=10)\n    ax.set_xticks(major_ticks, minor=False)\n    ax.set_xticklabels(labels, minor=False)\n    ax.set_xticks(minor_ticks, minor=True)\n    ax.set_xticklabels(minor_tick_labels(), minor=True, rotation=90)\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    ax.set_title(title, fontsize=24)\n    plt.show()\n</code></pre> <p>Note</p> <ul> <li>The function normalizes the vector so that the sum of all the frequencies is equal to 1.</li> <li>If you want to normalize the frequencies so that the trinucleotide composition of the genomic regions from which</li> <li>the mutations have been obtained, you need to normalize the vector taking into account the trinucleotide</li> <li>composition before using the function plot_signature.</li> </ul>"},{"location":"Plots_and_scripts/Mutational_profile/#normalization-of-the-vector","title":"Normalization of the vector","text":"<p>In order to normalize the vector you will need to import from bgreference the reference genome in which the data has been sequenced.</p> <p>You will also need the vector with the mutations frequency (<code>profile</code>) and the directory of a file with the genomic regions from which the mutations have been obtained (<code>regions_file_dir</code>), with at least the columns: <code>CHROMOSOME</code>, <code>START</code>, <code>END</code>.</p>"},{"location":"Plots_and_scripts/Mutational_profile/#needed-functions","title":"Needed functions","text":"<pre><code>from itertools import product\nimport pandas as pd\nimport numpy as np\n\ncb = dict(zip('ACGT','TGCA'))\n\ndef triplet_index(triplet):\n\n    a, ref, b = tuple(list(triplet))\n    s = 16 * (ref == 'T')\n    t = 4 * ((a == 'C') + 2 * (a == 'G') + 3 * (a == 'T'))\n    u = (b == 'C') + 2 * (b == 'G') + 3 * (b == 'T')\n    return s + t + u\n\n\ndef sbs_format(triplet_count):\n    \"\"\"Maps ref triplets to 96 SBS channel\"\"\"\n\n    vector = []\n    for ref in 'CT':\n        for alt in 'ACGT':\n            if alt != ref:\n                for a, b in product(cb, repeat=2):\n                    vector.append(triplet_count[triplet_index(a + ref + b)])\n    return vector\n\n\ndef triplets():\n\n    for ref in 'CT':\n        for a, b in product(cb, repeat=2):\n            yield a + ref + b\n\n\ndef count_triplets(seq):\n\n    return [seq.count(t) + seq.count(rev(t)) for t in triplets()]\n\ndef rev(seq):\n    \"\"\"reverse complement of seq\"\"\"\n\n    return ''.join(list(map(lambda s: cb[s], seq[::-1]))) \n\ndef get_triplet_counts_region(regions_file_dir,reference_genome=hg38):\n    \"\"\"\n    Function to obtain the vector with 96 triplet counts given the regions file.\n    \"\"\"\n    regions=pd.read_csv(regions_file_dir,sep='\\t',dtype={'CHROMOSOME':'string'})\n    assert(np.all(regions.apply(lambda r: r['END'] - r['START'] + 1 &gt; 0, axis=1)))\n    regions['interval'] = regions.apply(lambda r: (r['CHROMOSOME'], r['START'], r['END']), axis=1)\n\n    counts = np.zeros(32)\n    for chrom, start, end in list(regions['interval']):\n        try:\n            seq = reference_genome(chrom, start-1, size=end-start+3)  # sequence +1 nt 5' and 3' flanking nucleotides\n            c = np.array(count_triplets(seq))\n            counts += c\n        except Exception as e:\n            print(e)\n    return sbs_format(list(map(int, counts)))\n</code></pre>"},{"location":"Plots_and_scripts/Mutational_profile/#normalization","title":"Normalization","text":"<pre><code>region_triplet_abundance=get_triplet_counts_region(regions_file_dir)\nnormalized_profile = np.array(profile)/np.array(region_triplet_abundance)\n</code></pre>"},{"location":"Plots_and_scripts/Mutational_profile/#reference","title":"Reference","text":"<ul> <li>Paula Gomis</li> <li>Ferran Mui\u00f1os</li> </ul>"},{"location":"Plots_and_scripts/Needle_plot/","title":"Needle plot","text":""},{"location":"Plots_and_scripts/Needle_plot/#description","title":"Description","text":"<p>A needle plot displays vertical lines that connect the data points to a horizontal baseline. Needle plots are useful when you want to plot frequency of mutations on the protein body.</p> <p>The <code>needle_plot</code> function allows you to perform the needle plot with the representation of the protein body including domains. It requires an input matrix  including gene name column called <code>'gene'</code>, nucleotide position column called <code>'Protein_position'</code> and the number of mutations affecting the same position <code>'number_observed_muts'</code>. The function also needs the gene name and the transcript used.</p>"},{"location":"Plots_and_scripts/Needle_plot/#function","title":"Function","text":"<pre><code>def needle_plot(data, gene, transcript):\n\n    mpl.rcParams.update(mpl.rcParamsDefault)\n    plt.rcParams['font.sans-serif'] = ['arial']\n    plt.rcParams['font.size'] = 6\n    plt.rcParams['font.family'] = ['sans-serif']\n    plt.rcParams['svg.fonttype'] = 'none'\n    plt.rcParams['mathtext.fontset'] = 'custom'\n    plt.rcParams['mathtext.cal'] = 'arial'\n    plt.rcParams['mathtext.rm'] = 'arial'\n    mpl.rcParams['figure.dpi']= 200\n\n    # get PFAM domains and subset the mutation data\n    subset_data_pfam = get_PFAMs_per_transcript(df_pfam, df_names, transcript)\n\n    # define figure layout\n    fig = plt.figure(figsize=(8, 2.25))\n    # ! SDM change\n    gs = gridspec.GridSpec(11, 3, figure=fig)\n    ax1 = plt.subplot(gs[1:-1, :2])\n    ax2 = plt.subplot(gs[-1, :2], sharex=ax1)\n\n    # plot for each axes\n    plot_gene_full_nucleotide(subset_data_pfam, data, gene, transcript, ax1, ax2)\n    ax2.set_xlabel('CDS base position')\n    plt.show()\n</code></pre>"},{"location":"Plots_and_scripts/Needle_plot/#example","title":"Example","text":"<pre><code>data = pd.DataFrame({\n    'gene': ['TP53', 'TP53', 'TP53'],\n    'Protein_position': [45, 36, 89],\n    'number_observed_muts': [3, 5, 1]\n})\n\nneedle_plot(data, 'TP53', 'ENST00000269305')\n</code></pre>"},{"location":"Plots_and_scripts/Needle_plot/#dependencies","title":"Dependencies","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib import collections as mc\nfrom matplotlib import gridspec\nfrom collections import defaultdict\nfrom scipy.stats import norm\nimport matplotlib as mpl\nimport os\n\n### PFAM info\nPFAM_files = \"/data/bbg/projects/intogen_plus/intogen-plus-v2024/datasets/boostdm/pfam_biomart.tsv.gz\"\nPFAM_info = \"/data/bbg/projects/intogen_plus/intogen-plus-v2024/datasets/boostdm/pfam_names.info.csv\"\n\n\n### CDS coordinates\npath_coord =  \"/data/bbg/datasets/intogen/runs/v2020/20200703_oriolRun/CH_IMPACT_out/intogen_merge_20220325/cds_biomart.tsv\"\n\nBOOSTDM_DATASETS = \"/workspace/projects/intogen_plus/intogen-plus-v2024/datasets/boostdm/\"\nPFAM_file_path = os.path.join(BOOSTDM_DATASETS, 'pfam_biomart.tsv.gz')\nPFAM_info_path = os.path.join(BOOSTDM_DATASETS, 'pfam_info.name.tsv')\ndf_pfam = pd.read_csv(PFAM_file_path, sep=\"\\t\", names=[\"ENSEMBL_GENE\", \"TRANSCRIPT_ID\", \"START\", \"END\", \"DOMAIN\"])\ndf_names = pd.read_csv(PFAM_info_path, sep=\"\\t\", names=[\"DOMAIN\", \"CLAN\", \"CLAN_NAME\", \"DOMAIN_NAME\", \"Long Name\"])\n\n## Functions\n\ndef get_PFAMs_per_transcript(df_pfam, df_names, transcript):\n    # Get domains\n    df_pfam_gene = df_pfam[(df_pfam[\"TRANSCRIPT_ID\"] == transcript)]\n    df_pfam_gene = df_pfam_gene[[\"TRANSCRIPT_ID\", \"START\", \"END\", \"DOMAIN\"]].drop_duplicates()\n    df_pfam_gene = pd.merge(df_pfam_gene, df_names[[\"DOMAIN\", \"DOMAIN_NAME\"]].drop_duplicates(), how=\"left\")\n    df_pfam_gene[\"POS\"] = df_pfam_gene.apply(lambda row: row[\"START\"] + ((row[\"END\"] - row[\"START\"]) // 2), axis=1)\n    df_pfam_gene[\"SIZE\"] = df_pfam_gene.apply(lambda row: row[\"END\"] - row[\"START\"] + 1, axis=1)\n    df_pfam_gene[\"Color\"] = \"#998ec3\"\n\n    return df_pfam_gene\n\n\ndef plot_gene_full_nucleotide(subset_data_pfam, df, gene, transcript, ax1, ax2):\n\n    # plot for each axes\n    df = df[df['Protein_position'] != '-']\n    df = df[df['gene'] == gene]\n\n    # remove those mutations not falling in CDS:\n    ax1.set_title('Observed mutations')\n    ax1.set_ylabel(\"mutation count\")\n\n    ax1.spines['bottom'].set_visible(False)\n    ax1.spines['left'].set_linewidth(1)\n    ax1.spines['right'].set_visible(False)\n    ax1.spines['top'].set_visible(False)\n    ax1.tick_params(axis='y', labelsize=6, pad=0.25, width=0.25, length=1.5)\n    ax2.tick_params(axis='x', length=0)\n    ax2.set_yticks([])\n\n    # get the max_aa\n    path_coord =  \"/workspace/datasets/intogen/output/runs/v2020/20200703_oriolRun/CH_IMPACT_out/intogen_merge_20220325/cds_biomart.tsv\"\n    path_coord_gene = pd.read_csv(path_coord, sep='\\t', low_memory=False,\n                         names=['gene', 'gene_symbol', 'prot', 'chr', 's', 'e', 'aa', 'cds', 'genpos',\n                                'strand', 'transcript'])\n    path_coord_gene = path_coord_gene[path_coord_gene['transcript'] == transcript].sort_values(by='aa').reset_index(drop=True)\n    max_aa = int(path_coord_gene.loc[0,'genpos']/3)\n    ax1.set_xlim(0, max_aa)\n\n    # plot observed mutations\n    pos_list = df[\"Protein_position\"].tolist()\n    ys = df[\"number_observed_muts\"].values\n\n    coordinates_mutations = []\n    x_axis = []\n    y_axis = []\n\n    # for each of the positions\n    for i, p in enumerate(pos_list):\n        if ys[i] &gt; 0:\n            coordinates_mutations.append([(int(p), 0),\n                                      (int(p), ys[i])])\n\n            x_axis.append(int(p))\n            y_axis.append(ys[i])\n\n    lc = mc.LineCollection(coordinates_mutations, colors='black', linewidths=1, alpha=0.3)\n    ax1.add_collection(lc)\n\n    size = 12\n    ax1.scatter(x_axis, y_axis, s=size, c='red', alpha=0.7)\n\n    ax2.set_ylim(0, 1)\n\n\n    for i, r in subset_data_pfam.iterrows():\n        start_base = r['START']\n        size_base = r['SIZE']\n        rect = patches.Rectangle(xy=(start_base, 0), width=size_base, height=5, color=r[\"Color\"], alpha=0.5, zorder=2)\n        ax2.annotate(text=r[\"DOMAIN_NAME\"], xy=(start_base + 1, 0.3), fontsize=7)\n        ax2.add_patch(rect)\n        ax2.set_xticks(np.append(np.arange(0, max_aa, 100)[:-1], max_aa))\n        ax2.set_xticklabels(np.append(np.arange(0, max_aa, 100)[:-1], max_aa), fontsize = 6)\n        ax2.set_xlim(0, max_aa)\n        ax2.set_yticks([])\n        ax2.tick_params(axis='x', which='major', pad=3)\n\n    ax1.tick_params(axis='x', labelsize=0, color='w')\n\n    ax2.set_xlabel('CDS base position')\n\n    plt.show()\n</code></pre>"},{"location":"Plots_and_scripts/Needle_plot/#reference","title":"Reference","text":"<p>Joan Enric</p>"},{"location":"Plots_and_scripts/Streamlit/","title":"Streamlit data visualization","text":""},{"location":"Plots_and_scripts/Streamlit/#description","title":"Description","text":"<p>Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps.</p>"},{"location":"Plots_and_scripts/Streamlit/#installation","title":"Installation","text":"<p>Streamlit can be installed with <code>conda</code> (or <code>[micro]mamba</code>) and <code>pip</code>, as follows:</p> CondaMamba <p>Create environment and activate: </p> <pre><code># Create ENV\nconda create --name streamlit_app python=3.5 pip\n\n# Activate env\nconda activate strealit_app\n</code></pre> <p>Create environment and activate: </p> <pre><code># Create ENV\nmamba create --name streamlit_app python=3.5 pip\n\n# Activate env\nmamba activate strealit_app\n</code></pre> <p>Download package:</p> <pre><code>pip install streamlit\n</code></pre>"},{"location":"Plots_and_scripts/Streamlit/#demo","title":"Demo","text":"<pre><code>streamlit hello\n</code></pre>"},{"location":"Plots_and_scripts/Streamlit/#key-concepts","title":"Key concepts","text":""},{"location":"Plots_and_scripts/Streamlit/#running-the-app","title":"Running the app","text":"<p>Streamlit creates a local server as soon as you run the script with the following command:</p> <pre><code>streamlit run your_script.py [-- script args]\n</code></pre> <p>This command will open a new tab in your default web browser.</p>"},{"location":"Plots_and_scripts/Streamlit/#build-the-app","title":"Build the app","text":"<p>Let's imagine that your_script.py is like a canvas. In this canvas you can build the components as you prefer.</p> Components LayoutTextData and ChartsInputMessage status <p>These components are necessary to structure the app. We have 5 layout components:</p> <ul> <li> <p><code>st.container</code>: It divides the canvas in containers</p> <pre><code>import streamlit as st\n\ncont1 = st.container()\n\nwith cont1:\n    st.write(\"This is a text on container 1\")\n\n## is equivalent to: \n\ncont1.write(\"This is another text on container 1\")\n</code></pre> </li> <li> <p><code>st.sidebar</code> : It allows to add object to the sidebar.</p> <pre><code>import streamlit as st\n\nwith st.sidebar:\n    st.image('url/to/image.png')\n\n## is equivalent to:\n\nst.sidebar.image(\"url/to/image.png\")\n</code></pre> </li> <li> <p><code>st.columns</code> : It divides the container or the canvas in columns.</p> <pre><code>import steamlit st\n\ncol1, col2 = st.columns(2)\n\n## or you can specify the ratio and the gap\n\ncol1, col2 = st.columns([0.3, 0.7], gap='small')\n\nwith col1:\n    st.write(\"this is column 1\")\n\ncol2.write(\"this is column 2\")\n</code></pre> </li> <li> <p><code>st.expander</code> : it adds an expander in the canva</p> <pre><code>import streamlit as st\n\nwith st.expander(\"This is the title of the expander\", expanded=True)\n    st.write(\"this is the content of the expander\")\n</code></pre> </li> </ul> <p>In Streamlit there are several ways of writing text in the app:</p> <ul> <li><code>st.text</code> : Write fixed-width and preformatted text.</li> <li> <p><code>st.markdown</code> :  string formatted as Markdown, it can also read html string (although it is considered unsafe)</p> <pre><code>import streamlit as st\n\nst.markdown(\n        \"\"\"\n            &lt;h2&gt;This is heading 2&lt;/h2&gt;\n            &lt;p&gt; \n            This is paragraph 1 \n            &lt;/p&gt;\n        \"\"\",\n        allow_unsafe_html=True\n    )\n</code></pre> </li> <li> <p><code>st.code</code> : renders code as text</p> </li> <li><code>st.latex</code> : reads latex syntax, useful for math expressions.</li> <li><code>st.divider</code> : adds a horizontal line that divides the area of the container/column etc</li> <li><code>st.title</code>, <code>st.header</code>, <code>st.caption</code>, <code>st.subheader</code> : self-explanatory. </li> </ul> <p>Streamlit is optimize for visualize Data quickly, interactively, and from multiple different angles.  You can use several functions:</p> <ul> <li>Data<ul> <li><code>st.dataframe</code> : it reads and renders a Pandas dataframe</li> <li><code>st.metric</code> : it returns a KPI specified by the user</li> <li><code>st.table</code> : it returns a table</li> <li><code>st.json</code> : it returns a box with a json that can be copied by the user at certain level.</li> </ul> </li> </ul> <p>Streamlite has also built in wrapper for several package used to plot dynamically the data, some of them are: </p> <ul> <li>Charts<ul> <li><code>st.line_chart</code>, <code>st.area_chart</code>, <code>st.bar_chart</code></li> <li><code>st.pyplot</code> : wrapper of matplotlib pyplot. </li> <li><code>st.plotly</code> : wrapper of plotly, create interactive and dynamic plots.</li> </ul> </li> </ul> <p>Streamlit let you go through your data dynamically. This can be done through the use of button, selectors, several types of inputs or via file uploader. </p> <ul> <li><code>st.button</code>: Simple button that return a Boolean (False, True) if clicked or not - can be used as a condition in if statements.</li> <li><code>st.selectbox</code>: Dropdown menu that can be selected. Returns the string of the option selected.</li> <li><code>st.slider</code>, <code>st.checkbox</code>.</li> </ul> <p>Streamlit allows to return messages to the user in some specific cases. i.e. if the user needs to produce a plot by selecting some data, the message should appear only if the selection is not made.</p> <pre><code>import streamlit as st\n\nselection = st.selectbox(\"This is the title of the selectbox\",  (\"Select Option\", \"Option0\", \"option1\", \"option2\"))\n\nif selection == \"Select Option\":\n\n    st.info(\"please select an option\")\nelse:\n    st.markdown(\n        \"\"\"\n        You selected {option}\n        \"\"\".format(option=selection)\n        )\n</code></pre> <p>Types of message statuses:</p> <ul> <li><code>st.info</code> : Returns a blue box with some info.</li> <li><code>st.error</code> : Returns a red box with error info.</li> <li><code>st.warning</code> : Returns a yellow box.</li> <li><code>st.success</code>: Returns a green box.</li> </ul>"},{"location":"Plots_and_scripts/Streamlit/#examples","title":"Examples","text":"<ul> <li>Gallery</li> </ul>"},{"location":"Plots_and_scripts/Streamlit/#documentation","title":"Documentation","text":"<ul> <li>Streamlit Documentation</li> </ul>"},{"location":"Plots_and_scripts/Streamlit/#source","title":"Source","text":"<ul> <li>Federica Brando</li> </ul>"},{"location":"Plots_and_scripts/compute_type_variant/","title":"Compute type of variant","text":""},{"location":"Plots_and_scripts/compute_type_variant/#function-to-compute-the-type-of-variant-starting-from-the-ref-and-alt-columns","title":"Function to compute the type of variant starting from the REF and ALT columns","text":"<p>Function used in PROMINENT and in the outputs of the deepUMIcaller pipeline by consensus, to determine the variant type. (Designed for variants from VarDict and Ensembl-like format).</p> <pre><code>def vartype(x, letters = ['A', 'T', 'C', 'G'], len_SV_lim = 100):\n    if \"&gt;\" in (x[\"REF\"] + x[\"ALT\"]) or \"&lt;\" in (x[\"REF\"] + x[\"ALT\"]):\n        return \"SV\"\n\n    elif len(x[\"REF\"]) &gt; (len_SV_lim+1) or len(x[\"ALT\"]) &gt; (len_SV_lim+1) :\n        return \"SV\"\n\n    elif x[\"REF\"] in letters and x[\"ALT\"] in letters:\n        return \"SNV\"\n\n    elif len(x[\"REF\"]) == len(x[\"ALT\"]):\n        return \"MNV\"\n\n    elif x[\"REF\"] == \"-\" or ( len(x[\"REF\"]) == 1 and x[\"ALT\"].startswith(x[\"REF\"]) ):\n        return \"INSERTION\"\n\n    elif x[\"ALT\"] == \"-\" or ( len(x[\"ALT\"]) == 1 and x[\"REF\"].startswith(x[\"ALT\"]) ):\n        return \"DELETION\"\n\n    return \"COMPLEX\"\n</code></pre> <p>Open to suggestions to accommodate how other callers may report the variants.</p>"},{"location":"Plots_and_scripts/compute_type_variant/#reference","title":"Reference","text":"<ul> <li>Ferriol Calvet</li> <li>Paula Gomis</li> </ul>"},{"location":"Plots_and_scripts/minibam_hartwig_GCP/","title":"Extract minibams from Hartwig data in googleCloud","text":""},{"location":"Plots_and_scripts/minibam_hartwig_GCP/#1-create-the-vm-and-the-storage-bucket","title":"1. Create the VM and the storage bucket","text":"<p>(This tutorial assumes that you already have the credentials to access the data and use the GCP)</p> <p>First, from Google Cloud Platform website, you create a VM (Compute Engine -&gt; VM instances. Create Instance). In this case we choose an e2-highmem8 (8vCPU, 4 core, 64GB RAM). Region: europe-west3 (Frankfurt) . In Availability policies - VM,  Provisioning model: SPOT. Attach a disk of 250GB, persistent (Advanced options - Disks - Add new disk - Edit Name and Size). Then, we can connect using a ssh web terminal or a local one.</p> <p>Same region VM</p> <p>It's important to create the VM in the same region where the data is, so we avoid charges for moving data between regions, if any.</p> <p>Next, prepare the VM:</p> <pre><code>screen -R mysession\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-py39_22.11.1-1-Linux-x86_64.sh\nchmod 755 Miniconda3-py39_22.11.1-1-Linux-x86_64.sh\nbash Miniconda3-py39_22.11.1-1-Linux-x86_64.sh \neval \"$(/home/miguel_grau/miniconda3/bin/conda shell.bash hook)\"\nconda create -n samtools -y\nconda activate samtools\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda install samtools==1.11  -y\nconda install -c anaconda python  -y\nconda install -c anaconda python=3 -y\ngcloud auth login\ngcloud config set project instant-carrier-264511\n\n# if the persistent disk doesn't appear automatically with a df -H:\n# Replace google-disk-1 with your disk name\nmkdir -p /ext/ssd/\nsudo mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/disk/by-id/google-disk-1\nsudo mount -o discard,defaults /dev/disk/by-id/google-disk-1 /ext/ssd/\nsudo chmod a+w /ext/ssd/\n</code></pre> <p>Next, create a bucket where the results will be copied during the minibam generation (Cloud Storage - Buckets). Location type: Region, europe-west4 (Netherlands)</p>"},{"location":"Plots_and_scripts/minibam_hartwig_GCP/#2-prepare-the-input-file","title":"2. Prepare the input file","text":"<p>Our input file is a ~bed file with samples IDs and regions (IDs_regions.csv), e.g.:</p> <pre><code>CPCT02010702    3:43228348-43228748\nCPCT02010702    14:101303564-101303964\nCPCT02010702    19:18451144-18451544\nCPCT02010728    2:176568493-176568893\n</code></pre> <p>The regions file must be sort by sampleID.</p> <p>We need to extract the CRAM URLs from the manifest.json of the hartwig release of interest, in this case 20230914v:</p> <pre><code>#/data/bbg/datasets/hartwig/20230914/scripts/minibam/extract_urls.py\n\nimport json\nimport csv\n\ndef extract_cram_url(json_data, sample_id):\n    for item in json_data[\"data\"]:\n        if item[\"sampleId\"] == sample_id:\n            return item[\"crams\"][\"tumor\"][\"url\"]\n    return None\n\ndef process_manifest(json_file_path, txt_file_path, csv_output_path):\n    with open(json_file_path, 'r') as json_file:\n        manifest_data = json.load(json_file)\n\n    with open(txt_file_path, 'r') as txt_file, open(csv_output_path, 'w', newline='') as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter='\\t')\n        csv_writer.writerow([\"SampleID\", \"GenomicRegion\", \"Tumor_CRAM_URL\"])\n\n        for line in txt_file:\n            parts = line.strip().split()\n            sample_id, genomic_region = parts[0]+\"T\", parts[1]\n\n            cram_url = extract_cram_url(manifest_data, sample_id)\n\n            if cram_url is not None:\n                csv_writer.writerow([sample_id, genomic_region, cram_url])\n            else:\n                print(f\"Warning: No CRAM URL found for sample {sample_id}\")\n\nif __name__ == \"__main__\":\n    json_file_path = \"/data/bbg/datasets/hartwig/20230914/manifest.json\"\n    txt_file_path = \"/data/bbg/datasets/hartwig/20230914/scripts/minibam/IDs_regions.csv\"\n    csv_output_path = \"IDs_regions_url.csv\"\n\n    process_manifest(json_file_path, txt_file_path, csv_output_path)\n</code></pre> <p>So the output is a file including the urls:</p> <pre><code>SampleID        GenomicRegion   Tumor_CRAM_URL\nCPCT02010702T   3:43228348-43228748     gs://example_url/CPCT02010702T/cram/CPCT02010702T_dedup.realigned.cram\nCPCT02010702T   14:101303564-101303964  gs://example_url/CPCT02010702T/cram/CPCT02010702T_dedup.realigned.cram\nCPCT02010702T   19:18451144-18451544    gs://example_url/CPCT02010702T/cram/CPCT02010702T_dedup.realigned.cram\n</code></pre>"},{"location":"Plots_and_scripts/minibam_hartwig_GCP/#3-create-minibams","title":"3. Create minibams","text":"<p>For each line, it extracts the regions in a minibam. If the next region is from the same sample, it keeps the cram. If not, it deletes the cram and downloads the next one.</p> <pre><code>#/data/bbg/datasets/hartwig/20230914/scripts/minibam/dwnRunMiniBam_multi.py\n\nimport subprocess\n\ndef execute_command(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    output, error = process.communicate()\n    return output, error, process.returncode\n\ndef process_web_addresses(file_path, bucket_name):\n    previous_file = \"\"\n    regions = \"\"\n    with open(file_path, 'r') as file:\n        next(file)\n        for line in file:\n            web_address = line.strip().split(\"\\t\")[2]\n            file_name = web_address.split(\"/\")[-1]\n            region = line.split(\"\\t\")[1]\n\n            if file_name != previous_file:\n              # Delete previous crams\n              if previous_file != \"\":\n                ofile_name = previous_file.split('.cram')[0]+\".mini.bam\" \n                # Process the file with samtools\n                samtools_command = f\"samtools view -b /ext/ssd/{previous_file} {regions} &gt; /ext/ssd/{ofile_name}\"\n                print (\"Running samtools... \")\n                execute_command(samtools_command)\n                print (samtools_command)\n\n                # Upload the results to the Google Cloud Storage bucket\n                upload_command = f\"gsutil cp /ext/ssd/{ofile_name} gs://{bucket_name}/\"\n                print (\"Copying results... \")\n                execute_command(upload_command)\n                print (upload_command)\n\n                del_command = f\"rm /ext/ssd/{previous_file} /ext/ssd/{previous_file}.crai\"\n                execute_command(del_command)\n                print (del_command)\n                regions = \"\"\n\n              # Download the new crams using gsutil\n              download_command = f\"gsutil -u instant-carrier-264511 cp {web_address} /ext/ssd/\"\n              print (\"Downloading \"+file_name)\n              execute_command(download_command)\n              print (download_command)\n              download_icommand = f\"gsutil -u instant-carrier-264511 cp {web_address}.crai /ext/ssd/\"\n              execute_command(download_icommand)\n              print (download_icommand)\n\n            regions += region if regions == \"\" else \" \"+region\n            previous_file = file_name\n\n        #Last extraction\n        # Delete previous crams\n        if previous_file != \"\":\n            ofile_name = previous_file.split('.cram')[0]+\".mini.bam\"\n            # Process the file with samtools\n            samtools_command = f\"samtools view -b /ext/ssd/{previous_file} {regions} &gt; /ext/ssd/{ofile_name}\"\n            print (\"Running samtools... \")\n            execute_command(samtools_command)\n            print (samtools_command)\n\n            # Upload the results to the Google Cloud Storage bucket\n            upload_command = f\"gsutil cp /ext/ssd/{ofile_name} gs://{bucket_name}/\"\n            print (\"Copying results... \")\n            execute_command(upload_command)\n            print (upload_command)\n\n            del_command = f\"rm /ext/ssd/{previous_file} /ext/ssd/{previous_file}.crai\"\n            execute_command(del_command)\n            print (del_command)\n\nif __name__ == \"__main__\":\n    txt_file_path = \"IDs_regions_url.csv\"  # Replace with the actual path to your text file\n    gcs_bucket_name = \"masha_bkt\"  # Replace with the actual name of your GCS bucket\n\n    process_web_addresses(txt_file_path, gcs_bucket_name)\n</code></pre> <p>The script extracts the minibams one by one, deleting a cram before download a new one (that's why our persistent disk is only 250GB). It process ~100 samples per day, so this approach is only valid for small datasets. If you plan to do it for thousands of samples, contact us.</p>"},{"location":"Plots_and_scripts/minibam_hartwig_GCP/#reference","title":"Reference","text":"<ul> <li>Miguel</li> <li>Federica</li> <li>Carlos</li> </ul>"},{"location":"Tools/Conda/","title":"Conda and mamba","text":""},{"location":"Tools/Conda/#description","title":"Description","text":"<p>Conda and mamba are open source package manager and environment management systems for Python and other languages. They run on Windows, macOS, Linux and z/OS. These tools automate the process of installing, updating, configuring and removing software packages. They manage dependencies, ensuring that all required libraries and components are correctly installed and compatible. In our case, using package managers is specially useful because:</p> <ul> <li>They provide a way to ensure reproducibility in our code</li> <li>They allow a project's software tools to be portable</li> <li>They allow user-level installation (i.e. do not require sudo permissions)</li> </ul> <p>The difference between the two is that mamba is a fast, drop-in replacement for conda that highly improves package resolution speed and efficiency (and we recommend you use it instead of conda to make your life happier :)).</p> <p>Info</p> <p>The majority of the commands shown below are also supported by mamba</p>"},{"location":"Tools/Conda/#installation","title":"Installation","text":""},{"location":"Tools/Conda/#conda","title":"Conda","text":"<p>Conda has two possible installers:</p> <ul> <li>Anaconda: already comes with installed packages for data science and Anaconda Navigator (GUI application). Follow the instructions.</li> <li>Miniconda (recommended): minimal installer provided by Anaconda  </li> </ul> <p>Download any of the two installers and in your terminal run:</p> <pre><code>bash &lt;conda-installer-name&gt;-latest-Linux-x86_64.sh&gt;\n</code></pre> <p>Detailed instructions here. To update conda, run:</p> <pre><code>conda update conda\n</code></pre>"},{"location":"Tools/Conda/#mamba","title":"Mamba","text":"<p>The installer for mamba is micromamba. To install it run:</p> <pre><code>\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\n</code></pre> <p>To update mamba:</p> <pre><code>micromamba self-update\n</code></pre>"},{"location":"Tools/Conda/#get-started","title":"Get started","text":"<pre><code>conda create -n &lt;env name&gt; &lt;package[=&lt;version&gt;]&gt;\nconda activate &lt;env name&gt;\nconda install &lt;package[=version]&gt;\n</code></pre>"},{"location":"Tools/Conda/#cheatsheet","title":"Cheatsheet","text":""},{"location":"Tools/Conda/#environments","title":"Environments","text":"<p>Activate an environment:</p> <pre><code>conda activate &lt;environment name&gt;\n</code></pre> <p>Deactivates an environment. If in <code>base</code>, closes conda.</p> <pre><code>conda deactivate\n</code></pre> <p>List all environments:</p> <pre><code>conda env list\n</code></pre> <pre><code>conda info --envs\n</code></pre> <p>Create a new virtual environment with <code>&lt;packages&gt;</code></p> <pre><code>conda create --name &lt;environment name&gt; [&lt;packages[=&lt;version&gt;]&gt;]\n</code></pre> <p>Export active environment to a file</p> <pre><code>conda env export &gt; environment.yml\n</code></pre> <p>Export all environments to its own file:</p> <pre><code>for env in $(conda env list | cut -d\" \" -f1); do \n   if [[ ${env:0:1} == \"#\" ]] ; then continue; fi;\n   conda env export -n $env &gt; ${env}.yml\ndone\n</code></pre> <p>Create environment from file</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Clone an environment</p> <pre><code>conda create --name &lt;environment name&gt; --clone &lt;original environment&gt;\n</code></pre> <p>Remove an environment</p> <pre><code>conda env remove --name &lt;environment name&gt;\n</code></pre> <pre><code>conda remove --name &lt;environment name&gt; --all\n</code></pre> <p>List all packages installed (in current environment)</p> <pre><code>conda list\n</code></pre> <p>List all packages installed with path</p> <pre><code>conda list --explicit\n</code></pre> <p>Show history of changes in packages</p> <pre><code>conda list --revisions\n</code></pre>"},{"location":"Tools/Conda/#packages","title":"Packages","text":"<p>Install a package (use <code>-f</code> to force the installation)</p> <pre><code>conda install &lt;package&gt;\n</code></pre> <p>Install package(s) specified in a file (like a Python requirements file)</p> <pre><code>conda install --file &lt;file&gt;\n</code></pre> <p>Uninstall a package</p> <pre><code>conda remove &lt;package&gt;\n</code></pre> <p>Search for a package</p> <pre><code>conda search &lt;package&gt;\n</code></pre>"},{"location":"Tools/Conda/#configuration","title":"Configuration","text":"<p>Show configuration</p> <pre><code>conda config --show\n</code></pre> <p>Add channels (use <code>add</code> instead of <code>append</code> to put the channel on the top of the list)</p> <pre><code>conda config --append channels &lt;channel name&gt;\n</code></pre>"},{"location":"Tools/Conda/#building-packages","title":"Building Packages","text":"<p>Install conda build</p> <pre><code>conda install conda-build\n</code></pre> <p>Build package</p> <pre><code>conda build &lt;directory with the files&gt;\n</code></pre> <p>Build for other platforms</p> <pre><code>conda convert --platform all &lt;path to package&gt;\n</code></pre> <p>Install built package</p> <pre><code>conda install --use-local &lt;package&gt;\n</code></pre>"},{"location":"Tools/Conda/#from-pypi","title":"From Pypi","text":"<p>Create files</p> <pre><code>conda skeleton pypi &lt;package&gt;\n</code></pre> <p>Build for different Python version</p> <pre><code>conda build --python &lt;version&gt; &lt;directory with the files&gt;\n</code></pre>"},{"location":"Tools/Conda/#custom-channel","title":"Custom Channel","text":"<p>Add channel</p> <pre><code>conda config --append channels file://&lt;path to folder&gt;\n</code></pre> <p>(re)build the index</p> <pre><code>conda index &lt;channel folder&gt;/&lt;platform&gt;\n</code></pre>"},{"location":"Tools/Conda/#reference","title":"Reference","text":"<ul> <li>Jordi Deu Pons</li> <li>Miguel Grau</li> <li>Federica Brando</li> <li>Carlos L\u00f3pez-Elorduy</li> <li>Raquel Blanco</li> </ul>"},{"location":"Tools/Docker/","title":"Docker","text":""},{"location":"Tools/Docker/#description","title":"Description","text":""},{"location":"Tools/Docker/#reference","title":"Reference","text":""},{"location":"Tools/GitHub_Copilot/","title":"GitHub Copilot","text":""},{"location":"Tools/GitHub_Copilot/#features","title":"Features","text":"<p>Here's the official video from VSCode explaining the basic features of GitHub Copilot:</p>"},{"location":"Tools/GitHub_Copilot/#ghost-text","title":"Ghost Text","text":"<ul> <li>Code as you normally would, and the ghost text will start to appear.</li> <li>You can also trigger the ghost text by writting a comment about what you want to do.</li> <li>Hit <code>Tab</code> to accept the ghost text or <code>Ctrl + Right Arrow</code> to accept one word at a time.</li> <li>Hit <code>Esc</code> to dismiss the ghost text.</li> </ul>"},{"location":"Tools/GitHub_Copilot/#code-suggestions","title":"Code Suggestions","text":"<ul> <li>If you want to see more suggestions, you can hit <code>Ctrl + Enter</code> for Copilot to generate several suggestions in a</li> <li>separate window for you to accept whichever one you want to use.</li> </ul> <p>Tip</p> <p>If you are using a Jupyter Notebook, the <code>Ctrl + Enter</code> command will run the cell, so if you want to open the suggestions panel, you can change the keybinding in the settings. Go to <code>File &gt; Preferences &gt; Keyboard Shortcuts</code>, search for <code>GitHub Copilot: Open Completions Panel</code> and change the keybinding to whatever you want (for example, <code>Ctrl + K</code>, <code>Ctrl + Enter</code>).</p>"},{"location":"Tools/GitHub_Copilot/#inline-chat","title":"Inline Chat","text":"<ul> <li>Triggered by <code>Ctrl + I</code>.</li> <li>The generated code doesn't exist in your file until you hit accept.</li> <li>Highlight the part of the code you want to discuss or change, and then hit <code>Ctrl + I</code> to ask Copilot for what you want to do.</li> </ul>"},{"location":"Tools/GitHub_Copilot/#chat-sidebar","title":"Chat Sidebar","text":"<ul> <li>Classic chat interface.</li> <li>In the side bar, search for the chat icon.</li> </ul> <ul> <li>You can ask for help, ask for explanations, or ask for more suggestions.</li> <li>When you get code suggested, you also get a small panel to quickly insert the code into your file, copy the content, and more options.</li> </ul>"},{"location":"Tools/GitHub_Copilot/#code-refactoring","title":"Code Refactoring","text":"<p>Copilot is very useful even if you are not actively interacting with it. Just by typing and accepting the ghost suggestions, you will see your coding performance improving. Having said that, there are more ways you can use Copilot to refactor your code.</p>"},{"location":"Tools/GitHub_Copilot/#improve-code","title":"Improve Code","text":"<p>You can ask Copilot to improve your code, for example, by making it more efficient, more readable, or by adding documentation and comments to it.</p> <ul> <li>Efficiency: Copilot can suggest ways to make your code more efficient, for example, by using a more efficient algorithm or by reducing the number of lines of code.</li> <li>Readability: Copilot can suggest ways to make your code more readable, for example, by using more descriptive variable names or by breaking down complex code into smaller, more manageable pieces.</li> <li>Documentation: Copilot can suggest ways to add documentation and comments to your code, for example, by adding comments to explain what a particular piece of code does or by adding documentation to explain how a particular function works.</li> </ul>"},{"location":"Tools/GitHub_Copilot/#rename-variables","title":"Rename Variables","text":"<p>By typing <code>F2</code> when having your cursor on top of a variable or function name, you can rename it accross the entire file. This is integrated in VSCode by default, but with Github Copilot, you get access to suggestions for the new names.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#git-integration","title":"Git Integration","text":""},{"location":"Tools/GitHub_Copilot/#commits","title":"Commits","text":"<p>If you go to the source control tab in VSCode, you will see a sparkle \u2728 icon that when clicked will understand the changes you made and suggest a commit message for you.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#pull-requests","title":"Pull Requests","text":"<p>Github Copilot can also help you create a pull request. After clicking the icon to create a pull request next to your branch, you will see the prompt to add a title and the pull request body. Next to the title box, you will see the same sparkle \u2728 icon that will suggest both the title and the body of the pull request.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#useful-commands","title":"Useful Commands","text":""},{"location":"Tools/GitHub_Copilot/#fix","title":"<code>/fix</code>","text":"<p>Fix the selected code. You can also give the error message you are getting. This command is more powerful than not using it, even though sometimes there might be no difference.</p> <p> </p>"},{"location":"Tools/GitHub_Copilot/#explain","title":"<code>/explain</code>","text":"<p>Explain the selected code.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#tests","title":"<code>/tests</code>","text":"<p>The command <code>/tests</code> will generate a test for the selected code. This makes writting tests much easier and faster.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#new","title":"<code>/new</code>","text":"<p>Create any file or even workspace based on your instructions. This also includes the content of the file.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#newnotebook","title":"<code>/newNotebook</code>","text":"<p>Create a Jupyter Notebook based on the instructions you provide. It works better than the <code>/new</code> since this one is specifically designed for Jupyter Notebooks, which turns more powerful for this usecase.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#terminal","title":"<code>@terminal</code>","text":"<p>Ask how to do something in the terminal.</p>"},{"location":"Tools/GitHub_Copilot/#manage-context","title":"Manage Context","text":"<p>Normally, if you want to provide context for your query, you simply need to highlight the code you want to use as context. However, you can also use the following commands to provide context:</p>"},{"location":"Tools/GitHub_Copilot/#workspace","title":"<code>@workspace</code>","text":"<p>By adding this command next to your query, you are telling Copilot to look at the entire workspace to give you a better suggestion. The workspace corresponds to the root folder that you have opened in VSCode.</p> <p></p> <p>Tip</p> <p>Try to open VSCode in the root folder of your project, instead of a very general folder (such as <code>/workspace</code>, <code>/data/bbg/projects</code>, etc), to get more accurate suggestions, since Copilot won't have to look through a lot of files to give you a suggestion.</p>"},{"location":"Tools/GitHub_Copilot/#file","title":"<code>#file</code>","text":"<p>If you want to use one or more specific files as context for your query, you can add the <code>#file</code> command followed by the name of the file you want to use. This can be done as many times as you want, with different files.</p> <p></p> <p>Warning</p> <p>If the file you are using as context is too big, Copilot will highlight that file indicating that it couldn't use the whole content of the file, but still attempt to give you a suggestion based on the part of the file it could read.</p>"},{"location":"Tools/GitHub_Copilot/#terminalselection","title":"<code>#terminalSelection</code>","text":"<p>Similarly to the way that you can just select part of your code to use it as context for Copilot, you can highlight anything in the terminal and use the <code>#terminalSelection</code> command to tell Copilot to use that as context.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#terminallastcommand","title":"<code>#terminalLastCommand</code>","text":"<p>If you want to use the last command you executed in the terminal as context for your query, you can use the <code>#terminalLastCommand</code> command.</p> <p>Tip</p> <p>The difference between this one and the ' #terminalSelection' is that this one will use the last command you executed in the terminal, whithout having to highlight anything, while the other one will use whatever you have selected in the terminal, even if it has nothing to do with the latest command you executed.</p>"},{"location":"Tools/GitHub_Copilot/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"Tools/GitHub_Copilot/#local-machine","title":"Local Machine","text":"<p>If you want to install GitHub Copilot in your local machine, you can simply install it from the Extensions tab in VSCode. Once you hit install, it will ask you to sign in with your GitHub account.</p> <p>Note</p> <p>You need to have done the setup asked by IT in order to enrol into the GitHub Academy program. The instructions on how to do so are here.</p>"},{"location":"Tools/GitHub_Copilot/#cluster","title":"Cluster","text":"<p>As seen in VSCode in interactive node, you can also use GitHub Copilot in the cluster.</p> VSCode App SetupBrowser (code-server) <p>Same as with the VSCode App (see section Local Machine). Just go to the Extensions tab in  VSCode and install GitHub Copilot.</p> <ol> <li>Visit the GitHub Copilot Extension page.</li> <li>Look for a \"Download Extension\" link. It shoudl be in the side-bar under the \"Resources\" header.</li> <li>You'll get a file called something like <code>GitHub.copilot-1.226.0.vsix</code>. Move this file into the cluster.</li> <li>With your VSCode session opened from the cluster, go to the Extensions tab and click on the three dots in the top right corner. Select \"Install from VSIX...\" and select the file you just moved into the cluster.</li> <li>You will be asked to sign in with your GitHub account. Follow the instructions to do so.</li> <li>You should now have GitHub Copilot installed in your cluster session.</li> </ol> <p>Note</p> <p>As of the time of writing, this way of installing GitHub Copilot only includes the feature of the Ghost Text, but not the rest of the features.</p>"},{"location":"Tools/GitHub_Copilot/#github-copilot-cli","title":"GitHub Copilot CLI","text":"<p>You can use GitHub Copilot in the terminal as well!</p> <p>Here's a video with the installation process and an explanation of how to use GitHub Copilot CLI in the terminal:</p> <p>There is an option to use GitHub Copilot in the terminal. There, you will have mainly two commands:</p> <ul> <li> <p><code>gh copilot suggest</code> or <code>ghcs</code>: GitHub Copilot Suggest. It will return a command based on your instructions.   </p> </li> <li> <p><code>gh copilot explain</code> or <code>ghce</code>: GitHub Copilot Explain. It will return an explanation of the command that was suggested. You can ask for an explanation of the command by using the command <code>ghce</code> or by selecging the option <code>Explain command</code> when you run the <code>ghcs</code> command.   </p> </li> </ul> <p>If you see that the suggestion needs to be changed or you want to add extra information, you can select the option <code>Revide command</code> when you run the <code>ghcs</code> command.</p> <p></p>"},{"location":"Tools/GitHub_Copilot/#installation","title":"Installation","text":"<p>The instructions on how to set it up can be found here.</p>"},{"location":"Tools/GitHub_Copilot/#references","title":"References","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Federica Brando</li> <li>Izar de Villasante</li> </ul>"},{"location":"Tools/Jupyter/","title":"Jupyter - notebook - lab","text":"<p>Jupyterlab is an interactive notebook environment to run Python, R and many more languages. jupyter.org</p>"},{"location":"Tools/Jupyter/#installation","title":"Installation","text":"<p>Jupyter can be installed using either <code>conda</code> or <code>mamba</code> conda: <code>conda install -c anaconda jupyter</code> mamba: <code>mamba install -c anaconda jupyter</code></p>"},{"location":"Tools/Jupyter/#launching-jupyter-notebook-or-lab","title":"Launching jupyter notebook or lab","text":""},{"location":"Tools/Jupyter/#locally","title":"locally","text":"<p>Launch a new notebook session using :</p> <pre><code>jupyter notebook\n</code></pre> <p>or a jupyter lab session:</p> <pre><code>jupyter lab\n</code></pre>"},{"location":"Tools/Jupyter/#cluster","title":"cluster","text":"<p>See here for the instructions to launch and connect to jupyter sessions on the computing cluster</p>"},{"location":"Tools/Jupyter/#change-conda-environment-from-inside-a-running-cluster-session","title":"Change conda environment from inside a running cluster session","text":"<pre><code>conda install -c anaconda ipykernel\npython -m ipykernel install --user --name=env_name\n</code></pre>"},{"location":"Tools/Nextflow/","title":"Nextflow","text":"<p>Nextflow enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of data-driven, computational pipelines written in the most common scripting languages.</p>"},{"location":"Tools/Nextflow/#usage","title":"Usage","text":"<p>To run the default installed version of Nextflow, simply load the <code>nextflow</code> module:</p> <pre><code>$ module load nextflow\n$ nextflow help\n\nUsage: nextflow [options] COMMAND [arg...]\n</code></pre> <p>For usage documentation, run <code>nextflow help</code>.</p>"},{"location":"Tools/Nextflow/#submitting-processes-as-serial-jobs","title":"Submitting processes as serial jobs","text":"<p>Recommended for serial jobs only</p> <p>This section is recommended for serial jobs only. For parallel jobs, please see the Parallel jobs section below.</p> <p>Nextflow supports the ability to submit pipeline scripts as separate cluster jobs using the SGE executor.</p> <p>To enable the SGE executor, simply set to <code>process.executor</code> property to sge in a configuration file named <code>nextflow.config</code> in the job working directory. The amount of resources requested by each job submission is defined in the cluster options section, where all Univa scheduler resources are supported.</p> <p>For example, to run all pipeline jobs with 2 serial cores and 2GB of memory for 1 hour, create the following configuration file:</p> <pre><code>process.executor='sge'\nprocess.clusterOptions='-pe smp 2 -l h_vmem=1G,h_rt=1:0:0'\n</code></pre> <p>Setting the memory limit for serial jobs</p> <p>Add the <code>-DXmx</code> option to limit the amount of memory Nextflow can use in serial jobs. For more information regarding the Java VM memory allocation, see here.</p>"},{"location":"Tools/Nextflow/#parallel-jobs","title":"Parallel jobs","text":"<p>Parallel jobs will use the in-built Apache Ignite clustering platform; Execution will be performed on the nodes requested in the submit request over MPI rather than submitting new jobs for each pipeline.</p> <p>Do not use the SGE executor in parallel jobs</p> <p>Using the SGE executor for parallel jobs causes the master job to hang until it is killed by the scheduler for exceeding walltime. This is due to Apache Ignite not being able to communicate to other pipeline scripts submitted as separate jobs.</p> <p>To ensure parallel jobs use Apache Ignite, add the following to the configuration file (or omit the <code>process.executor</code> setting):</p> <pre><code>process.executor='ignite'\n</code></pre>"},{"location":"Tools/Nextflow/#example-jobs","title":"Example jobs","text":""},{"location":"Tools/Nextflow/#serial-job","title":"Serial job","text":"<p>Here is an example job taken from the Nextflow website to submit each process in the <code>input.nf</code> file as a new cluster job with 1 core and 1GB of memory. Ensure the cumulative runtime across all processes does not exceed the runtime requested in the master job:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\nmodule load nextflow\n\nnextflow -DXmx=1G \\\n         -C nextflow.config \\\n         run input.nf\n</code></pre>"},{"location":"Tools/Nextflow/#parallel-job","title":"Parallel job","text":"<p>Here is an example job taken from the Nextflow website to run each process in the <code>input.nf</code> file using 48 cores across 2 sdv nodes with Apache Ignite:</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe parallel 48\n#$ -l infiniband=sdv-i\n#$ -l h_rt=240:0:0\n\nmodule load nextflow openmpi\n\nmpirun --pernode \\\n       nextflow run input.nf \\\n       -with-mpi\n</code></pre>"},{"location":"Tools/Nextflow/#links","title":"Links","text":"<ul> <li>Nextflow documentation</li> <li>Nextflow basic pipeline example</li> <li>Nextflow presentation videos</li> <li>Nextflow community support</li> <li>Nextflow MPI</li> <li>Apache Ignite</li> </ul>"},{"location":"Tools/Nextflow/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Carlos L\u00f3pez-Elorduy</li> <li>Miguel Grau</li> </ul>"},{"location":"Tools/Snakemake/","title":"Snakemake","text":"<p>The Snakemake workflow management system is a tool to create reproducible and scalable data analyses. Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.</p>"},{"location":"Tools/Snakemake/#installation","title":"Installation","text":"<p>You can use conda/mamba  to install Snakemake:</p> <pre><code>#mamba way. FASTER\n$ conda  install  -n  mambaenv  -c  conda-forge  mamba\n$ conda  activate  mambaenv\n$ mamba  create  -c  conda-forge  -c  bioconda  -n  snakemakenv  snakemake\n$ conda  activate  snakemakenv\n</code></pre> <pre><code>#conda way\n$ conda  create  -c  conda-forge  -c  bioconda  -n  snakemakenv  snakemake\n$ conda  activate  snakemakenv\n</code></pre> <p>More details here</p>"},{"location":"Tools/Snakemake/#usage","title":"Usage","text":"<p>Before actually running a snakemake pipeline, you can try the <code>dry mode (-np)</code>, which do not execute anything, and display what would be done. This is a good way to check that all the commands/inputs/output are as expected:</p> <pre><code>snakemake --snakefile example.snk.py --cores 1 -np\n</code></pre> <p>And if you want to run it:</p> <pre><code>snakemake --snakefile example.snk.py --cores 1\n</code></pre> <p>For usage documentation, run <code>snakemake --help</code>.</p>"},{"location":"Tools/Snakemake/#examples","title":"Examples","text":"<p>Let's start with a very basic example, a pipeline with a single real step.</p>"},{"location":"Tools/Snakemake/#example1-a-simple-example","title":"Example1. A simple example","text":"<pre><code>#dry run\n#snakemake --snakefile example1.snk.py --cores 1 -np\n\nimport subprocess, sys, os, glob\n\n# Input parameters  ---------------------------------\n\n# Rules             --------------------------------- \n\nrule all:\n    input:\n        'output/sample1_1_fastqc.html'\n\nrule FASTQC:\n    input:\n        'data/sample1_1.fastq.gz'\n    output:\n        'output/sample1_1_fastqc.html'\n    threads: 1\n    shell:\"\"\" \n        fastqc -o output/ -t 1 {input}\n    \"\"\"\n</code></pre> <p> Snakemake starts at the end. </p> <p>By default snakemake executes the first rule in the snakefile. It is possible to overwrite this behavior by explicitly marking a rule as being the default target via the default_target directive.</p> <p>As you can see, there are two rules, <code>all</code> and <code>FASTQC</code> but actually only one of them (<code>FASTQC</code>) is a   proper rule. Our first rule, <code>all</code>, requires as an input the final output of the pipeline, the <code>fastqc.html</code>  files. We only define it as a final-rule, to explicity define what we want as a final pipeline ouput. To obtain  the rule <code>all</code> input (and finish the pipeline's execution), snakemake checks which rule needs to be executed to  obtain it (<code>FASTQC</code>, in this case). Then it checks again the input of <code>FASTQC</code>, in case it is the output from  another rule, and so on, until it reaches the beggining of the pipeline. In this example, there is only one rule,  so the final output (<code>fastqc.html</code> files) is the output of the unique rule (<code>FASTQC</code>).</p> <p>Every step from the pipeline is defined within a rule. Every rule has some basic parameters:</p> <ul> <li>Input/output where we define the input and expected output of our rule.</li> <li>Shell/run/script: A rule can be a bash command (<code>shell</code>), a python script (<code>run</code>) or a script in</li> <li>almost any language (<code>script</code>).</li> </ul> <p>Additionally, there are many optional parameters: threads, resources, preemptible-jobs, messages, Log-Files and more...</p>"},{"location":"Tools/Snakemake/#example2-wildcards","title":"Example2. Wildcards","text":"<p>In this case, we specified (hardcoded) the name of the samples (<code>['sample1_1',  'sample1_2']</code>) and the input folder (<code>data/</code>)</p> <pre><code>#dry run\n#snakemake --snakefile example2.snk.py --cores 1 -np\n\nimport subprocess, sys, os, glob\n\n# Input parameters  ---------------------------------\n\n# Rules             --------------------------------- \n\nSAMPLES = ['sample1_1', 'sample1_2']\n\nrule all:\n    input:\n        expand('output/{sample}_fastqc.html', sample=SAMPLES)\n\nrule FASTQC:\n    input:\n        'data/{sample}.fastq.gz'\n    output:\n        'output/{sample}_fastqc.html'\n    threads: 1\n    shell:\"\"\" \n        fastqc -o output/ -t 1 {input}\n    \"\"\"\n</code></pre> <p>Now, If we run the script in dry mode:</p> <pre><code>$ snakemake --snakefile example2.snk.py --cores 1 -np\nBuilding DAG of jobs...\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nFASTQC        2              1              1\nall           1              1              1\ntotal         3              1              1\n\n\n[Thu Mar 16 15:25:31 2023]\nrule FASTQC:\n    input: data/sample1_1.fastq.gz\n    output: output/sample1_1_fastqc.html\n    jobid: 1\n    reason: Missing output files: output/sample1_1_fastqc.html; Code has changed since last execution\n    wildcards: sample=sample1_1\n    resources: tmpdir=/tmp/jobs/mgrau/8723006\n\n\n        fastqc -o output/ -t 1 data/sample1_1.fastq.gz\n\n\n[Thu Mar 16 15:25:31 2023]\nrule FASTQC:\n    input: data/sample1_2.fastq.gz\n    output: output/sample1_2_fastqc.html\n    jobid: 2\n    reason: Missing output files: output/sample1_2_fastqc.html; Code has changed since last execution\n    wildcards: sample=sample1_2\n    resources: tmpdir=/tmp/jobs/mgrau/8723006\n\n\n        fastqc -o output/ -t 1 data/sample1_2.fastq.gz\n\n\n[Thu Mar 16 15:25:31 2023]\nlocalrule all:\n    input: output/sample1_1_fastqc.html, output/sample1_2_fastqc.html\n    jobid: 0\n    reason: Input files updated by another job: output/sample1_1_fastqc.html, output/sample1_2_fastqc.html\n    resources: tmpdir=/tmp/jobs/mgrau/8723006\n\nJob stats:\njob       count    min threads    max threads\n------  -------  -------------  -------------\nFASTQC        2              1              1\nall           1              1              1\ntotal         3              1              1\n</code></pre> <p>For every sample defined in the <code>SAMPLES = ['sample1_1', 'sample1_2']</code>, it executes the rule <code>FASTQC</code>:</p> <ul> <li><code>fastqc -o output/ -t 1 data/sample1_1.fastq.gz</code></li> <li><code>fastqc -o output/ -t 1 data/sample1_2.fastq.gz</code></li> </ul> <p>And it ends when the rule <code>all</code> is executed. Three rules are executed in total (<code>FASTQC</code> x2 times + <code>all</code> x 1 time).</p>"},{"location":"Tools/Snakemake/#to-be-continued","title":"TO BE CONTINUED","text":""},{"location":"Tools/Snakemake/#links","title":"Links","text":"<ul> <li>Snakemake Installation</li> </ul>"},{"location":"Tools/Snakemake/#reference","title":"Reference","text":"<ul> <li>Miguel Grau</li> </ul>"},{"location":"Tools/BBG-tools/BGconfig/","title":"BGconfig","text":""},{"location":"Tools/BBG-tools/BGconfig/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGconfig/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGdata/","title":"BGdata","text":""},{"location":"Tools/BBG-tools/BGdata/#description","title":"Description","text":"<p>bgdata is a simple data package manager. What it gives is essentially a path to a certain data package. The package is specified by 4 layers: project, dataset, version and build. Installation</p> <p>bgdata is written in Python and requires Python&gt;=3.4. bgdata can be installed through pip</p> <pre><code>pip install bgdata\n</code></pre> <p>Or by conda</p> <pre><code>conda install -c conda-forge -c bbglab bgdata\n</code></pre>"},{"location":"Tools/BBG-tools/BGdata/#how-it-works","title":"How it works","text":"<p>bgdata is configured to have a remote and a local repository where to look for data packages. When bgdata is asked to get a certain package it checks whether it is in the local repo. If not present, it is downloaded from the remote.</p> <p>If the build is not specified, bgdata returns the master.</p> <p>bgdata can do more things such as work in offline mode or be configured to use a different path for certain files.</p> <p>You can find out more in the documentation.</p>"},{"location":"Tools/BBG-tools/BGdata/#using-bgdata","title":"Using bgdata","text":"<p>bgdata is a Python package that can be used from Python or from a bash shell using its command line interface.</p> <p>From Python it is as simple as using the get function:</p> <pre><code>import bgdata\nbgdata.get('project/dataset/version?build')\n</code></pre> <p>From the command line is is equally simple:</p> <pre><code>bgdata get project/dataset/version?build\n</code></pre> <p>More info can be found in the docs</p>"},{"location":"Tools/BBG-tools/BGdata/#reference","title":"Reference","text":"<ul> <li>Davide Scarpetta</li> </ul>"},{"location":"Tools/BBG-tools/BGlogs/","title":"BGlogs","text":""},{"location":"Tools/BBG-tools/BGlogs/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGlogs/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGpack/","title":"BGpack","text":""},{"location":"Tools/BBG-tools/BGpack/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGpack/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/BGreference/","title":"BgReference","text":""},{"location":"Tools/BBG-tools/BGreference/#description","title":"Description","text":"<p>BgReference is a library to fast retrive Genome Reference partial sequences.</p>"},{"location":"Tools/BBG-tools/BGreference/#list-of-available-genomes-v07","title":"List of available genomes (v0.7)","text":"<pre><code># Using HUMAN_GENOME_SEQUENCE_MAPS\nhg19\nhg38\nhg18\n</code></pre> <pre><code># Using MOUSE_GENOME_SEQUENCE_MAPS\nc3h\nmm10\nmm39\ncast\ncar\nf344\n</code></pre>"},{"location":"Tools/BBG-tools/BGreference/#installation","title":"Installation","text":"<pre><code>conda install -c conda-forge -c bbglab bgreference\n</code></pre> <p>or</p> <pre><code>pip install bgreference\n</code></pre>"},{"location":"Tools/BBG-tools/BGreference/#examples","title":"Examples","text":"<pre><code>from bgreference import hg19, hg38\n# Get 10 bases from chromosome one build hg19\nhg19('1', 12345, size=10)\n\n# Get the sequence of the whole chromosome\nhg19('1',(1), size=None)\n\n# You can use synonymous sequence names\nhg19(2, 23456)\nhg19('2', 23456)\nhg19('chr2', 23456)\n\nhg19('MT', 234, size=3)\nhg19('chrM', 234, size=3)\nhg19('chrMT', 234, size=3)\n</code></pre>"},{"location":"Tools/BBG-tools/BGreference/#repository","title":"Repository","text":"<p>Click here to see the repository of BgReference. </p>"},{"location":"Tools/BBG-tools/BGreference/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Paula Gomis</li> </ul>"},{"location":"Tools/BBG-tools/BGsignature/","title":"BGsignature","text":""},{"location":"Tools/BBG-tools/BGsignature/#description","title":"Description","text":"<p>BGsignature is a tool to compute mutational profiles. A mutational profile corresponds to the count or the frequencies of mutations in every possible k-nucleotide (e.g., trinucleotide or pentanucleotide) contexts. BGsignature can be used from the command line or as a Python package.</p> <p>It includes three main functionalities:</p> <ul> <li>count: get the number of mutations in the k-nucleotide contexts or just count different k-mers</li> <li>frequency: get the frequency of mutations (count divided by total mutations) in the k-nucleotide contexts</li> <li>normalize: get the normalized frequency of mutations taking into account the k-nucleotide bias</li> </ul> <p>The count and the frequency of mutations can be computed for a set of regions or for a set of mutations that fall within certain regions.</p>"},{"location":"Tools/BBG-tools/BGsignature/#installation","title":"Installation","text":"<pre><code>pip install bgsignature\n</code></pre>"},{"location":"Tools/BBG-tools/BGsignature/#input","title":"Input","text":"<ul> <li>mutations file: tab separated file (can be compressed into gz, bgz or xz formats) with a header and</li> <li>at least these columns: CHROMOSOME, POSITION, REF, ALT. In addition, SAMPLE, CANCER_TYPE and SIGNATURE are</li> <li> <p>optional columns that can be used for grouping the signature.</p> </li> <li> <p>regions file: tab separated file (can be compressed into gz, bgz or xz formats) with a header and at</p> </li> <li>least these columns: CHROMOSOME, START, END, ELEMENT. In addition, SYMBOL, and SEGMENT are optional columns</li> <li>that can be used for grouping the signature.</li> </ul>"},{"location":"Tools/BBG-tools/BGsignature/#examples","title":"Examples","text":"<p>Example on how to compute the normalized frequency of mutations in trinucleotide contexts, i.e., the mutationalal profile of 192 channels often used by the driver discovery methods developed by the BBGLab.</p>"},{"location":"Tools/BBG-tools/BGsignature/#1-get-the-region-files","title":"1. Get the region files","text":"<p>The region file can simply include the coordinates of the reference genome (all regions), or the coordinates of a subset of regions of interest. The reference genome can be downloaded from different sources such as GenBank or UCSC Genome Browser.</p>"},{"location":"Tools/BBG-tools/BGsignature/#2-get-the-count-of-trinucleotides-it-will-be-used-to-normalize-the-frequency-of-mutations","title":"2. Get the count of trinucleotides (it will be used to normalize the frequency of mutations)","text":"<pre><code>bgsignature count -r my/regions/file -s 3 -g hg38 --cores 4 --collapse --exclude-N -o my/count.json\n</code></pre> <p><code>collapse</code> add together reverse complementary sequences</p>"},{"location":"Tools/BBG-tools/BGsignature/#3-get-the-frequency-of-mutations-normalized-by-trinucleotide-bias","title":"3. Get the frequency of mutations normalized by trinucleotide bias","text":"<pre><code>bgsignature normalize -m my/muts.tsv -r my/regions.tsv --normalize my/count.json -s 3 -g hg38 --collapse --cores 4 -o my/mut_profile.json\n</code></pre>"},{"location":"Tools/BBG-tools/BGsignature/#repository","title":"Repository","text":"<p>Check out the BGsignature repository. It includes additional information and (not fully comprehensive) examples on how to use the tool as Python package.</p>"},{"location":"Tools/BBG-tools/BGsignature/#reference","title":"Reference","text":"<ul> <li>Stefano Pellegrini</li> </ul>"},{"location":"Tools/BBG-tools/BGvep/","title":"BGvep","text":""},{"location":"Tools/BBG-tools/BGvep/#description","title":"Description","text":""},{"location":"Tools/BBG-tools/BGvep/#reference","title":"Reference","text":""},{"location":"Tools/BBG-tools/OpenVariant/","title":"OpenVariant","text":""},{"location":"Tools/BBG-tools/OpenVariant/#description","title":"Description","text":"<p>OpenVariant is a comprehensive Python package that provides different functionalities to read, parse and operate different multiple input file formats (e. g. <code>tsv</code>, <code>csv</code>, <code>vcf</code>, <code>maf</code>, <code>bed</code>), being able to build an unified output with a proper annotation file structure.</p>"},{"location":"Tools/BBG-tools/OpenVariant/#usage","title":"Usage","text":"<p>Click here to see the installation guide and the complete documentation of OpenVariant.</p> <p>When working with OpenVariant, we need to distinguish 3 different types of files: <code>input files</code> and <code>annotation file</code>, which are provided by the user and <code>output file</code>, which will returned from the function.</p> <ul> <li><code>Input files</code> will be the group of files in different formats (e.g. tsv, csv, vcf, maf, bed) that we want to parse.</li> <li><code>Annotation file</code> is a YAML file which describes how the <code>input files</code> are processed and how the <code>output file</code> will look like.</li> <li><code>Output files</code> are generated by OpenVariant and they are the result of the process.</li> </ul>"},{"location":"Tools/BBG-tools/OpenVariant/#functions","title":"Functions","text":"<p>OpenVariant has several functions to perform different tasks:</p> <ul> <li><code>find_files</code>: Find files with a given pattern name in a given folder.</li> <li><code>Variant</code>: Parse an input file through the annotation file. It will generate an object which you can apply different functionalities</li> <li><code>cat</code>: It will show on the stdout (standard out) the whole parsed output.</li> <li><code>group_by</code>: It will generate an iterator that will contain three variables: <code>group_key</code> (the value of each group), <code>group_result</code> (a list of all rows that pertain to each group) and <code>command</code> (if it uses the <code>script</code> parameter or not). It will group the parsed result for each different value of the specified <code>key_by</code>.</li> <li><code>count</code>: It returns the number of rows that matches a specific conditions.</li> </ul> <p>Click here to see several examples of each of the functions from OpenVariant.</p>"},{"location":"Tools/BBG-tools/OpenVariant/#parameters","title":"Parameters","text":"<p>The different options and parameters of these functions are specified in the <code>annotation file</code>, which has several required and optional parameters.</p> <p>Click here to learn about the parameters in the <code>annotation file</code> and a to see a template of the <code>annotation file</code>.</p>"},{"location":"Tools/BBG-tools/OpenVariant/#reference","title":"Reference","text":"<ul> <li>David Mart\u00ednez</li> <li>Paula Gomis</li> </ul>"},{"location":"Tools/Containers/Building_containers/","title":"Building containers","text":"<p>Containers are built from a definition file which allows the container to be built identically by anyone possessing the file.</p> <p>Root privileges required to build a container</p> <p>Note that the process of building a container requires elevated privileges</p> <p>One primary task per container</p> <p>HPC containers are designed to perform one primary task, and should consist of a main application and its dependencies, in a similar way to how module files are provided. Since containers are lightweight, you can use separate containers instead of general purpose containers containing a collection of applications. This improves supportability, performance and reproducibility.</p>"},{"location":"Tools/Containers/Building_containers/#how-to-build-a-container","title":"How to build a container","text":"<p>Docker and Singularity have different standard formats for building containers and writing the definition file. However since  Singularity 4 onwards these technology can use Dockerfile as a recipe so it would be nice to use this as a standard way of writing the container recipes.</p>"},{"location":"Tools/Containers/Building_containers/#comfortable-ways-of-building-containers","title":"Comfortable ways of building containers","text":"<ul> <li>https://seqera.io/containers/</li> </ul> <p>With this absolutely amazing resource you can specify what type of container you want to create (at the moment either Docker or Singularity) and what packages you want to include in the container. With these specifications, the website generates a container image meeting the specifications.</p>"},{"location":"Tools/Containers/Building_containers/#where-to-hostshare-your-containers","title":"Where to host/share your containers","text":"<ul> <li> <p>If the container is only for your interest you can keep it in your user, but it would be great that you have the Dockerfile somewhere that can be traceable and reproducible so that anyone can recreate it.</p> </li> <li> <p>If the container is from a tool of the lab then it would be great to follow the standard and best practice procedures and share it here: BBGLab dockerhub. In this link you can find the containers that have been built and shared by the lab. This is in general synchronized with the Dockerfile receipes here: GitHub bbglab/containers. So whenever you need to build a new container with BBGLab tools make sure that you keep this good work and add the container receipe in this GitHub repository and then ask someone with permission to push the container to the collective dockerhub account.</p> </li> </ul>"},{"location":"Tools/Containers/Building_containers/#advanced-details-on-how-to-build-a-container","title":"Advanced details on how to build a container","text":"<p>These can be found in  <code>Singularity-&gt;Building_containers</code> section.</p>"},{"location":"Tools/Containers/Building_containers/#reference","title":"Reference","text":"<ul> <li>Ferriol Calvet</li> <li>Ferran Mui\u00f1os</li> <li>Jordi Deu-Pons</li> </ul>"},{"location":"Tools/Containers/Overview/","title":"Overview","text":""},{"location":"Tools/Containers/Overview/#linux-containers","title":"Linux Containers","text":"<p>Linux containers are lightweight, portable execution environments that package up software and all its dependencies so it can run reliably across different computing environments. Unlike virtual machines, containers share the host operating system kernel while isolating the user space \u2014 meaning they can run different Linux distributions and applications in a self-contained way, with minimal overhead.</p> <p>Popular container technologies include Docker and Singularity (Apptainer). Docker is widely used in software development, CI/CD pipelines, and cloud environments, while Singularity is tailored for High Performance Computing (HPC) and multi-user systems where security and integration with shared filesystems and hardware (e.g., GPUs) are critical.</p>"},{"location":"Tools/Containers/Overview/#benefits-of-containers","title":"Benefits of Containers","text":"<ul> <li>Reproducible science \u2013 Containers encapsulate software and dependencies, ensuring consistency across different systems and over time.</li> <li>Version independence \u2013 Run software compiled for one Linux distribution (e.g., Ubuntu) on another (e.g., CentOS), without modification.</li> <li>Portability \u2013 Containers can be built once and deployed anywhere a compatible container runtime is available.</li> <li>Self-contained environments \u2013 Avoid conflicts with system-installed software and simplify the setup of complex applications.</li> </ul>"},{"location":"Tools/Containers/Overview/#comparing-docker-and-singularity","title":"Comparing Docker and Singularity","text":"Feature Docker Singularity (Apptainer) Root access needed Yes (typically requires root/daemon) No (designed for unprivileged users) HPC compatibility Limited Excellent (built for HPC) Uses Docker images Yes Yes GPU/Infiniband support With additional configuration Native support Suitable for shared clusters Rarely Yes Security model Daemon-based, less suitable for multi-user systems User-space execution with no privilege escalation"},{"location":"Tools/Containers/Overview/#singularity","title":"Singularity","text":"<p>Singularity (also known as Apptainer in newer releases) is a secure, Open Source container platform designed for HPC environments. Compared to Docker, Singularity offers several advantages for scientific computing:</p> <ul> <li>Runs without requiring root privileges, enhancing security in shared environments.</li> <li>Supports integration with high-performance hardware such as GPUs and Infiniband networks.</li> <li>Can directly use Docker container images, allowing users to leverage the extensive Docker ecosystem while benefiting from Singularity\u2019s HPC compatibility.</li> </ul>"},{"location":"Tools/Containers/Overview/#using-containers-in-the-cluster","title":"Using containers in the cluster","text":"<p>Warning</p> <p>Singularity is the only container technology that works in HPC systems, thus in our cluster we can only use Singularity</p> <p>Even if a container was built using Docker, and/or it is hosted in dockerhub you can still use it from Singularity.</p> <p>For more information on how to use it check the <code>Using_containers</code> section in the Singularity folder.</p>"},{"location":"Tools/Containers/Overview/#container-sources","title":"Container sources","text":"<p>There are different places where you can find containers that are ready to use:</p> <ul> <li> <p>Our cluster: <code>/data/bbg/datasets/pipelines/nextflow_containers</code> here there are several containers that are used by different people in the lab, mainly for running Nextflow pipelines.</p> </li> <li> <p>BBGLab dockerhub here you can find the containers that have been built and shared by the lab. This is in general synchronized with the Dockerfile recipes here: GitHub bbglab/containers. So whenever you need to build a new container with BBGLab tools make sure that you keep this good work and add the container recipe in this GitHub repository and then ask someone with permission to push the container to the collective dockerhub account.</p> </li> <li> <p>dockerhub this is a public repository of containers where you can push your own containers and also use other people's containers.</p> </li> </ul>"},{"location":"Tools/Containers/Overview/#further-reading","title":"Further Reading","text":"<ul> <li>Singularity website</li> <li>Docker website</li> <li>BBGcloud presentation on Singularity </li> <li>BBGcloud presentation on Linux and containers </li> <li>Use <code>singularity help</code> and <code>singularity CMD help</code> (replace <code>CMD</code> with a Singularity command, such as <code>run</code>)  </li> <li>View the system manual page: <code>man singularity</code></li> </ul>"},{"location":"Tools/Containers/Overview/#reference","title":"Reference","text":"<ul> <li>Reorganized and updated by Ferriol Calvet</li> </ul> <p>Previous explanation by:</p> <ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Axel Rosendahl Huber</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"Tools/Containers/Singularity/Building_containers/","title":"Building containers","text":"<p>Singularity containers are built from a definition file which allows the container to be built identically by anyone possessing the file.</p> <p>Root privileges required to build a container</p> <p>Note that the process of building a container requires elevated privileges</p> <p>One primary task per container</p> <p>HPC containers are designed to perform one primary task, and should consist of a main application and its dependencies, in a similar way to how module files are provided. Since containers are lightweight, you can use separate containers instead of general purpose containers containing a collection of applications. This improves supportability, performance and reproducibility.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#building-a-singularity-container-from-scratch","title":"Building a Singularity container from scratch","text":"<p>Building from scratch gives complete control over the contents of the container, including operating system and packages. Certain packages may only be available for a specific version of Linux (i.e. compatibility issues) so being able to build a container from scratch enhances research capability.</p> <p>The following example demonstrates building an Ubuntu 20 (focal) container using definition file <code>ubuntu20_helloworld.def</code> that installs the <code>python3</code> package via the Ubuntu package manager:</p> <pre><code>BootStrap: debootstrap\nOSVersion: focal\nMirrorURL: http://us.archive.ubuntu.com/ubuntu/\n\n%post\n  apt-get install --yes python3\n\n%runscript\n  python3 \"${@}\"\n</code></pre> <p>The build process is unattended, and will not succeed if any operations require interactive input. Be sure to use <code>-y</code> or <code>--yes</code> options when installing packages.</p> <p>Create the image (this step requires root privileges):</p> <pre><code>sudo singularity build ubuntu20_helloworld.simg ubuntu20_helloworld.def\n</code></pre> <p>This will result in a usable image in the current working directory. Be aware that if you want a very specific version of package from a repository, that package may not be available in future, so where possible, try to future-proof your containers.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#building-containers-for-other-linux-distributions","title":"Building containers for other Linux distributions","text":"<p>You may build Ubuntu images using CentOS and vice versa. However to bootstrap, you will need extra packages on the host OS to build the container. CentOS hosts require the <code>debootstrap</code> package to create Ubuntu containers, and Ubuntu hosts require the <code>yum</code> package to build CentOS containers. Alternatively you may create containers from an existing Singularity or Docker image, as explained in the following section. Since this method builds upon pre-built images, the <code>debootstrap</code> or <code>yum</code> packages are not required.</p> <p>Using LTS for Ubuntu definitions</p> <p>When building an Ubuntu container we recommend that you use a release with long term support (LTS release). Non-LTS Ubuntu releases have very limited support cycles which may lead to difficulties downloading packages if used after their end-of-life date.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#building-containers-from-an-existing-base-image","title":"Building containers from an existing base image","text":"<p>This enables you to either build or use an existing container as a base image to build other containers. Base images must be built first if part of a dependency chain and is no longer required once all dependent containers have been built.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#singularity-local-images","title":"Singularity local images","text":"<p>The following example demonstrates the creation of a local base Ubuntu 20 (focal) image using definition file <code>ubuntu20_base.def</code>, and then creating another container with <code>python3</code> installed, using the local base image:</p> <pre><code>BootStrap: debootstrap\nOSVersion: focal\nMirrorURL: http://us.archive.ubuntu.com/ubuntu/\n</code></pre> <p>Create the base image:</p> <pre><code>sudo singularity build ubuntu20_base.simg ubuntu20_base.def\n</code></pre> <p>The non-base image container (i.e. <code>python3</code> in this example) can be built using definition file <code>ubuntu20_python3.def</code>:</p> <pre><code>Bootstrap: localimage\nFrom: ubuntu20_base.simg\n\n%post\n  apt-get install --yes python3\n\n%runscript\n  python3 \"${@}\"\n</code></pre> <p>The result will be a container almost identical to the one created from scratch.</p> <pre><code>sudo singularity build ubuntu20_python3.simg ubuntu20_python3.def\n</code></pre>"},{"location":"Tools/Containers/Singularity/Building_containers/#docker-images","title":"Docker images","text":"<p>You can also bootstrap from Docker containers, although if supplied by a third party, you have less visibility or control over these images, so use with caution, as this may impact the future reproducibility of results.</p> <p>The below example demonstrates installing the <code>python3</code> package within an Ubuntu 20 (focal) container using definition file <code>ubuntu20_docker_python3.def</code>, which imports the <code>ubuntu:20.04</code> base container available on the Docker Hub:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n  apt-get update\n  apt-get install --yes python3\n\n%runscript\n  python3 \"${@}\"\n</code></pre> <p>Build the container. This will produce a container similar to the previous examples, but may vary slightly in overall size depending on packages installed in the base docker image:</p> <pre><code>sudo singularity build ubuntu20_docker_python3.simg ubuntu20_docker_python3.def\n</code></pre>"},{"location":"Tools/Containers/Singularity/Building_containers/#future-proofing-your-containers","title":"Future-proofing your containers","text":"<p>When building your own containers, be sure to make them portable and future-proof.</p> <ul> <li>Consider whether the container will still build and produce the same results if the OS release or application version changes.</li> <li>If copying files from a working directory as part of setup is unavoidable, ensure that any files copied from the working directory are are available for others to download (i.e. in a git repository if not large).</li> <li>Perform all setup as part of the build process. If any manual steps are performed after the container is built, they should be integrated within the definition file, and the container rebuilt.</li> <li>Consider if the ability to rebuild your container will be impacted by package updates, or deprecation of old releases.</li> </ul> <p>Legacy versions of CentOS applications</p> <p>Outdated minor CentOS releases are moved from the main CentOS servers to vault.centos.org. If you need to use a specific Operating System or application version other than the latest, you need to future-proof your container by using the CentOS vault.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#definition-file-sections","title":"Definition file sections","text":"<p>The following example definition file demonstrates commonly used definition file sections:</p> <ul> <li><code>%help</code></li> <li><code>%post</code></li> <li><code>%environment</code></li> <li><code>%test</code></li> <li><code>%runscript</code></li> </ul>"},{"location":"Tools/Containers/Singularity/Building_containers/#help-section","title":"Help section","text":"<p>The <code>%help</code> section is designed to provide information about the container when singularity run-help is run on the container, for example:</p> <pre><code>$ singularity run-help /data/containers/public/python3_helloworld.simg\nPurpose: Test container to print \"Hello, World!\" in Python3.\nAuthor:  ITS Research / QMUL.\n</code></pre>"},{"location":"Tools/Containers/Singularity/Building_containers/#post-section","title":"Post section","text":"<p>The <code>%post</code> section contains the commands used to build the container, such as package installs, file downloads, compilation and software configuration.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#environment-section","title":"Environment section","text":"<p>Environment settings supplied at build-time in the <code>%post</code> section are only set during build-time and are not available at run-time. Environment settings which need to be available at run-time should be added to the <code>%environment</code> section.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#test-section","title":"Test section","text":"<p>The <code>%test</code> section defines a set of commands or tests which should be run to validate the container has been built successfully. Some example tests include:</p> <ul> <li>installed binaries are available on the <code>PATH</code> variable</li> <li><code>--help</code> or <code>--version</code> parameter for binaries (if supported)</li> <li>libraries, header files and man pages exist</li> </ul> <p>All tests will be run during the build process, after <code>%post</code> has completed. To build a container without running the tests, pass the <code>-T</code> or <code>--notest</code> option to the singularity build command.</p> <p>To run the tests for an existing container, run the singularity test command, for example:</p> <pre><code>$ singularity test /data/containers/public/python3_helloworld.simg\n/usr/bin/python3\n</code></pre>"},{"location":"Tools/Containers/Singularity/Building_containers/#runscript-section","title":"Runscript section","text":"<p>The <code>%runscript</code> section defines the default action a container will perform when ran as an executable or with <code>singularity run</code>. This is configured during the build process.</p> <p>Application parameters or arguments</p> <p>If the runscript calls an application which takes parameters or arguments, include \"${@}\" after the application otherwise anything passed after the container name will be ignored by Singularity.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#inspecting-a-container","title":"Inspecting a container","text":"<p>To display information about how a container was build, use the <code>singularity inspect</code> command. The <code>-d</code> option to this command will print the definition file used to built the container and the <code>-r</code> option will print the runscript (if added during build-time). For example:</p> <pre><code>$ singularity inspect -d /data/containers/public/python3_helloworld.simg\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%help\n  Purpose: Test container to print \"Hello, World!\" in Python3.\n  Author:  ITS Research / QMUL.\n\n%post\n  apt-get update\n  apt-get install --yes python3\n\n  apt-get clean &amp;&amp; \\\n   rm -rf /var/lib/apt/lists/*\n\n%test\n  which python3\n\n%runscript\n  python3 -c 'print(\"Hello, World!\")'\n</code></pre> <p>The <code>singularity help inspect</code> command provides additional options for inspecting the container.</p>"},{"location":"Tools/Containers/Singularity/Building_containers/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"Tools/Containers/Singularity/Overview/","title":"Overview","text":""},{"location":"Tools/Containers/Singularity/Overview/#singularity-containers","title":"Singularity containers","text":"<p>Linux containers are self-contained execution environments that share a Linux kernel with the host, but have isolated resources for CPU, I/O, memory, etc. A container can run a completely different Linux environment, without the overhead required by virtual machines.</p>"},{"location":"Tools/Containers/Singularity/Overview/#benefits-of-containers","title":"Benefits of containers","text":"<ul> <li>Reproducible science - containers can include an application and its dependencies, and be run on other systems where Singularity is installed.</li> <li>Version independent - run code designed for other versions of Linux e.g. Ubuntu packages on a CentOS system.</li> <li>Self-contained - allow isolation of complicated application installs.</li> </ul>"},{"location":"Tools/Containers/Singularity/Overview/#singularity","title":"Singularity","text":"<p>Singularity is a popular Open Source container solution designed for HPC. Unlike other container solutions such as Docker, it allows utilisation of GPUs and Infiniband interconnects for MPI jobs, and does not allow privilege escalation within a container, which would compromise the security in a multi-user environment with a shared filesystem.</p>"},{"location":"Tools/Containers/Singularity/Overview/#using-singularity-on-the-bbgcluster","title":"Using Singularity on the bbgcluster","text":"<p>Singularity is available as a system package on the bbgcluster. We may update the version of Singularity installed on the cluster to address security vulnerabilities or to provide extra features as they become available. Recently, the default version of Singularity has been changed to singularity v3, although Singularity v2 can still be used with the command <code>singularity2</code>.</p>"},{"location":"Tools/Containers/Singularity/Overview/#resources","title":"Resources","text":"<p>Containers built by ITS Research are stored in <code>/data/containers</code> and are supported in a similar way to the globally available supported applications. Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands. See the Singularity usage page for more information about containers provided as modules.</p>"},{"location":"Tools/Containers/Singularity/Overview/#further-reading","title":"Further reading","text":"<ul> <li>Singularity website</li> <li>BBGcloud presentation on Singularity </li> <li> <p>BBGcloud presentation on Linux and containers </p> </li> <li> <p>Running <code>singularity help</code> and <code>singularity CMD help</code> (replace <code>CMD</code> with a Singularity command, such as <code>run</code>)</p> </li> <li>Viewing the \"singularity\" manual page</li> </ul>"},{"location":"Tools/Containers/Singularity/Overview/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Axel Rosendahl Huber</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"Tools/Containers/Singularity/Using_containers/","title":"Using containers","text":""},{"location":"Tools/Containers/Singularity/Using_containers/#running-commands-inside-a-container","title":"Running commands inside a container","text":"<p>The <code>singularity exec</code> command will allow you to execute any program within a given container. The <code>singularity run</code> command performs the action defined by the <code>%runscript</code> section, which is the primary task of the container. Using the <code>singularity run</code> command is the simpler approach for job submissions.</p> <p>You can even \"execute\" a container, which performs the same action as the <code>singularity run</code> command. For example, the following demonstrates how to inspect the runscript and execute the <code>/data/containers/public/python3_helloworld.simg</code> container:</p> <pre><code>$ singularity inspect -r /data/containers/public/python3_helloworld.simg\n#!/bin/sh\n  python3 -c 'print(\"Hello, World!\")'\n\n$ /data/containers/public/python3_helloworld.simg\nHello, World!\n\n$ singularity run /data/containers/public/python3_helloworld.simg\nHello, World!\n</code></pre>"},{"location":"Tools/Containers/Singularity/Using_containers/#execute-a-script-from-outside-the-container","title":"Execute a script from outside the container","text":"<p>Using on the cluster</p> <p>For typical use, you want to use the singularity run or singularity exec commands, especially when submitting the work via the scheduler.</p> <p>The following example runs a python script <code>hello_world2.py</code> from the current directory using the <code>/data/containers/public/python3_helloworld.simg</code> container:</p> <pre><code>$ singularity exec /data/containers/public/python3_helloworld.simg python3 ./hello_world2.py\nHello, World!\nHello, World (again)!\n</code></pre> <p>The file hello_world2.py contains the following code:</p> <pre><code>print(\"Hello, World!\")\nprint(\"Hello, World (again)!\")\n</code></pre> <p>If command <code>singularity exec</code> was replaced by <code>singularity run</code>, the runscript would be called, ignoring any parameters after the container name.</p> <p>Customised Environments</p> <p>While we encourage users to customise their environment to make their workflow easier, please be aware that customisations which change the user's environment for example by setting variables in the <code>~/.bash_profile</code> file, or by using python's pip to create a ~/.local folder, may cause problems with Singularity which can be difficult to troubleshoot.</p>"},{"location":"Tools/Containers/Singularity/Using_containers/#using-containers-with-grid-engine","title":"Using containers with Grid Engine","text":"<p>One of the major benefits of Singularity is the simplicity with which it can be used in an HPC environment. Your Grid Engine submission script may not require any modules loading to run your container. The resource requirements should be very similar to native code.</p>"},{"location":"Tools/Containers/Singularity/Using_containers/#simple-example","title":"Simple example","text":"<pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\nsingularity run /data/containers/public/python3_helloworld.simg\n</code></pre>"},{"location":"Tools/Containers/Singularity/Using_containers/#modules-example","title":"Modules example","text":"<p>Applications installed within Singularity containers may also be provided as a module to abstract the container invocation commands (<code>singularity run</code> and <code>singularity exec</code>). In these cases, the container name will match the runscript command. For example, to use Pandoc as a module, simply load the <code>pandoc</code> module to use the application.</p> <pre><code>#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -pe smp 1\n#$ -l h_rt=1:0:0\n#$ -l h_vmem=1G\n\nmodule load pandoc\npandoc --help\n</code></pre>"},{"location":"Tools/Containers/Singularity/Using_containers/#shell-access-to-the-container","title":"Shell access to the container","text":"<p>It is possible to launch a shell within the container using the <code>shell</code> command. Interacting directly with a shell inside the container can be useful for code debugging and running multiple commands in a single interactive session, as an alternative to writing a single script. Below demonstrates how to invoke python3 from inside the <code>/data/containers/public/python3_helloworld.simg</code> container using an interactive shell:</p> <pre><code>$ singularity shell /data/containers/public/python3_helloworld.simg\nSingularity&gt; python3\nPython 3.8.10 (default, Sep 28 2021, 16:10:42)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>Documentation is available on the Singularity Hub Wiki</p>"},{"location":"Tools/Containers/Singularity/Using_containers/#running-containers-from-external-sources","title":"Running containers from external sources","text":"<p>Use of external containers for Research</p> <p>For long term reproducibility of containers, we recommend that you build your own native Singularity containers from definition files instead of relying on 3rd party containers for your research. Using containers from external sources may produce undesirable results if the container is rebuilt after upstream changes such as updated or obsoleted packages.</p> <p>Containers created elsewhere can be copied or imported, and run on the cluster. The following example demonstrates how to import and run the latest Ubuntu official image stored in the Docker Hub:</p> <pre><code>$ singularity pull ubuntu.simg docker://ubuntu:latest\n$ singularity exec ubuntu.simg cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=20.04\nDISTRIB_CODENAME=focal\nDISTRIB_DESCRIPTION=\"Ubuntu 20.04.3 LTS\"\n</code></pre>"},{"location":"Tools/Containers/Singularity/Using_containers/#reference","title":"Reference","text":"<ul> <li>Jordi Deu-Pons</li> <li>Miguel Grau</li> <li>Carlos L\u00f3pez-Elorduy</li> </ul>"},{"location":"Tools/Graphic%20design/Repositories%20of%20SVG%20icons/","title":"Repositories of SVG icons","text":"<p>In this page you can find a list of resources to download free SVGs and PNGs. SVGs are very useful as they are vector-based images, so they can be scaled without losing quality and are highly customisable with tools like Inkscape.</p> <p>When using graphical icons in our slides, posters, reports and other materials we should take into account that in some cases there are usage rules (attribution is sometimes required, some icons cannot be modified...). To avoid these problems, I have gathered several pages that are \"open-access\", and the usage of these icons is less restrictive. However, I recommend to always check the rules in case any policy is updated.</p> <ul> <li>Iconpacks: a repository of SVG and PNG icons from a wide range of topics,</li> <li>not only biomedicine related. These icons are free and can be used for both commercial and personal projects.</li> <li>You can also modify the icons as you wish (they even offer a customisation tool). No attribution is required,</li> <li>but they state that credit will be appreciated.</li> </ul> <p> </p> <ul> <li>Healthicons: a repository of SVG and PNG icons biomedicine related. These icons are free and can be used for both commercial and personal projects. You are also free to modify the icons as you wish. No attributions is required. A cool feature is that you can send requests of icons that are not there.</li> </ul> <p></p> <ul> <li>NIH Bioart: a repository of SVG and PNG icons biomed-related. This is an</li> <li>initiative from the NIH. Most icons are of public domain so citation is appreciated but not required; however,</li> <li>there are other icons subject to other licenses, so I recommend looking to the citation guides.</li> </ul> <p></p>"},{"location":"Tools/Graphic%20design/Repositories%20of%20SVG%20icons/#references","title":"References","text":"<ul> <li>Olivia Dove-Estrella</li> </ul>"},{"location":"Tools/Programming/git/","title":"Git","text":"<p>This section contains information about how you can use Git to manage your projects in terms of version control and collaboration.</p> <p>Advantages:</p> <ul> <li>\ud83d\udcdd Keep track of changes in your project and scripts.</li> <li>\ud83d\udc65 Collaborate with other people.</li> <li>\ud83c\udf10 Share your project with other people.</li> <li>\u23ea Go back to a previous version of your project.</li> </ul>"},{"location":"Tools/Programming/git/#key-concepts","title":"Key concepts","text":"<ul> <li>Repository: A repository is a collection of files and folders that you want to track with Git. A repository can be local (on your computer) or remote (on a server such as GitHub).</li> <li>Commit: A commit is a snapshot of your project at a given time. It contains all the files and folders that are part of your project at that time. When you want to change something in your project, you need to create a commit. This commit will contain the changes you made to your project.</li> <li>Branch: A branch is a parallel version of your project. It allows you to work on different versions of your project at the same time. For example, you can have a branch for the development of a new feature and another branch for fixing bugs.</li> <li>Merge: A merge is the combination of two branches. When you merge two branches, the changes made in one branch are added to the other branch.</li> <li>Pull request: A pull request is a request to merge two branches. It is usually used to merge a branch into the main branch of your project.</li> <li>Workspace: A workspace is your working area where you can make changes to your project. It is usually a folder on your computer.</li> <li>Staging: The staging area is a temporary storage where you can add files before creating a commit. This allows you to create commits with only the files you want to include.</li> </ul>"},{"location":"Tools/Programming/git/#initialize-git-in-your-project","title":"Initialize Git in your project","text":"<p>To initialize Git in your project, you need to run the following command in the root directory of your project:</p> <pre><code>cd &lt;project-directory&gt;\ngit init\n</code></pre> <p>This will create a <code>.git</code> directory in your project. This directory contains all the information about your project and its history.</p>"},{"location":"Tools/Programming/git/#git-commands","title":"Git commands","text":"<p>In this page, we will explain the most common Git commands. For more information, you can check the Git documentation or the Git cheetsheet.</p>"},{"location":"Tools/Programming/git/#basic","title":"Basic","text":"<ul> <li><code>git status</code>: Check the status of your project (files changed, files added, files removed, etc.).</li> <li><code>git add &lt;file&gt;</code>: Add a file to the staging area. This means that the file will be included in the next commit.</li> <li><code>git commit -m \"&lt;message&gt;\"</code>: Create a commit with the files in the staging area and add a message to the commit.</li> <li><code>git push</code>: Push/Upload your commits to a remote repository.</li> <li><code>git pull</code>: Pull/Apply the changes from a remote repository to your local repository.</li> </ul>"},{"location":"Tools/Programming/git/#advanced","title":"Advanced","text":"<ul> <li><code>git clone &lt;url&gt;</code>: Clone a remote repository to your local computer.</li> <li><code>git log</code>: Show the history of your project.</li> <li><code>git reset &lt;file&gt;</code>: Remove a file from the staging area.</li> <li><code>git reset &lt;commit&gt;</code>: Remove a commit from the history of your project.</li> <li><code>git checkout -b &lt;branch&gt;</code>: Create a new branch and switch to it.</li> <li><code>git checkout &lt;branch&gt;</code>: Switch to another branch.</li> <li><code>git merge &lt;branch&gt;</code>: Merge a branch into the current branch.</li> <li><code>git branch -d &lt;branch&gt;</code>: Delete a branch.</li> <li><code>git diff</code>: Show the changes made to your project.</li> <li><code>git stash</code>: Save your changes in a temporary storage.</li> <li><code>git stash pop</code>: Apply the changes from the temporary storage to your project.</li> <li><code>git fetch</code>: Download the changes from a remote repository to your local repository.</li> </ul>"},{"location":"Tools/Programming/git/#github","title":"GitHub","text":"<p>GitHub is a platform that allows you to host your Git repositories in the cloud. It also provides a web interface to visualize the history of your project and collaborate with other people.</p> <p>The BBGLab github is: https://github.com/bbglab. Here you can find all the repositories of the BBGLab, as well as create new ones for your projects.</p>"},{"location":"Tools/Programming/git/#clone-a-repository","title":"Clone a repository","text":"<p>To clone a repository from GitHub, follow the next steps:</p> <ol> <li>Go to the repository page on GitHub.</li> <li>Click on the green button \"Code\" and copy the URL. </li> <li>Open a terminal and run the following command:</li> </ol> <pre><code>git clone &lt;url&gt;\n</code></pre>"},{"location":"Tools/Programming/git/#visual-studio-code","title":"Visual Studio Code","text":"<p>The Visual Studio Code editor has a built-in Git integration that allows you to manage your Git repositories from the editor. It offers a graphical interface to visualize the history of your project, create commits, switch branches, etc.</p>"},{"location":"Tools/Programming/git/#features","title":"Features","text":"<ul> <li>Source Control: Manage your Git repositories from the editor. </li> <li>Git Graph: Visualize the history of all the changes in your project. </li> <li>Diff Editor: Visualize the changes made to your project. </li> </ul>"},{"location":"Tools/Programming/git/#best-practices","title":"Best practices","text":""},{"location":"Tools/Programming/git/#commits","title":"Commits","text":"<ul> <li> <p>Commit Related Changes   A commit should be a wrapper for related changes. One commit = one task.</p> </li> <li> <p>Commit Often   Small commits are easier to review, rollback, and share.</p> </li> <li> <p>Don't Commit Half-Done Work   You should only commit code when a logical component is completed.   If you're tempted to commit just because you need a clean working copy (to check out a branch, pull in changes, etc.)   consider using <code>git stash</code> instead.</p> </li> <li> <p>Test Your Code Before You Commit   Test it thoroughly to make sure it really is completed and has no side effects (as far as one can tell).</p> </li> <li> <p>Write Good Commit Messages</p> </li> </ul> Type Description <code>add</code> new code <code>fix</code> bug fix <code>chore</code> maintenance, tooling, or other non-production changes <code>feat</code> new feature for the user <code>test</code> adding or updating tests <code>refactor</code> code restructuring (variable names, functions, folders) <code>docs</code> documentation only <p>Each commit message should start with one of the types above, followed by a colon and a short description.   For example:</p> <pre><code>feat: add functionality\nfix: resolve crash\ndocs: update usage guide\n</code></pre>"},{"location":"Tools/Programming/git/#branching","title":"Branching","text":"<ul> <li> <p>Clear Naming Convention   Use <code>type/short-description</code> (e.g. <code>feature/this-is-a-new-feature</code>, <code>bugfix/in-this-we-fix-an-error</code>,   <code>chore/update-dependencies</code>).</p> </li> <li> <p>One Concern per Branch   Keep each branch focused on a single feature, bugfix, or chore to make review and testing easier.</p> </li> <li> <p>Regularly Sync with Main   Pull or rebase the latest <code>main</code> into your branch often to minimize merge conflicts.</p> </li> <li> <p>Short-lived Branches   Aim to finish and merge branches within a few days; long-lived branches increase integration pain.</p> </li> <li> <p>Use Pull Requests for Merging   Open a PR against <code>main</code> (or your integration branch), add a clear description (you can use GitHub copilot to   automatically generate a clear description of your PR), link related issues. Add at least one reviewer and wait for   the approval before merging to <code>main</code>.</p> </li> <li> <p>Clean up after merge   Delete branches once they\u2019ve been merged to keep your repo tidy and avoid confusion.</p> </li> </ul>"},{"location":"Tools/Programming/git/#references","title":"References","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Federica Brando</li> <li>Davide Scarpetta</li> </ul>"},{"location":"Tools/Programming/python/Updating_pypi/","title":"Updating package on pypi","text":"<p>Pre-requisite</p> <p>Before updating a python package on pypi for a bug fix or feature or a new release, you should have the following:</p> <ul> <li>Up-to-date local repository with all changes pushed to <code>GitHub</code> </li> <li>You have updated <code>Setup.py</code> with an incrementation of the number in version.</li> <li>You have a README.md which will be your pypi documentationo</li> <li>a LICENSE file</li> </ul>"},{"location":"Tools/Programming/python/Updating_pypi/#1-step-update-local-package-for-distribution","title":"1 Step - Update local package for distribution","text":"<p>Updates the tools necessary for the packages:</p> <pre><code>python -m pip install --user --upgrade setuptools wheel build\npython -m pip install --user --upgrade twine\npython -m pip install -U packaging\n</code></pre>"},{"location":"Tools/Programming/python/Updating_pypi/#2-step-create-a-local-distribution-packages","title":"2 Step - Create a local distribution packages","text":"<p>The following command will make the folder <code>dist/</code> with new version of the files.</p> <pre><code>python -m build --sdist\npython -m build --wheel\ntwine check dist/*\n</code></pre> <p>If everything passes the checks then you can go ahead.</p>"},{"location":"Tools/Programming/python/Updating_pypi/#3-step-upload-the-distribution-to-a-test-server","title":"3 Step - Upload the distribution to a test server","text":"<p>In order to perform this step you need to have an account in the test.pypi.org registry.</p> <pre><code>python -m twine upload --repository testpypi dist/* -u __token__\n</code></pre> <p>An API token is required to upload the package. You can generate one in your account settings. Once everything is all set, you can check the package in the test.pypi server:</p> <pre><code>https://test.pypi.org/project/&lt;PACKAGE&gt;/&lt;VERSION&gt;/\n</code></pre>"},{"location":"Tools/Programming/python/Updating_pypi/#31-step-environment-with-test-installation","title":"3.1 Step - Environment with test installation","text":"<p>Create a local environment to see if the package works:</p> <pre><code>conda create --name test_pypi python\n</code></pre> <p>Then install the package from the test server (An API token is required to upload the package):</p> <pre><code>python -m pip install --index-url https://test.pypi.org/simple/ --no-deps &lt;PACKAGE&gt;\n</code></pre>"},{"location":"Tools/Programming/python/Updating_pypi/#4-step-upload-the-distribution-files-on-pypi","title":"4 Step - Upload the distribution files on pypi","text":"<p>Ready to roll!</p> <pre><code>python -m twine upload --repository pypi -u __token__ dist/*\n</code></pre> <p>Check the package online:</p> <pre><code>https://pypi.org/project/&lt;PACKAGE&gt;/&lt;VERISON&gt;/\n</code></pre>"},{"location":"Tools/Programming/python/Updating_pypi/#additional-resources","title":"Additional resources","text":"<ul> <li>Full documentation</li> </ul>"},{"location":"Tools/Programming/python/Updating_pypi/#references","title":"References","text":"<ul> <li>Federica Brando</li> <li>Source</li> </ul>"},{"location":"Tools/Programming/python/python_resources/","title":"Python Resources","text":"<p>Set of useful Python resources as listed on the BBGcloud Dashboard Learning platform</p>"},{"location":"Tools/Programming/python/python_resources/#coding-basics","title":"Coding basics","text":"<p>Get started with Python - Python tutorial covering the basics of Python</p> <p>Good coding practices - Good practices for the BBLab coders</p>"},{"location":"Tools/Programming/python/python_resources/#more-specific-topics","title":"More specific topics","text":"<p>Python Classes -  Tutorial on Python classes</p> <p>Write docs  Create documentation for your project using Sphinx</p> <p>Python Decorators  - Introduction to Python decorators</p>"},{"location":"Tools/Programming/python/python_resources/#reference","title":"Reference","text":"<ul> <li>Axel Rosendahl Huber</li> </ul>"},{"location":"Tools/VSCode/cluster_node/","title":"VSCode in interactive node","text":""},{"location":"Tools/VSCode/cluster_node/#description","title":"Description","text":"<p>These are the instructions to use Visual Studio Code to run and debug scripts/notebooks within an interactive node from the cluster.</p>"},{"location":"Tools/VSCode/cluster_node/#using-vscode-app","title":"Using VSCode App","text":""},{"location":"Tools/VSCode/cluster_node/#step-1-cluster-open-interactive-session","title":"Step 1: [CLUSTER] Open interactive session","text":"<p>The first step is to allocate the resources you will need for your executions. We can do this with the <code>interactive</code> command.</p> <p>Tip: Use <code>screen</code></p> <p>Consider launching the interactive session in a screen so that it doesn't get killed when the terminal is closed.</p> <pre><code>screen -S vscode\n</code></pre> <p>Launch an interactive session within a given node, allocating some computing resources:</p> <pre><code>interactive -c 6 -m 20\n</code></pre> <p>Then, check on which node you are. You can look at the prompt, or use the <code>hostname</code> command.</p> <pre><code>hostname\n</code></pre> <p>Example:</p> <pre><code>$ hostname\nirbccn43.hpc.irbbarcelona.pcb.ub.es\n</code></pre>"},{"location":"Tools/VSCode/cluster_node/#step-2-cluster-execute-job-vscode-interactive-jobsh","title":"Step 2: [CLUSTER] Execute job vscode-interactive-job.sh","text":"<p>First time setup</p> <p>If it is the first time you are using this method, copy the following lines into a file called <code>vscode-interactive-job.sh</code> in your home directory.</p> <p>File: <code>vscode-interactive-job.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"tunnel\"\n#SBATCH --time=8:00:00     # walltime\n\n/usr/sbin/sshd -D -p 2222 -f /dev/null -h ${HOME}/.ssh/id_ecdsa # uses the user key as the host key\n</code></pre> <p>Execute the <code>vscode-interactive-job.sh</code> script with:</p> <pre><code>bash vscode-interactive-job.sh\n</code></pre> <p><code>id_ecdsa</code> not found</p> <p>If you get an error saying that the file <code>id_ecdsa</code> is not found, you can generate it with the following command:</p> <pre><code>ssh-keygen -t ecdsa\n</code></pre> <p>It will prompt you several times to confirm the generation of the key. You can just press <code>Enter</code> to accept the default values.</p> <p>The terminal will look like it got stuck, but what is happening is that it is waiting for the SSH tunnel to be established.</p>"},{"location":"Tools/VSCode/cluster_node/#step-3-local-open-ssh-tunnel","title":"Step 3: [LOCAL] Open SSH tunnel","text":"<p>In your local machine, you need to open the SSH tunnel with the following command:</p> <pre><code>ssh -L 2222:&lt;interactive_hostname&gt;:22 &lt;user&gt;@irblogin01.irbbarcelona.pcb.ub.es\n</code></pre> <p>Example:</p> <pre><code>ssh -L 2222:irbccn43.hpc.irbbarcelona.pcb.ub.es:22 clopeze@irblogin01.irbbarcelona.pcb.ub.es\n</code></pre>"},{"location":"Tools/VSCode/cluster_node/#step-4-local-one-time-step-add-the-ssh-configuration","title":"Step 4: [LOCAL] [ONE-TIME-STEP] Add the SSH configuration","text":"<p>Modify the file <code>~/.ssh/config</code> in your local machine to include the following configuration:</p> <pre><code>Host irbccn*\n    HostName %h\n    ProxyJump irblogin01.irbbarcelona.pcb.ub.es\n    User &lt;user&gt;\n    Port 2222\n</code></pre> <p>Example:</p> <pre><code>Host irbccn43\n    HostName %h\n    ProxyJump irblogin01.irbbarcelona.pcb.ub.es\n    User clopeze\n    Port 2222\n</code></pre> <p>Node change</p> <p>This is marked as a \"ONE-TIME-STEP\", but in reality it depends on the node you are allocated. If you need to change the node or add a new node, the configuration will need to be updated.</p> <p>Tip (Optional): Helper function to add SSH configuration</p> <p>Here is a helper function that you can add to your <code>.bashrc</code> file to make it easier to setup the node and the tunnel.</p> <p>Steps the function does:</p> <ol> <li>Prompts only for the host alias.</li> <li>Checks if a config entry for that host already exists in <code>~/.ssh/config</code> (and skips adding it if so).</li> <li>Uses the fixed configuration values (with HostName as \"%h\", the fixed ProxyJump, port 2222, and the current computer user).</li> <li>Computes the full hostname for the tunnel (by appending \".hpc.irbbarcelona.pcb.ub.es\" to the alias) and creates the SSH tunnel (running in the background).</li> </ol> <pre><code>add_cluster_ssh_host_with_tunnel() {\n    read -p \"Enter host alias (e.g., irbccn43): \" host_alias\n    current_user=$(whoami)\n\n    # Check if SSH config for this host already exists\n    if grep -qE \"^Host[[:space:]]+${host_alias}\\$\" ~/.ssh/config; then\n        echo \"SSH configuration for host '${host_alias}' already exists. Skipping configuration update.\"\n    else\n        new_entry=\"Host ${host_alias}\n    HostName %h\n    ProxyJump irblogin01.irbbarcelona.pcb.ub.es\n    User ${current_user}\n    Port 2222\n    \"\n        echo -e \"\\n${new_entry}\" &gt;&gt; ~/.ssh/config\n        echo \"SSH configuration added for host '${host_alias}'.\"\n    fi\n\n    # Create the SSH tunnel.\n    # Assuming the full hostname is &lt;host_alias&gt;.hpc.irbbarcelona.pcb.ub.es\n    computed_hostname=\"${host_alias}.hpc.irbbarcelona.pcb.ub.es\"\n    tunnel_cmd=\"ssh -f -N -L 2222:${computed_hostname}:22 ${current_user}@irblogin01.irbbarcelona.pcb.ub.es\"\n    echo \"Establishing SSH tunnel with command:\"\n    echo \"${tunnel_cmd}\"\n    eval ${tunnel_cmd}\n    if [ $? -eq 0 ]; then\n        echo \"SSH tunnel established for host '${host_alias}'.\"\n    else\n        echo \"Failed to establish SSH tunnel.\"\n    fi\n}\n\nalias addsshcluster='add_cluster_ssh_host_with_tunnel'\n</code></pre>"},{"location":"Tools/VSCode/cluster_node/#step-41-local-one-time-step-access-though-the-terminal-to-the-node","title":"Step 4.1: [LOCAL] [ONE-TIME-STEP] Access though the terminal to the node","text":"<p>To make sure the configuration is working, you can access the node through the terminal with the following command:</p> <pre><code>ssh irbccn*\n</code></pre> <p>Example:</p> <pre><code>ssh irbccn43\n</code></pre> <p>This is needed the first time you connect to a new node, so that the node is added to the known hosts.</p>"},{"location":"Tools/VSCode/cluster_node/#step-5-vscode-connect-vscode-to-the-interactive-node","title":"Step 5: [VSCODE] Connect VSCode to the interactive node","text":"<p>Open Visual Studio Code and make sure you have installed the <code>Remote - SSH</code> extension.</p> <p>Open the side bar at the \"Remote - SSH\" panel, and then click the <code>irbccn*</code> option. A new window will open with the terminal connected to the interactive node. All the execution of notebooks and debuggers will be done from this node.</p> <p>Tip: Create a 'Profile' to keep the VSCode settings and extensions</p> <p>You can create a profile in VSCode to keep the settings and extensions for the connection to the cluster. This works similar to the idea of conda environments, but for the VSCode settings.</p> <p>To create a profile, click on the gear icon in the bottom left corner of VSCode, and then click on Profiles and create a new profile.</p>"},{"location":"Tools/VSCode/cluster_node/#from-browser-code-server","title":"From Browser (code-server)","text":"<p>This approach doesn't use the VSCode app, but instead uses the code-server package to run a VSCode instance in the cluster. This will result in launching VSCode from the browser.</p> Setup 1: Code-ServerSetup 2: PyNgrok <p>Description</p> <p>The macgiver way of mounting the bbgcluster filesystem in our respective local computers is very inefficient when it comes to coding with VSCode, particularly with regards to git version control capabilities.</p> <p>But there is a way to use full-fledged capabilities of VSCode for projects that are kept in the bbgcluster.</p> <p>TL;DR</p> <ul> <li>Launch a vscode server in the bbgcluster.</li> <li>SSH tunnel to this session from a local terminal.</li> <li>Connect to the server using a browser.</li> <li>Do so while keeping everything secure.</li> </ul> <p>Setting all up</p> <p>For the time being this tutorial does not cover the set up part in depth. However, here there is a bundle of command lines that are needed for setting up the security part.</p> <p>Become a certificate authority</p> <pre><code># Generate private key\nopenssl genrsa -des3 -out myCA.key 2048\n\n# Generate root certificate\nopenssl req -x509 -new -nodes -key myCA.key -sha256 -days 825 -out myCA.pem\n</code></pre> <p>Create CA-signed certs</p> <pre><code>NAME=vscode  # Use your own domain name\n# Generate a private key\nopenssl genrsa -out $NAME.key 2048\n\n# Create a certificate-signing request\nopenssl req -new -key $NAME.key -out $NAME.csr\n\n# Create a config file for the extensions\n&gt;$NAME.ext cat &lt;&lt;-EOF\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = $NAME # Be sure to include the domain name here because Common Name is not so commonly honoured by itself\nDNS.2 = bar.$NAME # Optionally, add additional domains (I've added a subdomain here)\nIP.1 = 0.0.0.0 # Optionally, add an IP address (if the connection which you have planned requires it)\nEOF\n\n# Create the signed certificate\nopenssl x509 -req -in $NAME.csr -CA myCA.pem -CAkey myCA.key -CAcreateserial \\\n-out $NAME.crt -days 825 -sha256 -extfile $NAME.ext\n</code></pre> <p>VSCode server in the bbgcluster</p> <p>Screen</p> <p>In the bbgcluster, either create</p> <pre><code>screen -S vscode\n</code></pre> <p>or access an existing vscode screen</p> <pre><code>screen -r vscode\n</code></pre> <p>Warning</p> <p>When opening a new screen, this should be done from the <code>login01</code> node, since this guarantees that the screen will be constantly running and not shut down (which could happen if the screen is opened in one of the other nodes).</p> <p>Interactive</p> <p>Launch an interactive session within a given node, allocating some computing resources:</p> <pre><code>interactive -w &lt;bbg-node&gt; -c 6 -m 20\n</code></pre> <p>Conda activate vsc_node environment</p> <pre><code>conda activate vsc_node\n</code></pre> <p>Warning</p> <p>the environment vsc_node is supposed to have been already created by the user; check this reference.</p> <p>Launch vscode server</p> <p>From your bbgcluster home do:</p> <pre><code>unset XDG_RUNTIME_DIR &amp;&amp; \\\ncode-server --port 8090 \\\n            --bind-addr 0.0.0.0 \\\n            --cert vscode.crt \\\n            --cert-key vscode.key\n</code></pre> <p>Warning</p> <p>--cert file vscode.crt --cert-key file vscode.key are supposed to be already generated by the user.</p> <p>SSH tunnel</p> <p>In a terminal of your local computer, do the following:</p> <pre><code>ssh -L 8090:&lt;bbgnode&gt;:8090 \\\n    -p 22022 \\\n    &lt;bbg-user&gt;@bbgcluster \\\n    -t \"htop\"\n</code></pre> <p>Warning</p> <p>htop is just a dirty trick to ensure the SSH tunnel does not spontaneouly shut down.</p> <p>Connect to the server</p> <p>Type the following https address in the browser:</p> <pre><code>https://0.0.0.0:8090/\n</code></pre> <p>A password prompt will appear. Fullfill the password request with a password you must have generated during the security setup referred to above.</p> <p>Requirements</p> <ul> <li>Conda or Mamba installed</li> <li>Have an account on ngrok</li> </ul> <p>Create a conda environment</p> <p>The conda environment must include the packages <code>code-server</code>, <code>pyngrok</code> and <code>screen</code></p> <pre><code>conda create -n vsc_node -c conda-forge code-server screen pyngrok -y\nconda activate vsc_node\n</code></pre> <p>Run code server in a screen (inside <code>login01</code>)</p> <pre><code>screen -S vscode\n</code></pre> <p>Right after creating the screen, create an interactive session and remember on which node you are allocated.</p> <pre><code>interactive\nconda activate vsc_node\ncode-server\n</code></pre> <p>Exit the screen with <code>Ctrl + A + D</code></p> <p>Run a Ngrok tunnel in a screen (inside <code>login01</code>)</p> <p>Note</p> <p>If this is your first time doing this step, you'll first need to setup your authentication token for ngrok.</p> <ol> <li>Log in to your ngrok home page.</li> <li>On the left-hand side bar: <code>Getting Started &gt; Your Authtoken</code></li> <li>On the <code>Command Line</code> section, copy only the key, which is the big string with random letters and numbers.</li> <li> <p>Go back to the terminanl in the cluster (with the <code>vsc_pyngrok</code> environment activated) and add your authentication token with the following command:</p> <p><code>ngrok authtoken &lt;the_token_you_copied_in_the_previous_step&gt;</code></p> </li> </ol> <p>This setup only has to be done once.</p> <pre><code>screen -S pyngrok\n</code></pre> <pre><code>interactive -w &lt;bbgnXXX&gt; # Node of previous step\nconda activate vsc_node\npyngrok http 8080\n</code></pre> <p>Copy the URL to your browser and exit the session <code>Ctrl + A + D</code></p> <p>Check your VSCode password</p> <pre><code>cat ~/.config/code-server/config.yaml\n</code></pre> <p>Browse your VSCode remotely</p> <p>When entering the URL in your browser, click <code>Visit site</code> and introduce the password you obtained in the previous step in order to be able to use VSCode.</p> <p></p>"},{"location":"Tools/VSCode/cluster_node/#reference","title":"Reference","text":"<ul> <li>Carlos L\u00f3pez-Elorduy</li> <li>Federica Brando</li> <li>Ferriol Calvet</li> <li>Ferran Mui\u00f1os</li> <li>Jordi Deu Pons</li> </ul>"},{"location":"pipelines/Intogenplus/","title":"IntOGen Plus","text":"<p>It's a framework for automatic and comprehensive knowledge extraction based on mutational data from sequenced tumor samples from patients.</p>"},{"location":"pipelines/Intogenplus/#run-intogen-dsl2","title":"Run IntOGen DSL2","text":"<p>Great effort was put to migrate IntOGen from nextflow DSL1 to nextflow DSL2. This effort allowed to be able to run the pipeline within our seqera platform dashboard.</p> <p>From the bbglabirb/ALP_pipelines workspace launchpad, you can access the pipelines available in our workspace.</p> <p>I can't see the workspace, what should I do?</p> <p>Please refer to Miguel or to Federica to solve this issue</p> <p>By clicking on intOGen-plus-dsl2 you'll be able to launch the pipeline.</p> <p></p> <p>Before launching the pipeline, some parameters need to be configured. Here a simple but complete list of useful parameters is explained.</p> <p>We highly recommend to keep the defaults for those parameters not discussed in this page.</p> General config sectionRun parameters section <p>Once both those sections are completed we are safe to run the pipeline.</p>"},{"location":"pipelines/Intogenplus/#revision-number","title":"Revision number","text":"<p>By default, the revision number is linked to the stable tag of the pipeline. As of now - it's <code>2024.11-dsl2</code>.  This can eventually be changed if a run is resumed or relaunched from the run section.</p> <p>Please be aware that changing this section may affect the <code>resume</code> option</p>"},{"location":"pipelines/Intogenplus/#config-profile","title":"Config profile","text":"<ul> <li><code>test</code> --&gt; this is using the CBIOP cohort in the repo [optional]</li> <li><code>test_full</code> --&gt; this is using the full datasets of intogen [optional].</li> <li><code>singularity</code> --&gt; this is allowing the use of singularity for using the containers</li> <li><code>irb</code> --&gt; this is allocating the right resources and queue for the slurm executor in the IRBCluster</li> </ul>"},{"location":"pipelines/Intogenplus/#workflow-run-name","title":"Workflow run name","text":"<p>It's mandatory to write a meaningful name. Here follows some examples:</p> <ul> <li>If I am running a new combination optimization I would call the run: <code>optimization_combination</code></li> <li>If I am running a FULL run with a new final version of intogen I would call it: <code>v3.0_ALL</code></li> <li>If I am reproducing the v2024 run I would call it: <code>v2024_ALL</code></li> <li>If I am running a specific cohort from an external collaborator I would call it: <code>v2024_EXT_COLLAB</code></li> </ul>"},{"location":"pipelines/Intogenplus/#work-directory","title":"Work directory","text":"<p>By default, the work directory is <code>/data/bbg/nobackup2/work/IntOGenDSL2/v2024/</code>. For faster execution you can use the scratch partition in the cluster: <code>/scratch/bbg/work/IntOGenDSL2/v2025/&lt;your-subfolder&gt;</code>. Replace <code>&lt;your-subfolder&gt;</code> with a meaningful name, such as the <code>Outdir</code> value from the next section, to avoid conflicts.</p> <p>Delete the work folder once the intogen run finishes successfully.</p>"},{"location":"pipelines/Intogenplus/#input","title":"Input","text":"<p>This parameter is read as a string, and it should be the absolute paths of the folder that openvariant will iterate separated by a space. Here it follows an example:</p> <pre><code>/path/to/datasets/for/intogen/input1 /path/to/datasets/for/intogen/input2 /path/to/datasets/for/intogen/input3\n</code></pre> <p>How do I prepare the input for IntOGen?</p> <p>Great question! Here the documentation where everything is explained:  intogen-plus.readthedocs</p>"},{"location":"pipelines/Intogenplus/#outdir","title":"Outdir","text":"<p>This parameter is where the output of intogen will be stored. By default we store intermediate runs that might fail here:</p> <pre><code>/data/bbg/nobackup2/scratch/intogen_dev_tests/dev-DSL2/v2024/&lt;MeaningfulName&gt;\n</code></pre> <p>It's important to add a meaningful name as a final directory output</p> <p>by default IntOGen will create a folder with a date where all the results will be stored. This although requires an higher level of specificity in the top folder.</p> <p>e.g. If I am running an external collab for LUNG data, I will add as an <code>outdir</code> parameter: <pre><code>/data/bbg/nobackup2/scratch/intogen_dev_tests/dev-DSL2/v2024/Lung_external_collab\n</code></pre></p> <p>The IntOGen pipeline will by default create a subdirectory with the date of the launch where it will store all the files: <pre><code>/data/bbg/nobackup2/scratch/intogen_dev_tests/dev-DSL2/v2024/Lung_external_collab/20250423/\n</code></pre></p> <p>Stable runs and releases are officially stored in a safer partition:  <pre><code>/data/bbg/datasets/intogen/output/runs\n</code></pre></p>"},{"location":"pipelines/Intogenplus/#faqs","title":"FAQs","text":"<p>The pipeline failed. How do I resume?</p> <p>In the run tab click on the three dots on the right of your run and click <code>Resume</code>.</p> <ul> <li>TBC</li> </ul>"},{"location":"pipelines/Intogenplus/#references","title":"References","text":"<ul> <li>Federica Brando</li> <li>Miguel Grau</li> </ul>"}]}